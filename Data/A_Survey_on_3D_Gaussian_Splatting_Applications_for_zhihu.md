# A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation

# 3D高斯点云应用综述:分割、编辑与生成

Shuting He, Peilin Ji, Yitong Yang, Changshuo Wang, Jiayi Ji, Yinglin Wang, Henghui Ding

何书婷，季沛霖，杨怡彤，王长硕，季佳怡，王英林，丁恒辉

Abstract-3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative to Neural Radiance Fields (NeRF) for 3D scene representation, offering high-fidelity photorealistic rendering with real-time performance. Beyond novel view synthesis, the explicit and compact nature of 3DGS enables a wide range of downstream applications that require geometric and semantic understanding. This survey provides a comprehensive overview of recent progress in 3DGS applications. It first introduces 2D foundation models that support semantic understanding and control in 3DGS applications, followed by a review of NeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS applications into segmentation, editing, generation, and other functional tasks. For each, we summarize representative methods, supervision strategies, and learning paradigms, highlighting shared design principles and emerging trends. Commonly used datasets and evaluation protocols are also summarized, along with comparative analyses of recent methods across public benchmarks. To support ongoing research and development, a continually updated repository of papers, code, and resources is maintained at https://github.com/heshuting555/Awesome-3DGS-Applications.

摘要-3D高斯点云(3D Gaussian Splatting，3DGS)作为神经辐射场(Neural Radiance Fields，NeRF)的强有力替代方案，近年来在三维场景表示中崭露头角，提供了高保真度的真实感渲染及实时性能。除新视角合成外，3DGS的显式且紧凑的特性使其能够支持需要几何和语义理解的多种下游应用。本文综述了3DGS应用的最新进展，首先介绍支持3DGS语义理解与控制的二维基础模型，随后回顾了启发3DGS方法的NeRF相关技术。我们将3DGS应用划分为分割、编辑、生成及其他功能任务，分别总结代表性方法、监督策略及学习范式，突出共性设计原则与新兴趋势。同时，本文汇总了常用数据集与评测协议，并对近期方法在公开基准上的表现进行了比较分析。为支持持续研究与开发，相关论文、代码及资源的持续更新仓库维护于https://github.com/heshuting555/Awesome-3DGS-Applications。

Index Terms-Survey, 3D Gaussian Splatting, Application, Segmentation, Editing, Generation

关键词-综述，3D高斯点云，应用，分割，编辑，生成

## 1 INTRODUCTION

## 1 引言

3 D Gaussian Splatting (3DGS) [1] has recently emerged as a powerful paradigm for real-time neural rendering, achieving high-fidelity photorealistic synthesis with superior efficiency. Unlike implicit field-based methods such as Neural Radiance Fields (NeRFs) [2], which rely on volumetric integration over coordinate-based neural networks, 3DGS represents scenes as explicit collections of anisotropic 3D Gaussians and renders them via differentiable rasterization. This explicit yet learnable formulation enables efficient optimization and fast inference, while preserving fine-grained geometric and appearance details. As a next-generation neural representation, 3DGS has demonstrated remarkable potential across a broad spectrum of applications, including virtual and augmented reality, robotics, autonomous navigation, and urban mapping.

3D高斯点云(3DGS)[1]作为一种强大的实时神经渲染范式，近年来兴起，能够以卓越效率实现高保真真实感合成。不同于基于隐式场的神经辐射场(NeRFs)[2]，其依赖于基于坐标的神经网络进行体积积分，3DGS将场景表示为各向异性的显式3D高斯集合，并通过可微光栅化进行渲染。这种显式且可学习的表述方式不仅支持高效优化和快速推理，还能保留细粒度的几何与外观细节。作为下一代神经表示技术，3DGS在虚拟现实、增强现实、机器人、自主导航及城市建模等广泛应用领域展现出显著潜力。

While early research on 3DGS [1], [3], [4] primarily focused on novel view synthesis, recent works have extended its scope to a growing number of downstream tasks, such as simultaneous localization and mapping (SLAM) [5], human avatar [6], segmentation [7], editing [8], generation [9], and more. These applications require richer representations that go beyond geometry to incorporate semantics, spatial relationships, and multi-modal cues. Compared to NeRF-based frameworks, 3DGS offers a more structured and interpretable formulation that enables efficient optimization, direct supervision, and intuitive manipulation. These properties make it advantageous for high-level tasks beyond rendering.

尽管早期3DGS研究[1]，[3]，[4]主要聚焦于新视角合成，近期工作已将其应用范围扩展至更多下游任务，如同时定位与地图构建(SLAM)[5]、人类虚拟形象[6]、分割[7]、编辑[8]、生成[9]等。这些应用需求超越几何信息，融合语义、空间关系及多模态线索。相比NeRF框架，3DGS提供了更结构化且可解释的表述，支持高效优化、直接监督及直观操作，使其在渲染之外的高层任务中更具优势。

Although several recent surveys [10], [11], [12], [13] have documented the rapid development of 3DGS, they primarily focus on global taxonomies, real-time rendering pipelines, or compression strategies, while offering limited insights into 3DGS-driven downstream applications. A few works [14], [15], [16] attempt to cover application domains, but typically lack systematic analysis of underlying design principles, methodological innovations, or benchmarking protocols. To address this gap, we present the first dedicated survey that systematically reviews downstream applications of 3DGS beyond classical view synthesis. Specifically, we focus on three rapidly evolving directions, namely segmentation, editing, and generation, as illustrated in Fig. 1.

尽管已有多篇综述[10]，[11]，[12]，[13]记录了3DGS的快速发展，主要聚焦于全局分类、实时渲染流程或压缩策略，对3DGS驱动的下游应用关注有限。少数工作[14]，[15]，[16]尝试涵盖应用领域，但通常缺乏对设计原则、方法创新及基准协议的系统分析。为填补此空白，本文首次系统综述了3DGS在经典视角合成之外的下游应用，重点关注快速发展的分割、编辑与生成三大方向，如图1所示。

![bo_d39q8b77aajc73907iag_0_908_968_754_437_0.jpg](https://raw.githubusercontent.com/brandfucker/Markdown4Zhihu/master/Data/A_Survey_on_3D_Gaussian_Splatting_Applications/bo_d39q8b77aajc73907iag_0_908_968_754_437_0.jpg)

Fig. 1: Overview of the three main 3DGS applications. (a) Segmentation: Segment each semantic region. (b) Editing: Perform editing via text or image prompt. (c) Generation: Support text-to- 3D and image-to-3D generation.

图1:三大主要3DGS应用概览。(a)分割:对每个语义区域进行分割。(b)编辑:通过文本或图像提示进行编辑。(c)生成:支持文本到3D及图像到3D的生成。

- Contribution. This survey provides a comprehensive and structured review of recent literature on 3DGS with a focus on its emerging downstream applications. We begin by introducing 2D foundation models that enable semantic control and understanding in 3DGS-based systems, and briefly revisit relevant NeRF-based methods to establish conceptual continuity. The core of our survey is organized around three major task categories: segmentation, editing, and generation. Within each category, we systematically compare representative works in terms of technical designs, learning paradigms, and supervision strategies. We further analyze benchmark settings and performance results in each domain, aiming to provide a self-contained and accessible resource for both newcomers and experienced researchers. Finally, we discuss open challenges and future research opportunities, with the goal of fostering deeper exploration and broader adoption of 3DGS across high-level vision and graphics application tasks.

- 贡献。本文系统且全面地回顾了近期3DGS文献，聚焦其新兴下游应用。首先介绍支持3DGS系统中语义控制与理解的二维基础模型，并简要回顾相关NeRF方法以建立概念连续性。核心内容围绕分割、编辑与生成三大任务类别展开，系统比较代表性工作在技术设计、学习范式及监督策略上的异同。进一步分析各领域的基准设置与性能表现，旨在为新手与资深研究者提供自洽且易用的资源。最后讨论开放挑战与未来研究方向，促进3DGS在高层视觉与图形应用中的深入探索与广泛应用。

---

- S. He, P. Ji, Y. Yang, and Y. Wang are with Shanghai University of Finance and Economics, China. shuting.he@sufe.edu.cn.

- 何书婷、季沛霖、杨怡彤及王英林均来自上海财经大学，邮箱:shuting.he@sufe.edu.cn。

- C. Wang is with University College London, United Kingdom.

- C. Wang 隶属于英国伦敦大学学院。

- J. Ji is with National University of Singapore, Singapore.

- J. Ji 隶属于新加坡国立大学，新加坡。

- H. Ding is with Fudan University, China. henghui.ding@gmail.com.

- H. Ding 隶属于中国复旦大学。henghui.ding@gmail.com。

---

![bo_d39q8b77aajc73907iag_1_142_127_1522_698_0.jpg](https://raw.githubusercontent.com/brandfucker/Markdown4Zhihu/master/Data/A_Survey_on_3D_Gaussian_Splatting_Applications/bo_d39q8b77aajc73907iag_1_142_127_1522_698_0.jpg)

Fig. 2: Overview of the structure of this survey.

图2:本综述结构概览。

- Organization. Fig. 2 provides an overview of the structure of this survey. In Sec. 2, we introduce essential background knowledge, including key definitions, commonly used 2D foundation models, and relevant NeRF-based research that informs 3DGS development. Sec. 3 presents a task-centric review of recent 3DGS-based methods, with a focus on segmentation, editing, and generation. In Sec. 4, we summarize evaluation protocols and benchmark settings, and provides quantitative comparisons across tasks and datasets to assess the performance of existing approaches. Sec. 5 discusses open challenges and outlines potential future research directions. Finally, Sec. 6 concludes the survey with a summary of key insights and takeaways.

- 组织结构。图2展示了本综述的结构概览。第2节介绍了基本背景知识，包括关键定义、常用的二维基础模型(2D foundation models)以及相关的基于NeRF(神经辐射场)的研究，这些为三维高斯表示(3DGS)发展提供了理论基础。第3节以任务为中心回顾了近期基于3DGS的方法，重点关注分割、编辑和生成。第4节总结了评估协议和基准设置，并提供了跨任务和数据集的定量比较，以评估现有方法的性能。第5节讨论了开放挑战并概述了潜在的未来研究方向。最后，第6节总结了综述的关键见解和收获。

## 2 BACKGROUND

## 2 背景

In this section, we first categorize the main research directions of 3DGS for downstream applications. We then provide a concise overview of widely used 2D foundation models that support these tasks. Finally, we review related research domains in the NeRF literature that lay the groundwork for 3DGS development.

本节首先对3DGS在下游应用中的主要研究方向进行分类。随后简要介绍支持这些任务的广泛使用的二维基础模型。最后回顾NeRF文献中相关的研究领域，这些领域为3DGS的发展奠定了基础。

### 2.1 Problem Formulation and Taxonomy

### 2.1 问题表述与分类

Let  <img src="https://www.zhihu.com/equation?tex=\mathcal{X}" alt="\mathcal{X}" class="ee_img tr_noresize" eeimg="1">  and  <img src="https://www.zhihu.com/equation?tex=\mathcal{Y}" alt="\mathcal{Y}" class="ee_img tr_noresize" eeimg="1">  denote the input and output spaces, respectively. In the context of 3DGS-based downstream applications, the objective is to learn an ideal mapping function  <img src="https://www.zhihu.com/equation?tex={f}^{*!} : \mathcal{X} \mapsto  \mathcal{Y}" alt="{f}^{*!} : \mathcal{X} \mapsto  \mathcal{Y}" class="ee_img tr_noresize" eeimg="1">  that transforms visual observations into task-specific outputs such as semantic understanding, controllable editing, or content generation. Specifically, given a set of posed RGB images  <img src="https://www.zhihu.com/equation?tex={\left\{  {x}_{n}^{s}\right\}  }_{n, s} \subset  \mathcal{X}" alt="{\left\{  {x}_{n}^{s}\right\}  }_{n, s} \subset  \mathcal{X}" class="ee_img tr_noresize" eeimg="1">  , where  <img src="https://www.zhihu.com/equation?tex=n" alt="n" class="ee_img tr_noresize" eeimg="1">  denotes the number of views,  <img src="https://www.zhihu.com/equation?tex=s" alt="s" class="ee_img tr_noresize" eeimg="1">  represents different scenes,3DGS provides a differentiable intermediate representation that bridges perception and application-level tasks.

设 <img src="https://www.zhihu.com/equation?tex=\mathcal{X}" alt="\mathcal{X}" class="ee_img tr_noresize" eeimg="1"> 和 <img src="https://www.zhihu.com/equation?tex=\mathcal{Y}" alt="\mathcal{Y}" class="ee_img tr_noresize" eeimg="1"> 分别表示输入空间和输出空间。在基于3DGS的下游应用中，目标是学习一个理想的映射函数 <img src="https://www.zhihu.com/equation?tex={f}^{*!} : \mathcal{X} \mapsto  \mathcal{Y}" alt="{f}^{*!} : \mathcal{X} \mapsto  \mathcal{Y}" class="ee_img tr_noresize" eeimg="1"> ，将视觉观测转换为特定任务的输出，如语义理解、可控编辑或内容生成。具体而言，给定一组带姿态的RGB图像 <img src="https://www.zhihu.com/equation?tex={\left\{  {x}_{n}^{s}\right\}  }_{n, s} \subset  \mathcal{X}" alt="{\left\{  {x}_{n}^{s}\right\}  }_{n, s} \subset  \mathcal{X}" class="ee_img tr_noresize" eeimg="1"> ，其中 <img src="https://www.zhihu.com/equation?tex=n" alt="n" class="ee_img tr_noresize" eeimg="1"> 表示视角数量， <img src="https://www.zhihu.com/equation?tex=s" alt="s" class="ee_img tr_noresize" eeimg="1"> 代表不同场景，3DGS提供了一种可微分的中间表示，连接感知与应用级任务。

#### 2.1.1 Application Tasks Category

#### 2.1.1 应用任务类别

Depending on the nature of the output space  <img src="https://www.zhihu.com/equation?tex=\mathcal{Y}" alt="\mathcal{Y}" class="ee_img tr_noresize" eeimg="1">  , downstream applications built upon 3DGS can be broadly categorized into three representative directions: segmentation, editing, and generation.

根据输出空间 <img src="https://www.zhihu.com/equation?tex=\mathcal{Y}" alt="\mathcal{Y}" class="ee_img tr_noresize" eeimg="1"> 的性质，基于3DGS的下游应用大致可分为三类代表性方向:分割、编辑和生成。

- 3DGS Segmentation. Based on the input space  <img src="https://www.zhihu.com/equation?tex=\mathcal{X}" alt="\mathcal{X}" class="ee_img tr_noresize" eeimg="1">  , existing 3DGS segmentation methods can be broadly classified into two categories: generic segmentation and promptable segmentation. Generic segmentation aims to predict pixel- or point-level labels without external input prompts, and can be further divided into: (i) Semantic segmentation [17]; (ii) Instance segmentation [18], [19], [20]; and (iii) Panoptic segmentation [21], [22]. Promptable segmentation extends the generic setting by introducing external input prompts to guide the segmentation process. Depending on the form of prompts, this category includes: (i) Interactive segmentation, where users provide spatial hints such as clicks, scribbles, boxes, or masks [23], [24], [25]; (ii) Open-vocabulary segmentation, which uses category names as textual prompts to support segmenting novel or unseen classes [26], [27]; (iii) Referring segmentation, which relies on natural language expressions to localize and segment specific objects [7]. In addition, depending on the representation of the output space  <img src="https://www.zhihu.com/equation?tex=\mathcal{Y}" alt="\mathcal{Y}" class="ee_img tr_noresize" eeimg="1">  , segmentation results can be produced either on rendered 2D images [26], [28] or directly over the 3D Gaussian representations [29], [30], [31], [32]. - 3DGS Editing. Editing tasks aim to modify scene attributes such as geometry, appearance, or illumination, while maintaining structural and visual consistency across viewpoints. In this setting,  <img src="https://www.zhihu.com/equation?tex=\mathcal{Y}" alt="\mathcal{Y}" class="ee_img tr_noresize" eeimg="1">  denotes the edited scene state, often represented by an updated set of Gaussians. 3DGS facilitates localized and differentiable editing operations, including addition, deletion, and transformation of scene components, thus enabling applications such as content-aware inpainting and style-consistent relighting. Based on the form of inputs  <img src="https://www.zhihu.com/equation?tex=\mathcal{X}" alt="\mathcal{X}" class="ee_img tr_noresize" eeimg="1">  , editing approaches can be categorized into image-guided [33], [34] and text-guided methods [8], [35], [36].

- 3DGS分割。基于输入空间 <img src="https://www.zhihu.com/equation?tex=\mathcal{X}" alt="\mathcal{X}" class="ee_img tr_noresize" eeimg="1"> ，现有3DGS分割方法大致分为两类:通用分割和可提示分割。通用分割旨在预测像素级或点级标签，无需外部输入提示，进一步细分为:(i)语义分割[17]；(ii)实例分割[18]，[19]，[20]；(iii)全景分割[21]，[22]。可提示分割通过引入外部输入提示引导分割过程，依据提示形式包括:(i)交互式分割，用户提供空间提示如点击、涂鸦、框选或掩码[23]，[24]，[25]；(ii)开放词汇分割，使用类别名称作为文本提示以支持新颖或未见类别的分割[26]，[27]；(iii)指称分割，依赖自然语言表达定位并分割特定对象[7]。此外，依据输出空间 <img src="https://www.zhihu.com/equation?tex=\mathcal{Y}" alt="\mathcal{Y}" class="ee_img tr_noresize" eeimg="1"> 的表示，分割结果可在渲染的二维图像上生成[26]，[28]，或直接在三维高斯表示上生成[29]，[30]，[31]，[32]。- 3DGS编辑。编辑任务旨在修改场景属性，如几何形状、外观或光照，同时保持视角间的结构和视觉一致性。在此情境下， <img src="https://www.zhihu.com/equation?tex=\mathcal{Y}" alt="\mathcal{Y}" class="ee_img tr_noresize" eeimg="1"> 表示编辑后的场景状态，通常由更新后的高斯集合表示。3DGS支持局部且可微分的编辑操作，包括场景组件的添加、删除和变换，从而实现内容感知的修补和风格一致的重光照等应用。基于输入形式 <img src="https://www.zhihu.com/equation?tex=\mathcal{X}" alt="\mathcal{X}" class="ee_img tr_noresize" eeimg="1"> ，编辑方法可分为图像引导[33]，[34]和文本引导方法[8]，[35]，[36]。

- 3DGS Generation. Generation involves synthesizing novel 3D scenes or objects from limited inputs such as a single image, a sparse set of views, or a textual prompt. Here,  <img src="https://www.zhihu.com/equation?tex=\mathcal{Y}" alt="\mathcal{Y}" class="ee_img tr_noresize" eeimg="1">  represents the desired 3D output in the form of Gaussian primitives. Recent approaches enable direct generation of structured  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  content, supporting high-quality synthesis and compositionality within multimodal AIGC frameworks. Depending on the modality of inputs, generation methods can be grouped into image-to-3D [37], [38], [39] and text-to-3D paradigms [40], [41], [42]. Alternatively, generation methods can be categorized by output granularity into object-level [9], [40] and scene-level generation [43], [44].

- 3DGS生成。生成涉及从有限输入(如单张图像、稀疏视角集合或文本提示)合成新颖的3D场景或物体。这里， <img src="https://www.zhihu.com/equation?tex=\mathcal{Y}" alt="\mathcal{Y}" class="ee_img tr_noresize" eeimg="1"> 表示以高斯基元(Gaussian primitives)形式的期望3D输出。近期方法支持直接生成结构化的 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 内容，在多模态AIGC框架内实现高质量合成与组合性。根据输入模态，生成方法可分为图像到3D [37]，[38]，[39]和文本到3D范式 [40]，[41]，[42]。另外，生成方法也可按输出粒度分为对象级 [9]，[40]和场景级生成 [43]，[44]。

#### 2.1.2 Learning Paradigms for Application Tasks

#### 2.1.2 应用任务的学习范式

- Per-scene Optimization. In this paradigm, the model learns a scene-specific mapping  <img src="https://www.zhihu.com/equation?tex={f}_{{\theta }_{s}}" alt="{f}_{{\theta }_{s}}" class="ee_img tr_noresize" eeimg="1">  for each individual scene  <img src="https://www.zhihu.com/equation?tex=s" alt="s" class="ee_img tr_noresize" eeimg="1">  , where  <img src="https://www.zhihu.com/equation?tex={\theta }_{s}" alt="{\theta }_{s}" class="ee_img tr_noresize" eeimg="1">  denotes parameters optimized solely for that scene. Given multi-view observations  <img src="https://www.zhihu.com/equation?tex={\left\{  {x}_{n}^{s}\right\}  }_{n}" alt="{\left\{  {x}_{n}^{s}\right\}  }_{n}" class="ee_img tr_noresize" eeimg="1">  for one scene, the objective is to minimize a reconstruction loss across views:

- 每场景优化。在此范式中，模型为每个单独场景 <img src="https://www.zhihu.com/equation?tex=s" alt="s" class="ee_img tr_noresize" eeimg="1"> 学习特定的映射 <img src="https://www.zhihu.com/equation?tex={f}_{{\theta }_{s}}" alt="{f}_{{\theta }_{s}}" class="ee_img tr_noresize" eeimg="1"> ，其中 <img src="https://www.zhihu.com/equation?tex={\theta }_{s}" alt="{\theta }_{s}" class="ee_img tr_noresize" eeimg="1"> 表示仅针对该场景优化的参数。给定某场景的多视角观测 <img src="https://www.zhihu.com/equation?tex={\left\{  {x}_{n}^{s}\right\}  }_{n}" alt="{\left\{  {x}_{n}^{s}\right\}  }_{n}" class="ee_img tr_noresize" eeimg="1"> ，目标是最小化跨视角的重建损失:


<img src="https://www.zhihu.com/equation?tex={\widetilde{\theta }}_{s} \in  \underset{{\theta }_{s}}{\arg \min }\frac{1}{N}\mathop{\sum }\limits_{n}\varepsilon \left( {{f}_{{\theta }_{s}}\left( {x}_{n}^{s}\right) ,{x}_{n}^{s}}\right) , \tag{1}
" alt="{\widetilde{\theta }}_{s} \in  \underset{{\theta }_{s}}{\arg \min }\frac{1}{N}\mathop{\sum }\limits_{n}\varepsilon \left( {{f}_{{\theta }_{s}}\left( {x}_{n}^{s}\right) ,{x}_{n}^{s}}\right) , \tag{1}
" class="ee_img tr_noresize" eeimg="1">

where  <img src="https://www.zhihu.com/equation?tex=\varepsilon \left( {\cdot , \cdot  }\right)" alt="\varepsilon \left( {\cdot , \cdot  }\right)" class="ee_img tr_noresize" eeimg="1">  denotes a task-specific reconstruction loss function, such as an  <img src="https://www.zhihu.com/equation?tex={\ell }_{1}" alt="{\ell }_{1}" class="ee_img tr_noresize" eeimg="1">  loss, depending on the target application. This per-scene learning scheme enables high-fidelity modeling tailored to each scene, making it well-suited for tasks requiring view-consistent reconstruction or dense appearance modeling. However, the lack of generalization and high computational cost make it impractical for large-scale or real-time applications.

其中 <img src="https://www.zhihu.com/equation?tex=\varepsilon \left( {\cdot , \cdot  }\right)" alt="\varepsilon \left( {\cdot , \cdot  }\right)" class="ee_img tr_noresize" eeimg="1"> 表示特定任务的重建损失函数，如根据目标应用选择的 <img src="https://www.zhihu.com/equation?tex={\ell }_{1}" alt="{\ell }_{1}" class="ee_img tr_noresize" eeimg="1"> 损失。该每场景学习方案实现了针对每个场景的高保真建模，适合需要视角一致重建或密集外观建模的任务。然而，缺乏泛化能力且计算成本高，使其不适合大规模或实时应用。

- Feed-forward Learning. This alternative seeks to approximate a universal function  <img src="https://www.zhihu.com/equation?tex={f}_{\theta }" alt="{f}_{\theta }" class="ee_img tr_noresize" eeimg="1">  that generalizes across scenes. A shared parameter set  <img src="https://www.zhihu.com/equation?tex=\theta" alt="\theta" class="ee_img tr_noresize" eeimg="1">  is learned from a collection of scenes  <img src="https://www.zhihu.com/equation?tex={\left\{  {x}_{n}^{s}\right\}  }_{n, s} \subset" alt="{\left\{  {x}_{n}^{s}\right\}  }_{n, s} \subset" class="ee_img tr_noresize" eeimg="1">   <img src="https://www.zhihu.com/equation?tex=\mathcal{X}" alt="\mathcal{X}" class="ee_img tr_noresize" eeimg="1">  drawn from diverse environments:


<img src="https://www.zhihu.com/equation?tex=\mathcal{X} <img src="https://www.zhihu.com/equation?tex=中学习共享参数集" alt="中学习共享参数集" class="ee_img tr_noresize" eeimg="1"> \theta$:

" alt="\mathcal{X} <img src="https://www.zhihu.com/equation?tex=中学习共享参数集" alt="中学习共享参数集" class="ee_img tr_noresize" eeimg="1"> \theta$:

" class="ee_img tr_noresize" eeimg="1">
\widetilde{\theta } \in  \underset{\theta }{\arg \min }\frac{1}{NS}\mathop{\sum }\limits_{{n, s}}\varepsilon \left( {{f}_{\theta }\left( {x}_{n}^{s}\right) ,{x}_{n}^{s}}\right) . \tag{2}
$$

The learned function  <img src="https://www.zhihu.com/equation?tex={f}_{\theta }" alt="{f}_{\theta }" class="ee_img tr_noresize" eeimg="1">  allows direct inference on novel scenes via a single forward pass, supporting efficient generation and scalable deployment. While typically less precise than per-scene methods in modeling fine-grained geometry or photometric details, feed-forward approaches offer strong potential for real-time applications and serve as a foundation for generalizable 3D perception.

所学函数 <img src="https://www.zhihu.com/equation?tex={f}_{\theta }" alt="{f}_{\theta }" class="ee_img tr_noresize" eeimg="1"> 支持对新场景的单次前向推理，实现高效生成和可扩展部署。虽然在细粒度几何或光度细节建模上通常不及每场景方法精确，前馈方法在实时应用中展现出强大潜力，并作为可泛化3D感知的基础。

### 2.2 2D Foundation Model

### 2.2 2D基础模型

The success of 3DGS applications relies heavily on the integration of powerful 2D foundation models. Given the limited availability of large-scale 3D datasets, learning robust 3D representations remains challenging. To address this, a number of methods leverage pretrained 2D priors to enhance  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  representation.

3DGS应用的成功在很大程度上依赖于强大2D基础模型的整合。鉴于大规模3D数据集的有限性，学习鲁棒的3D表示仍具挑战。为此，许多方法利用预训练的2D先验来增强 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 表示能力。

- Foundation Features for Vision Tasks (DINO&DINOv2). DINO [45] frames self-supervised ViT training as self-distillation without labels, where the model learns from its own predictions. Despite its moderate size, it yields semantically rich features effective for object understanding. DINOv2 [46] scales this approach with larger models, more data, and refined training, producing strong general-purpose features competitive with supervised baselines. These models reveal emergent capabilities in part-level reasoning and robust representations, making them valuable.

- 视觉任务的基础特征(DINO&DINOv2)。DINO [45]将自监督ViT训练视为无标签的自蒸馏，模型从自身预测中学习。尽管规模适中，但其语义丰富的特征对物体理解效果显著。DINOv2 [46]通过更大模型、更多数据和精细训练扩展此方法，产出与监督基线竞争的强通用特征。这些模型展现了部分级推理和鲁棒表示的涌现能力，极具价值。

- Contrastive Language-Image Pre-training (CLIP). CLIP [47] trains separate image and text encoders on  <img src="https://www.zhihu.com/equation?tex={400}\mathrm{M}" alt="{400}\mathrm{M}" class="ee_img tr_noresize" eeimg="1">  web pairs, using a contrastive loss that brings matched embeddings together in a shared space while pushing mismatches apart. The resulting representation carries broad semantic knowledge and strong zero-shot ability, making CLIP a standard backbone for open-vocabulary and multimodal tasks.

- 对比语言-图像预训练(CLIP)。CLIP [47]在 <img src="https://www.zhihu.com/equation?tex={400}\mathrm{M}" alt="{400}\mathrm{M}" class="ee_img tr_noresize" eeimg="1"> 网络对上训练独立的图像和文本编码器，采用对比损失使匹配嵌入在共享空间中靠近，非匹配嵌入远离。所得表示蕴含广泛语义知识和强大零样本能力，使CLIP成为开放词汇和多模态任务的标准骨干。

- Segment Anything (SAM). SAM [48] is a foundation model for image segmentation, trained on  <img src="https://www.zhihu.com/equation?tex=1\mathrm{\;B}" alt="1\mathrm{\;B}" class="ee_img tr_noresize" eeimg="1">  masks over  <img src="https://www.zhihu.com/equation?tex={11}\mathrm{M}" alt="{11}\mathrm{M}" class="ee_img tr_noresize" eeimg="1">  images using a promptable paradigm. It supports versatile inputs-points, boxes, masks, or text-and enables zero-shot generalization across diverse segmentation tasks. While powerful, its heavy vision backbone limits efficiency, SAM2 [49] mitigates this with a hiera-based encoder, achieving real-time performance and improved accuracy in the video domain [50], [51], [52], [53].

- 任意分割(SAM)。SAM [48]是图像分割的基础模型，基于 <img src="https://www.zhihu.com/equation?tex=1\mathrm{\;B}" alt="1\mathrm{\;B}" class="ee_img tr_noresize" eeimg="1"> 掩码和 <img src="https://www.zhihu.com/equation?tex={11}\mathrm{M}" alt="{11}\mathrm{M}" class="ee_img tr_noresize" eeimg="1"> 图像采用可提示范式训练。它支持多样输入——点、框、掩码或文本——并实现多样分割任务的零样本泛化。尽管功能强大，其沉重的视觉骨干限制了效率，SAM2 [49]通过基于hiera的编码器缓解此问题，实现视频领域的实时性能和精度提升 [50]，[51]，[52]，[53]。

- Diffusion Models (DMs). These generative models start from Gaussian noise and progressively denoise it into an image (DDPM [54]). Latent Diffusion Models [55] (LDM) improve computational efficiency by performing denoising in a compressed latent space, while Stable Diffusion [55] builds on this approach by enabling text-conditioned generation through a CLIP [47] text encoder and a cross-attention mechanism. DiT [56] replaces the U-Net backbone with a Transformer, enhancing scalability and performance, and Flux [57] further advances this line by adopting a flow-matching framework with Transformer backbones, enabling faster sampling and higher-fidelity image generation.

- 扩散模型(Diffusion Models, DMs)。这些生成模型从高斯噪声开始，逐步去噪生成图像(DDPM [54])。潜在扩散模型(Latent Diffusion Models, LDM)[55]通过在压缩的潜在空间中进行去噪，提高了计算效率，而Stable Diffusion [55]基于此方法，通过CLIP [47]文本编码器和交叉注意力机制实现了文本条件生成。DiT [56]用Transformer替代了U-Net骨干网络，增强了可扩展性和性能，Flux [57]则进一步采用了基于Transformer骨干的流匹配框架，实现了更快的采样速度和更高保真度的图像生成。

### 2.3 Related Research Areas

### 2.3 相关研究领域

- Segmentation Based on NeRF. Semantic NeRF [58] pioneers the incorporation of semantic information into radiance fields by introducing per-point semantic layers. Follow-up works can be broadly categorized into two directions. The first focuses on feature distillation, where high-dimensional 2D features from pretrained vision models (e.g., DINO, CLIP) are embedded into 3D neural fields. Representative methods such as Distilled Feature Fields [59], Neural Feature Fusion Fields [60], ISRF [61], LERF [62], and 3D-OVS [63] optimize volumetric feature fields to reconstruct semantic features via differentiable rendering. The second line of research targets mask lifting, which seeks to overcome the scarcity of  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  annotations by projecting  <img src="https://www.zhihu.com/equation?tex=2\mathrm{D}" alt="2\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  semantic masks into 3D space. Early methods rely on either ground-truth masks [64] or pretrained segmentation models, such as methods like Panoptic Lifting [65], ContrastiveLift [66], and SPIn-NeRF [67]. However, these approaches often suffer from limited generalization due to closed-set assumptions. With the recent emergence of powerful foundation models like SAM and SAM2 [48], newer works aim to leverage their open-world segmentation capabilities. For instance, SA3D [68] proposes an interactive pipeline that propagates a single SAM mask across multiple views into a NeRF-based scene, enabling sparse-to-dense segmentation. GARField [69] integrates SAM-derived masks into radiance field by learning a scale-conditioned affinity field.

- 基于NeRF的分割。Semantic NeRF [58]开创性地将语义信息引入辐射场，通过引入每点语义层实现。后续工作大致分为两类。第一类聚焦于特征蒸馏，将预训练视觉模型(如DINO、CLIP)中的高维二维特征嵌入三维神经场。代表方法如Distilled Feature Fields [59]、Neural Feature Fusion Fields [60]、ISRF [61]、LERF [62]和3D-OVS [63]通过可微渲染优化体积特征场以重建语义特征。第二类研究针对掩码提升，旨在通过将二维语义掩码投射到三维空间，克服 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 标注稀缺的问题。早期方法依赖真实掩码[64]或预训练分割模型，如Panoptic Lifting [65]、ContrastiveLift [66]和SPIn-NeRF [67]。但这些方法通常因封闭集假设而泛化能力有限。随着强大基础模型如SAM和SAM2 [48]的出现，最新工作尝试利用其开放世界分割能力。例如，SA3D [68]提出了一个交互式流程，将单个SAM掩码跨多视图传播到基于NeRF的场景，实现稀疏到密集的分割。GARField [69]通过学习尺度条件亲和场，将SAM生成的掩码整合进辐射场。

- Editing Based on NeRF. Recent progress in NeRF-based editing and manipulation has led to more intuitive and controllable frameworks for modifying  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  scenes and objects. Early efforts introduce conditional radiance fields that support localized editing through sparse  <img src="https://www.zhihu.com/equation?tex=2\mathrm{D}" alt="2\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  user inputs, enabling view-consistent changes in appearance and shape with minimal supervision [70]. Building on this, CLIP-NeRF [71] pioneers text- and image-driven NeRF editing using disentangled latent spaces for geometry and appearance, allowing independent control guided by CLIP em-beddings and supporting both category-level generation and real image inversion. Subsequent methods have addressed object-level editing and inpainting. Object-NeRF [72] and LaTeRF [73] further explore object-centric manipulation and structural completion in NeRF scenes. CoNeRF [74] introduces fine-grained, attribute-specific control with few-shot supervision, leveraging sparse 2D mask annotations and a quasi-conditional latent structure to enable localized edits and novel expression synthesis. Unified models that combine segmentation with NeRF-based inpainting further enable intuitive scene manipulation, such as object removal and completion from sparse cues [67]. Instruct-NeRF2NeRF [75] employs an image-conditioned diffusion model to iteratively edit input images while jointly optimizing the underlying 3D scene.

- 基于NeRF的编辑。NeRF编辑与操作的最新进展带来了更直观且可控的框架，用于修改 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 场景和对象。早期工作引入条件辐射场，支持通过稀疏 <img src="https://www.zhihu.com/equation?tex=2\mathrm{D}" alt="2\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 用户输入进行局部编辑，实现外观和形状的视图一致性变化，且监督需求低[70]。在此基础上，CLIP-NeRF [71]开创了基于文本和图像驱动的NeRF编辑，利用几何和外观的解耦潜空间，实现由CLIP嵌入引导的独立控制，支持类别级生成和真实图像反演。后续方法关注对象级编辑和修补。Object-NeRF [72]和LaTeRF [73]进一步探索了NeRF场景中的对象中心操作和结构补全。CoNeRF [74]引入了细粒度属性特定控制，结合少量监督，利用稀疏二维掩码注释和准条件潜结构，实现局部编辑和新颖表达合成。结合分割与NeRF修补的统一模型进一步支持直观的场景操作，如基于稀疏线索的对象移除和补全[67]。Instruct-NeRF2NeRF [75]采用图像条件扩散模型，迭代编辑输入图像，同时联合优化底层三维场景。

- Generation Based on NeRF. NeRF-based generative models have rapidly advanced 3D-aware content synthesis. Early works utilize implicit volumetric representations with adversarial training to achieve  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  consistent image generation from unposed views, improving shape-appearance disentanglement and scalability [76]. To boost efficiency and detail, later methods constrain sampling to 2D manifolds via ray-surface intersections [77]. A major shift occurs with Score Distillation Sampling (SDS), which enables NeRF optimization from text prompts using 2D diffusion models, eliminating the need for 3D supervision [78]. Follow-up work adopts coarse-to-fine strategies and hybrid representations to enhance quality and speed [79]. Latent-NeRF [80] uses Sketch-Shape to define the coarse structure of abstract geometric objects, enabling text- and shape-guided control over the generation process. Wang et al. [81] apply the chain rule to the learned gradients and backpropagate the diffusion model's score through the Jacobian of a differentiable renderer, instantiated as a voxel radiance field. In parallel, GAN-based models introduce tri-plane features and pose-aware training to generate high-resolution, multi-view consistent outputs [82]. Recent diffusion-guided approaches further extend to full-scene generation by combining semantic priors and progressive inpainting for photorealistic results [83].

- 基于NeRF的生成。基于NeRF(神经辐射场)的生成模型迅速推动了具备三维感知的内容合成。早期工作利用隐式体积表示结合对抗训练，实现了从无姿态视角生成一致图像，提升了形状与外观的解耦及可扩展性[76]。为提高效率和细节表现，后续方法通过射线-表面交点将采样限制在二维流形上[77]。一个重大转变是引入了评分蒸馏采样(Score Distillation Sampling，SDS)，该方法利用二维扩散模型从文本提示优化NeRF，免除了对三维监督的需求[78]。后续工作采用粗到细策略和混合表示以提升质量和速度[79]。Latent-NeRF[80]使用草图形状(Sketch-Shape)定义抽象几何体的粗略结构，实现了基于文本和形状的生成过程控制。Wang等[81]将链式法则应用于学习到的梯度，并通过可微渲染器的雅可比矩阵反向传播扩散模型的评分，该渲染器以体素辐射场形式实现。与此同时，基于GAN的模型引入三平面特征和姿态感知训练，生成高分辨率、多视角一致的输出[82]。近期的扩散引导方法进一步结合语义先验和渐进式修补，实现了全场景生成，获得了逼真的结果[83]。

## 3 3DGS APPLICATION TASKS

## 3 3DGS应用任务

### 3.1 3DGS Segmentation

### 3.1 3DGS分割

3DGS segmentation encompasses a broad spectrum of task definitions, as discussed in Sec. 2.1.1. Given the overlapping nature of segmentation tasks and that a single method may apply to multiple scenarios, we organize existing approaches based on their technical characteristics rather than task formulations.

3DGS分割涵盖了广泛的任务定义，如第2.1.1节所述。鉴于分割任务的重叠性及单一方法可能适用于多种场景，我们基于技术特征而非任务形式对现有方法进行组织。

#### 3.1.1 Feature Distillation-based Methods

#### 3.1.1 基于特征蒸馏的方法

These methods [26], [27], [84], [85], [86] aim to distill the semantic knowledge embedded in 2D foundation models (e.g., CLIP [47], SAM [48], DINO [45]) into 3D scene representations. By leveraging the semantic understanding learned from large-scale 2D datasets, these approaches enhance 3D models with open-vocabulary recognition capabilities. LangSplat [26] and Feature3DGS [27] are among the earliest works to explore this direction. LangSplat first segments images using SAM [48], and then feeds the resulting hierarchical semantic masks into CLIP to extract region-level semantic embeddings. To alleviate memory constraints, it employs a scene-specific language autoencoder to compress these high-dimensional semantic features. These compressed features are used to supervise a set of initialized 3D Gaussian language features, effectively transferring CLIP's open-vocabulary semantics into the 3D domain for downstream tasks. Feature3DGS, developed concurrently, distills features from LSeg [87] and SAM [48], and further employs SAM's decoder to interpret  <img src="https://www.zhihu.com/equation?tex=2\mathrm{D}" alt="2\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  rendered views, establishing a bridge between 2D and 3D segmentation. While effective, these methods may suffer from semantic information loss and high computational cost. To address these limitations, subsequent works have evolved along two key directions: improving segmentation accuracy and reducing computation overhead.

这些方法[26]，[27]，[84]，[85]，[86]旨在将嵌入于二维基础模型(如CLIP[47]、SAM[48]、DINO[45])中的语义知识蒸馏到三维场景表示中。通过利用从大规模二维数据集中学习到的语义理解，这些方法增强了三维模型的开放词汇识别能力。LangSplat[26]和Feature3DGS[27]是最早探索该方向的工作。LangSplat首先使用SAM[48]对图像进行分割，然后将得到的层级语义掩码输入CLIP以提取区域级语义嵌入。为缓解内存限制，它采用场景特定的语言自编码器压缩这些高维语义特征。压缩后的特征用于监督一组初始化的三维高斯语言特征，有效地将CLIP的开放词汇语义转移到三维领域以支持下游任务。Feature3DGS同期开发，蒸馏自LSeg[87]和SAM[48]的特征，并进一步利用SAM的解码器解释 <img src="https://www.zhihu.com/equation?tex=2\mathrm{D}" alt="2\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 渲染视图，建立了二维与三维分割之间的桥梁。尽管有效，这些方法可能存在语义信息丢失和计算成本高的问题。为解决这些限制，后续工作沿着提升分割精度和降低计算开销两大方向发展。

- Enhancing Segmentation Accuracy. N2F2 [88] addresses LangSplat's inefficiency in scale selection by learning a unified high-dimensional feature field under hierarchical supervision, where each feature dimension encodes scene semantics at different granularities, enabling fine-grained yet efficient representation without multi-scale querying. LangSurf [89] improves LangSplat by extracting dense pixel-level semantics from the entire image, followed by SAM-based mask pooling to preserve contextual cues. It also jointly optimizes language Gaussians on object surfaces using geometry supervision and contrastive loss, enhancing object's spatial consistency in 3D. seconGS [90] further refines LangSplat by using SAM2-generated masklets to provide semantically consistent supervision. To overcome the limitations of 2D querying in 3D space, it introduces a two-step querying mechanism that first retrieves distilled ground-truth and then queries individual Gaussians for spatially accurate 3D understanding. VLGaussian [28] observes that prior color-based rasteriza-tion is unsuitable for language modalities due to its dependence on the same color opacity. Instead, it introduces a cross-modal rasterizer tailored for rendering semantic language features with newly introduced opacity for language. SuperGSeg [91], inspired by superpoint representations, clusters Gaussians into Super-Gaussians using spatial and instance cues based on Scaffold-GS [3]. High-dimensional language features are assigned to each Super-Gaussian to support structured and comprehensive scene understanding while preserving linguistic granularity. FreeGS [92] introduces a semantic-embedded 3DGS framework by coupling semantic features with instance identities via the proposed IDSF field. A two-step alternating optimization and a 2D-3D contrastive loss ensure view-consistent semantics without requiring 2D labels. Beyond the aforementioned approaches that solely introduce semantic information, GLS [93] incorporates geometric priors (surface normals) to effectively refine the rendered depth.

- 提升分割精度。N2F2 [88] 通过在分层监督下学习统一的高维特征场解决了LangSplat在尺度选择上的低效问题，其中每个特征维度编码不同粒度的场景语义，实现了细粒度且高效的表示，无需多尺度查询。LangSurf [89] 通过从整张图像提取密集的像素级语义，随后利用基于SAM的掩码池化保留上下文信息，改进了LangSplat。它还通过几何监督和对比损失联合优化物体表面的语言高斯分布，增强了物体在三维空间中的空间一致性。seconGS [90] 进一步利用SAM2生成的掩码块提供语义一致的监督来细化LangSplat。为克服二维查询在三维空间中的局限性，提出了两步查询机制，先检索蒸馏的真实标签，再查询单个高斯分布，实现空间上精确的三维理解。VLGaussian [28] 观察到以往基于颜色的光栅化因依赖相同颜色不透明度而不适用于语言模态，因而引入了专为渲染语义语言特征设计的跨模态光栅化器，并为语言引入了新的不透明度。SuperGSeg [91] 受超点表示启发，基于Scaffold-GS [3]利用空间和实例线索将高斯分布聚类为超高斯分布。为每个超高斯分配高维语言特征，以支持结构化且全面的场景理解，同时保持语言粒度。FreeGS [92] 通过提出的IDSF场将语义特征与实例身份耦合，构建了语义嵌入的三维高斯场框架。采用两步交替优化和二维-三维对比损失，确保视图一致的语义，无需二维标签。除上述仅引入语义信息的方法外，GLS [93] 融入几何先验(表面法线)以有效细化渲染深度。

- Efforts toward Efficiency and Compactness. To mitigate the substantial memory overhead incurred by increasing feature channels in raw semantic features, LEGaussian [84] quantizes dense language features into a discrete feature space and semantic indices, and applies a smoothing loss to retain rendering quality with quantized representation. CLIP-GS [94] introduces semantic attribute compactness to efficiently encode scene semantics with Gaussians, enabling fast training and inference. It further improves 3D segmentation by employing a 3D-coherent self-training strategy that enhances cross-view semantic consistency. Later, FMGS [95] integrates 3D Gaussian scene representation with multi-resolution hash encodings (MHE) to enable efficient semantic embedding. Unlike previous methods, it avoids scene-specific quantization or autodecoders, thereby preserving the semantic fidelity of foundation model features. GOI [96] introduces a trainable feature clustering codebook that compresses high-dimensional, noisy semantic features into compact representations guided by scene priors. It effectively preserves semantic expressiveness while significantly reducing storage and rendering overhead. FastLGS [97] constructs a semantic feature grid that stores multi-view CLIP features extracted from SAM-generated masks. These grid features are then projected into a low-dimensional space and used to supervise semantic field training. Building upon FastLGS, FMLGS [98] enables part-level open-vocabulary queries in 3DGS by constructing consistent object-part semantics via SAM2, and introduces a semantic deviation strategy to enrich fine-grained features, allowing natural language queries over both objects and their parts. To address the high memory cost of jointly encoding color and semantics in existing 3DGS-based methods [27], [84], [94], DF-3DGS [99] decouples color and semantic fields to reduce Gaussian usage for semantics, and introduces a hierarchical compression pipeline combining dynamic quantization and scene-specific autoencoding for efficient representation. In summary, knowledge distillation provides a practical mechanism to bridge 2D foundation models and 3D Gaussian representations. While early methods focus on direct supervision, recent advances have introduced architectural, supervisory, and efficiency-oriented innovations to improve precision and scalability. These works collectively advance open-vocabulary  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  understanding under constrained computational and data resources.

- 提升效率与紧凑性。为缓解原始语义特征中增加特征通道带来的巨大内存开销，LEGaussian [84] 将密集语言特征量化为离散特征空间和语义索引，并应用平滑损失以保持量化表示下的渲染质量。CLIP-GS [94] 引入语义属性紧凑性，利用高斯分布高效编码场景语义，实现快速训练和推理。它进一步通过采用三维一致性自训练策略提升跨视图语义一致性，从而改进三维分割。随后，FMGS [95] 将三维高斯场表示与多分辨率哈希编码(MHE)结合，实现高效语义嵌入。不同于以往方法，它避免了场景特定的量化或自动解码器，从而保持基础模型特征的语义保真度。GOI [96] 引入可训练的特征聚类码本，在场景先验指导下将高维且噪声较大的语义特征压缩为紧凑表示，有效保持语义表达能力，同时显著降低存储和渲染开销。FastLGS [97] 构建语义特征网格，存储从SAM生成掩码中提取的多视角CLIP特征，再将这些网格特征投影到低维空间，用于监督语义场训练。在FastLGS基础上，FMLGS [98] 通过SAM2构建一致的物体部件语义，实现三维高斯场中的部件级开放词汇查询，并引入语义偏差策略丰富细粒度特征，支持对物体及其部件的自然语言查询。为解决现有三维高斯场方法[27],[84],[94]中联合编码颜色和语义导致的高内存成本，DF-3DGS [99] 解耦颜色和语义场，减少语义高斯使用量，并引入结合动态量化和场景特定自动编码的分层压缩流程，实现高效表示。综上，知识蒸馏为连接二维基础模型与三维高斯表示提供了实用机制。早期方法侧重直接监督，近期进展则引入了架构、监督及效率导向的创新，提升了精度与可扩展性。这些工作共同推动了受限计算和数据资源下的开放词汇 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 理解。

#### 3.1.2 2D Mask Lifting-based Methods

#### 3.1.2 基于二维掩码提升的方法

Accurate 3DGS segmentation remains challenging due to limited annotation and costly labeling. To address this, recent works explore lifting 2D masks from foundation models (e.g., SAM [48], SAM2 [49]) into 3D. However, 2D masks often suffer from cross-view inconsistency, where the same object may receive different instance IDs from different views, as well as quality issues like over- and under-segmentation, making direct lifting non-trivial.

由于标注有限且标注成本高，准确的三维高斯场分割仍具挑战。为此，近期工作探索将基础模型(如SAM [48]、SAM2 [49])的二维掩码提升至三维。然而，二维掩码常存在跨视图不一致问题，即同一物体在不同视角下可能获得不同实例ID，以及过度分割和欠分割等质量问题，使得直接提升变得复杂。

- Consistency-Oriented Pre-Processing. Several methods [19], [21], [22], [100], [101], [102] aim to improve multi-view mask consistency through pre-processing strategies before lifting 2D masks into 3D. GaussianGrouping [22] introduces an object association technique as a pre-processing step to align 2D segmentation maps across views, enhancing cross-view consistency. However, the quality of such pre-processing is often limited by errors in object correspondence under varying viewpoints. Gaga [19] addresses this by leveraging spatial cues and a 3D-aware memory bank to effectively associate object masks across diverse camera poses. CCGS [21] further argues that Gaga lacks segmentation-aware optimization within the Gaussian representation itself, which may lead to inconsistent or floating structures in the 3D segmentation field. To overcome this, CCGS constructs a unified 3D point cloud field and performs segmentation-constrained optimization to reinforce mask association.

- 面向一致性的预处理。多种方法[19]、[21]、[22]、[100]、[101]、[102]旨在通过预处理策略提升多视角掩码的一致性，在将二维掩码提升至三维之前进行处理。GaussianGrouping[22]引入了一种对象关联技术作为预处理步骤，以对齐跨视角的二维分割图，增强跨视角一致性。然而，此类预处理的质量常受限于不同视角下对象对应关系的误差。Gaga[19]通过利用空间线索和具备三维感知的记忆库，有效关联不同相机姿态下的对象掩码，解决了该问题。CCGS[21]进一步指出Gaga在高斯表示中缺乏分割感知的优化，可能导致三维分割场中出现不一致或漂浮结构。为克服此问题，CCGS构建了统一的三维点云场，并执行受分割约束的优化以强化掩码关联。

- Consistency-Oriented Post-Processing. While pre-processing strategies help improve mask alignment across views, they are often susceptible to accumulated errors that degrade segmentation quality. To address this, recent methods [18], [103] have shifted focus to post-processing techniques that refine multi-view predictions after initial lifting. For instance, OmniSeg3D [18] encodes instance-level semantics into a feature field through hierarchical contrastive learning. It then applies a hierarchical clustering algorithm in post-processing to merge and refine the lifted masks, ultimately yielding consistent and coherent  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  instance segmentations across views. Similarly, CAGS [103] leverages instance clustering and semantic matching to associate  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  instances with  <img src="https://www.zhihu.com/equation?tex=2\mathrm{D}" alt="2\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  open-vocabulary priors, thereby enhancing semantic consistency.

- 面向一致性的后处理。尽管预处理策略有助于提升视角间掩码的对齐，但往往易受累积误差影响，降低分割质量。为此，近期方法[18]、[103]转向后处理技术，在初步提升后细化多视角预测。例如，OmniSeg3D[18]通过分层对比学习将实例级语义编码进特征场，随后在后处理中应用分层聚类算法合并并细化提升的掩码，最终实现跨视角一致且连贯的 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 实例分割。同样，CAGS[103]利用实例聚类和语义匹配，将 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 实例与 <img src="https://www.zhihu.com/equation?tex=2\mathrm{D}" alt="2\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 开放词汇先验关联，从而增强语义一致性。

- Consistency-Aware End-to-End Learning. To reduce reliance on heuristic pre- and post-processing, recent works [17], [24], [30], [32], [104], [105], [106] propose end-to-end learning frameworks that directly construct consistent and distinguishable  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  feature fields. SAGA [30] distills 2D masks into 3D features that introduce a scale-aware contrastive training strategy that distills SAM's segmentation capability into a scale-gated affinity representation, which handles multi-granularity ambiguity. SA3D [104] introduces an iterative refinement strategy that alternates between inverse rendering of  <img src="https://www.zhihu.com/equation?tex=2\mathrm{D}" alt="2\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  masks and cross-view self-prompting, enabling progressively more consistent and accurate object segmentation across multiple views. Click-Gaussian [24] enables efficient interactive segmentation of 3D Gaussians by incorporating multi-granularity feature fields derived from 2D masks. It further introduces Global Feature-guided Learning (GFL) to alleviate inconsistencies across different views. Unified-Lift [20] develops an object-aware lifting pipeline that eliminates the need for additional alignment steps. It formulates a codebook-based object representation and aligns object-level semantics with Gaussian features via contrastive learning, leading to improved 3D instance understanding. OpenGaussian [32] focuses on 3D open-vocabulary segmentation by leveraging SAM-predicted masks to supervise 3D instance features. It adopts a coarse-to-fine feature discretization strategy via a two-stage codebook and introduces an instance-level 3D-2D association module that bridges Gaussian points with 2D masks and CLIP embeddings. econSG [107] improves zero-shot 3D semantic understanding by projecting refined multi-view language features into a low-dimensional 3D latent space, enabling efficient optimization and consistent semantic field initialization within the 3D Gaussian framework. CCL-LGS [108] aligns SAM-generated masks across views via a zero-shot tracker, extracts semantics with CLIP, and applies contrastive codebook learning to enforce cross-view consistency by promoting intra-class compactness and inter-class separation. VoteSplat [109] integrates Hough voting with 3DGS by learning per-Gaussian 3D offsets whose projections align with SAM-derived 2D vote maps and are depth-regularized, so that votes aggregate near object centroids. ILGS [110] mitigates view-inconsistent language embeddings by enforcing cross-view semantic alignment with an identity-aware semantic consistency loss and refining boundaries via progressive mask expansion.

- 面向一致性的端到端学习。为减少对启发式预处理和后处理的依赖，近期工作[17]、[24]、[30]、[32]、[104]、[105]、[106]提出端到端学习框架，直接构建一致且可区分的 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 特征场。SAGA[30]将二维掩码蒸馏为三维特征，引入尺度感知对比训练策略，将SAM(Segment Anything Model)的分割能力蒸馏至尺度门控亲和表示，处理多粒度歧义。SA3D[104]提出交替进行 <img src="https://www.zhihu.com/equation?tex=2\mathrm{D}" alt="2\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 掩码逆渲染与跨视角自提示的迭代细化策略，实现多视角下更一致准确的对象分割。Click-Gaussian[24]通过融合源自二维掩码的多粒度特征场，实现三维高斯的高效交互式分割，并引入全局特征引导学习(GFL)缓解不同视角间不一致。Unified-Lift[20]开发了面向对象的提升流程，免除额外对齐步骤，通过基于码本的对象表示及对比学习对齐对象级语义与高斯特征，提升三维实例理解。OpenGaussian[32]聚焦三维开放词汇分割，利用SAM预测掩码监督三维实例特征，采用两阶段码本的粗到细特征离散策略，并引入实例级三维-二维关联模块，桥接高斯点与二维掩码及CLIP嵌入。econSG[107]通过将精炼的多视角语言特征投影至低维三维潜空间，提升零样本三维语义理解，实现高效优化及三维高斯框架内一致的语义场初始化。CCL-LGS[108]通过零样本跟踪器对齐SAM生成的跨视角掩码，利用CLIP提取语义，并应用对比码本学习，通过促进类内紧凑性和类间分离性强化跨视角一致性。VoteSplat[109]结合霍夫投票与3DGS，学习每个高斯的三维偏移，其投影与SAM导出的二维投票图对齐并受深度正则化，使投票聚集于对象质心附近。ILGS[110]通过身份感知语义一致性损失强制跨视角语义对齐，并通过渐进式掩码扩展细化边界，缓解视角不一致的语言嵌入问题。

- Joint Optimization of Multiple Cues. To enhance the precision of 2D mask lifting, recent methods jointly optimize semantic, appearance, and geometric cues end-to-end. COB-GS [111] proposes a joint optimization framework that simultaneously learns semantics and texture, leading to sharper object boundaries while preserving visual fidelity. PanoGS [112] formulates a language-guided graph cut mechanism that aggregates 3D Gaussian primitives into super-primitives by leveraging both reconstructed geometry and linguistic signals. It further applies graph-based clustering with SAM-guided edge affinity to produce 3D-consistent panoptic segmentations. InstanceGaussian [31] mitigates the mismatch between appearance and semantics by progressively co-training both modalities within the Scaffold-GS [3] framework, achieving better alignment and instance-level coherence.

- 多重线索的联合优化。为了提升二维掩码提升的精度，近期方法端到端联合优化语义、外观和几何线索。COB-GS [111] 提出一个联合优化框架，同时学习语义和纹理，实现更清晰的物体边界并保持视觉保真度。PanoGS [112] 构建了一个语言引导的图割机制，利用重建几何和语言信号将三维高斯基元聚合为超基元，进一步通过基于图的聚类和SAM引导的边缘亲和力生成三维一致的全景分割。InstanceGaussian [31] 通过在Scaffold-GS [3]框架内逐步联合训练外观和语义两种模态，缓解了外观与语义的不匹配，实现更好的对齐和实例级一致性。

- Training-Free Mask Lifting. To avoid the computational cost and data requirements of model training, several methods explore training-free strategies that directly associate  <img src="https://www.zhihu.com/equation?tex=2\mathrm{D}" alt="2\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  segmentation masks with 3D Gaussian primitives for efficient 3D segmentation. SAGD [23] adopts a simple yet effective projection-based strategy: the centers of 3D Gaussians are projected onto 2D masks, and those falling within foreground regions are classified as foreground Gaussians. FlashSplat [113] formulates the mask lifting process as a one-step linear programming (LP) optimization problem, directly transforming 2D mask supervision into 3D Gaussian masks without iterative training. GaussianCut [29] represents the scene as a graph over Gaussians and applies graph-cut optimization to partition them into foreground and background. It leverages coarse segmentation from 2D image/video models and refines the results through learned edge affinities in the constructed graph. THGS [114] constructs a hierarchical superpoint graph from 3D Gaussian primitives and reprojects  <img src="https://www.zhihu.com/equation?tex=2\mathrm{D}" alt="2\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  semantic features onto this structure, producing a view-consistent semantic field without the need for supervised training. iSegMan [25] introduces a visibility-guided voting scheme that links 2D segmentations from SAM with 3D Gaussians, treating the association as a voting process weighted by Gaussian visibility. LBG [115] assigns semantics to 3D Gaussians via a 2D-to-3D lifting and incrementally merges them based on semantic and geometric overlap, enabling fast training-free 3D instance segmentation. LUDVIG [116] introduces "inverse rendering" aggregation, where 2D features or masks from multiple views are uplifted to the  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  by performing a weighted average guided by the 3DGS rendering weights. Then, a graph diffusion mechanism is designed for feature refinement.

- 无需训练的掩码提升。为避免模型训练的计算开销和数据需求，若干方法探索无需训练的策略，直接将 <img src="https://www.zhihu.com/equation?tex=2\mathrm{D}" alt="2\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 分割掩码与三维高斯基元关联，实现高效的三维分割。SAGD [23] 采用简单有效的基于投影的策略:将三维高斯中心投影到二维掩码中，落在前景区域的被判定为前景高斯。FlashSplat [113] 将掩码提升过程形式化为一步线性规划(LP)优化问题，直接将二维掩码监督转化为三维高斯掩码，无需迭代训练。GaussianCut [29] 将场景表示为高斯图，并应用图割优化将其划分为前景和背景，利用二维图像/视频模型的粗分割结果，并通过构建图中的学习边缘亲和力进行细化。THGS [114] 从三维高斯基元构建分层超点图，并将 <img src="https://www.zhihu.com/equation?tex=2\mathrm{D}" alt="2\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 语义特征重投影到该结构上，生成视图一致的语义场，无需监督训练。iSegMan [25] 引入基于可见性的投票方案，将SAM的二维分割与三维高斯关联，视关联为由高斯可见性加权的投票过程。LBG [115] 通过二维到三维的提升为三维高斯赋予语义，并基于语义和几何重叠逐步合并，实现快速无需训练的三维实例分割。LUDVIG [116] 引入“逆渲染”聚合，将多视角的二维特征或掩码通过三维GS渲染权重加权平均提升至 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> ，随后设计图扩散机制进行特征细化。

#### 3.1.3 Feed-Forward-based Methods

#### 3.1.3 基于前馈的方法

To overcome the inefficiency of per-scene optimization, especially under dense calibrated images conditions, several methods [117], [118], [119], [120], [121], [122] adopt feed-forward architectures to enable fast and generalizable  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  semantic field construction. LSM [123] is the first feed-forward framework that integrates DUSt3R [124], point transformer [125], and LSeg [87] for language-driven scene understanding. SLGaussian [117] builds a feed-forward pipeline that infers 3D semantic Gaussians directly from sparse viewpoints based on MVSplat [4]. Dr. Splat [119] departs from traditional rendering-based supervision by directly associating CLIP embeddings with 3D Gaussians. It introduces a language feature registration mechanism that assigns CLIP features to dominant Gaussians intersected by pixel rays. Additionally, it employs Product Quantization (PQ) learned from large-scale image datasets to compress features. LangScene-X [121] first uses a TriMap video diffusion network based on CogVideoX [126] to densify sparse inputs, synthesizing appearance, geometry, and semantics. It then employs a Language-Quantized Compressor (LQC) to encode language embeddings on large-scale datasets. SceneSplat [127] introduces a self-supervised framework that facilitates rich 3D feature learning from unlabeled scenes. To support this effort, SceneSplat-7K is introduced as the first large-scale dataset specifically designed for 3DGS in indoor environments.

为克服每场景优化的低效，尤其在密集标定图像条件下，若干方法[117]，[118]，[119]，[120]，[121]，[122]采用前馈架构，实现快速且具泛化能力的 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 语义场构建。LSM [123] 是首个集成DUSt3R [124]、点变换器(point transformer)[125]和LSeg [87]的语言驱动场景理解前馈框架。SLGaussian [117] 构建前馈流水线，基于MVSplat [4]从稀疏视角直接推断三维语义高斯。Dr. Splat [119] 摒弃传统基于渲染的监督，直接将CLIP嵌入与三维高斯关联，引入语言特征注册机制，将CLIP特征分配给被像素射线交叉的主导高斯，并采用从大规模图像数据集学习的乘积量化(PQ)压缩特征。LangScene-X [121] 首先使用基于CogVideoX [126]的TriMap视频扩散网络对稀疏输入进行密化，合成外观、几何和语义，随后采用语言量化压缩器(LQC)对大规模数据集上的语言嵌入进行编码。SceneSplat [127] 引入自监督框架，促进从无标签场景中学习丰富的三维特征。为支持该工作，SceneSplat-7K作为首个专为室内环境三维GS设计的大规模数据集被提出。

### 3.2 3DGS Editing

### 3.2 3DGS 编辑

Inspired by 2D editing methods [128], [129], editing works based on 3D Gaussian Splatting (3DGS) have advanced. These methods leverage the rich scene representation of 3DGS and often incorporate 2D diffusion models to guide precise 3D modifications. This section reviews and categorizes editing techniques and tasks.

受二维编辑方法[128]，[129]启发，基于三维高斯溅射(3DGS)的编辑工作取得进展。这些方法利用3DGS丰富的场景表示，常结合二维扩散模型指导精确的三维修改。本文回顾并分类编辑技术与任务。

#### 3.2.1 Editing Based on Text Prompt

#### 3.2.1 基于文本提示的编辑

Text-Driven 3D Editing [130], [131], [132] represents an emerging and rapidly advancing technology that allows users to enable convenient and efficient editing or generation of  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  scenes through simple natural language instructions, significantly reducing the technical barriers to 3D modeling and content creation.

基于文本驱动的三维编辑[130]，[131]，[132]是一项新兴且快速发展的技术，允许用户通过简单的自然语言指令便捷高效地编辑或生成 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 场景，显著降低了三维建模和内容创作的技术门槛。

- Localizing the Editing Object. Most current research first requires text to locate the objects that need to be edited. Gaus-sianEditor [8], [35], as pioneering work in the field of 3D editing based on 3D Gaussian Splatting technology, proposes innovative editing methodologies. The former significantly enhances editing precision by integrating textual descriptions throughout the training process to track editing targets. The latter achieves refined control over the editing process by extracting regions of interest (ROIs) corresponding to textual instructions and mapping these regions to 3D Gaussians. These methods leverage the Instruct-Pix2Pix [128] model during the editing process, but due to its inherent limitations, issues like inaccurate localization and limited editing control still persist. To address these, GSEditPro [133] introduces an attention-based progressive localization module, leveraging cross-attention layers from the T2I model to classify Gaussians and precisely localize editing regions. It also incorporates DreamBooth [134] for enhanced diversity and flexibility. Xiao et al. [135] propose a lighting-aware 3D scene editing pipeline, utilizing an Anchor View Proposal (AVP) algorithm to identify target regions and a coarse-to-fine optimization process with Depth-guided Inpainting Score Distillation Sampling (DI-SDS) for texture and lighting consistency.

- 定位编辑对象。目前大多数研究首先需要通过文本定位需要编辑的对象。Gaus-sianEditor[8]，[35]作为基于三维高斯点渲染技术的三维编辑领域的开创性工作，提出了创新的编辑方法。前者通过在训练过程中融合文本描述以追踪编辑目标，显著提升了编辑精度。后者通过提取与文本指令对应的感兴趣区域(ROI)并将其映射到三维高斯，实现了对编辑过程的精细控制。这些方法在编辑过程中利用了Instruct-Pix2Pix[128]模型，但由于其固有限制，仍存在定位不准确和编辑控制有限等问题。为解决这些问题，GSEditPro[133]引入了基于注意力的渐进式定位模块，利用T2I模型的交叉注意力层对高斯进行分类并精确定位编辑区域，同时结合DreamBooth[134]以增强多样性和灵活性。Xiao等[135]提出了一个考虑光照的三维场景编辑流程，采用锚视图提议(AVP)算法识别目标区域，并通过深度引导的修复评分蒸馏采样(DI-SDS)进行粗到细的优化，以保证纹理和光照的一致性。

- Efforts toward Multi-View Consistency. Editing 3D objects while maintaining multi-view consistency is a core challenge in the field of 3D editing. GaussCtrl [136] leverages depth-conditioned editing by utilizing naturally consistent depth maps to ensure geometric consistency across multi-view images, along with attention-based latent code alignment to achieve a unified appearance across views. Luo et al. [137] propose a progressive 3D editing strategy using a Trajectory-Anchored Scheme (TAS) and a dual-branch editing mechanism to ensure multi-view consistency, addressing challenges in error accumulation during text-to-image processes. Gomel et al. [138] introduce a geometry-guided warping mechanism that leverages the scene's depth and structural information to accurately map edits across views, ensuring multi-view consistency. DGE [139] employs spatiotemporal attention to jointly edit the selected key views and inject their features into other views, using correspondences derived from epipolar-constrained visual features, thereby establishing multiview consistency. SplatFlow [140] leverages a multi-view rectified flow (RF) model to simultaneously generate multi-view images, depth information, and camera poses based on text prompts. By incorporating training-free inversion and inpainting techniques, it achieves seamless editing of 3DGS. InterGSEdit [141] introduces a 3D Geometry-Consistent Attention Prior and an Adaptive Cross-Dimensional Attention Fusion Network to ensure multi-view consistency and enable fine-grained detail recovery.

- 致力于多视角一致性。保持多视角一致性地编辑三维对象是三维编辑领域的核心挑战。GaussCtrl[136]利用基于深度的编辑，通过自然一致的深度图确保多视角图像的几何一致性，并结合基于注意力的潜码对齐，实现视角间统一的外观。Luo等[137]提出了采用轨迹锚定方案(TAS)和双分支编辑机制的渐进式三维编辑策略，以确保多视角一致性，解决文本到图像过程中的误差累积问题。Gomel等[138]引入了基于几何引导的扭曲机制，利用场景的深度和结构信息准确映射跨视角的编辑，确保多视角一致性。DGE[139]采用时空注意力联合编辑选定关键视角，并将其特征注入其他视角，利用极线约束的视觉特征对应关系，从而建立多视角一致性。SplatFlow[140]利用多视角校正流(RF)模型，根据文本提示同时生成多视角图像、深度信息和相机位姿。通过引入无训练反演和修复技术，实现了三维高斯点渲染(3DGS)的无缝编辑。InterGSEdit[141]引入了三维几何一致性注意力先验和自适应跨维度注意力融合网络，确保多视角一致性并实现细粒度细节恢复。

- Efforts toward Efficiency and Speed. Additionally, to enhance the efficiency and speed of  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  editing, ProGDF [142] comprises Progressive Gaussian Sampling (PGS) and Gaussian Difference Field (GDF). PGS applies progressive constraints to generate diverse intermediate results during the editing process. GDF employs a lightweight neural network, leveraging these intermediate results to model the editing process, allowing for real-time, controllable, and flexible editing within a single training session. The optimization process of DreamCatalyst [36] approximates the reverse diffusion process, aligning with diffusion sampling dynamics and thereby reducing training time. 3DSceneEditor [143] utilizes a streamlined 3D pipeline, allowing direct manipulation of Gaussians. By leveraging input prompts, semantic labeling, and CLIP's zero-shot capabilities, it enables efficient, high-quality editing. Chen et al. [139] propose Direct Gaussian Editor, which adapts 2D image editing models into a multi-view consistent version and integrates the  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  geometric representation of the underlying scene. This method enables direct optimization of the 3D representation, eliminating the need for iterative editing. 3DitScene [144] incorporates generative priors and optimization techniques to refine the 3D Gaussian model and utilizes language features extracted by CLIP for object disentanglement.

- 致力于效率和速度。此外，为提升 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 编辑的效率和速度，ProGDF[142]包含渐进式高斯采样(PGS)和高斯差分场(GDF)。PGS在编辑过程中施加渐进约束以生成多样的中间结果。GDF采用轻量级神经网络，利用这些中间结果对编辑过程建模，实现单次训练内的实时、可控且灵活的编辑。DreamCatalyst[36]的优化过程近似逆扩散过程，符合扩散采样动态，从而缩短训练时间。3DSceneEditor[143]采用简化的三维流程，允许直接操作高斯。通过利用输入提示、语义标注和CLIP的零样本能力，实现高效且高质量的编辑。Chen等[139]提出了直接高斯编辑器，将二维图像编辑模型适配为多视角一致版本，并整合了底层场景的 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 几何表示。该方法支持对三维表示的直接优化，免去了迭代编辑的需求。3DitScene[144]结合生成先验和优化技术以细化三维高斯模型，并利用CLIP提取的语言特征进行对象解耦。

#### 3.2.2 Editing Based on Image Prompt

#### 3.2.2 基于图像提示的编辑

Editing results that rely solely on text prompts often fail to fully meet user expectations, as textual descriptions may not sufficiently convey user preferences regarding details, styles, or specific appearances. To address this, some methods [145], [146], [147], [148] have begun incorporating image prompts as a supplement. Image prompts offer more intuitive and customized visual information, including color, texture, shape, and lighting details, enabling more precise visual guidance in the  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  editing process.

仅依赖文本提示的编辑结果往往难以完全满足用户期望，因为文本描述可能无法充分传达用户对细节、风格或特定外观的偏好。为了解决这一问题，一些方法[145]、[146]、[147]、[148]开始引入图像提示作为补充。图像提示提供了更直观且个性化的视觉信息，包括颜色、纹理、形状和光照细节，从而在 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 编辑过程中实现更精确的视觉引导。

- Parameter-Efficient Fine-Tuning. To achieve personalized customization, it is necessary to learn from reference images and transfer them to the target object. TIP-Editor [33] introduces a progressive 2D personalization strategy that incorporates localized loss to ensure edits remain confined to user-defined regions. It employs LoRA [149] technology to fine-tune the text-to-image (T2I) model, binding the reference image to specific tokens for content personalization. Additionally, it utilizes explicit and flexible 3D Gaussian splatting as the  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  representation, enabling localized editing without altering the background. With a similar idea of [150], GS-VTON introduces a reference-driven image editing approach that integrates personalized information into a pre-trained 2D VTON model using LoRA fine-tuning. This method enables multi-view image editing while ensuring consistency across all views. In contrast, Cha et al. [151] propose PERSE, which learns a disentangled latent space from synthetic data to enable intuitive 3D facial editing, allowing attribute transfer from a reference image while preserving identity. Texture-GS [147] employs a UV mapping MLP, a local Taylor expansion of the MLP, and a learnable texture to decouple appearance from geometry by representing it as a  <img src="https://www.zhihu.com/equation?tex=2\mathrm{D}" alt="2\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  texture on a  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  surface.

- 参数高效微调。为了实现个性化定制，需要从参考图像中学习并迁移到目标对象。TIP-Editor [33]提出了一种渐进式二维个性化策略，结合局部损失以确保编辑仅限于用户定义的区域。它采用LoRA [149]技术对文本到图像(T2I)模型进行微调，将参考图像绑定到特定的token以实现内容个性化。此外，它利用显式且灵活的三维高斯点渲染(3D Gaussian splatting)作为 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 表示，实现局部编辑而不改变背景。与[150]类似，GS-VTON引入了一种基于参考的图像编辑方法，通过LoRA微调将个性化信息整合到预训练的二维虚拟试衣(VTON)模型中。该方法支持多视角图像编辑，同时保证所有视角的一致性。相比之下，Cha等人[151]提出了PERSE，从合成数据中学习解耦的潜在空间，实现直观的三维面部编辑，允许从参考图像转移属性同时保持身份。Texture-GS [147]采用UV映射的多层感知机(MLP)、MLP的局部泰勒展开及可学习纹理，通过将外观表示为 <img src="https://www.zhihu.com/equation?tex=2\mathrm{D}" alt="2\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 纹理映射在 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 表面上，实现外观与几何的解耦。

- Improving Multi-View Consistency with Diffusion Models. Given their robust theoretical foundation and exceptional performance, diffusion models have naturally been widely adopted in the field of 3D editing. VcEdit [34] introduces the Cross-attention Consistency Module and the Editing Consistency Module, integrating 3DGS into the image editing process. This ensures that the edited guidance images maintain consistency across multiple views. Similarly, TIGER [152] introduces Coherent Score Distillation, which combines a 2D image editing diffusion model with a multi-view diffusion model to enhance score distillation, enabling multi-view consistent editing with finer details.

- 利用扩散模型提升多视角一致性。鉴于其坚实的理论基础和卓越的性能，扩散模型自然被广泛应用于三维编辑领域。VcEdit [34]引入了交叉注意力一致性模块和编辑一致性模块，将3DGS整合进图像编辑流程，确保编辑后的引导图像在多视角间保持一致。类似地，TIGER [152]提出了连贯评分蒸馏(Coherent Score Distillation)，结合二维图像编辑扩散模型与多视角扩散模型以增强评分蒸馏，实现细节更丰富的多视角一致编辑。

- Multi-Stage Progressive Refinement. The multi-stage approach, through phased processing, enables the model to preserve important details of the original image while enhancing the editing effects in the target regions, thereby better adapting to complex editing tasks. Point'n Move [153] additionally introduces 2D hint points as conditions and designs a two-stage self-prompting segmentation algorithm for mask refinement and merging, enabling real-time editing without requiring training for each edit. Szymkowiak et al. [154] design a three-stage method: it begins with a set of input images and camera poses, utilizing a neural signed distance field (SDF) to reconstruct the scene surface, which guides the training of Gaussian splatting components to ensure alignment with the scene geometry. Finally, visual and geometric information is encoded into a lightweight triangle soup proxy. Edits are propagated through this intermediate structure to the mesh extracted from neural surface, thereby updating restored appearance. GaussianVTON [155] pioneers a 3D Virtual Try-On pipeline by integrating Gaussian Splatting with 2D VTON, introducing a three-stage refinement strategy to enhance consistency.

- 多阶段渐进式细化。多阶段方法通过分阶段处理，使模型在保留原图重要细节的同时，增强目标区域的编辑效果，从而更好地适应复杂编辑任务。Point'n Move [153]引入二维提示点作为条件，设计了两阶段自提示分割算法用于掩码细化与合并，实现无需针对每次编辑训练的实时编辑。Szymkowiak等人[154]设计了三阶段方法:首先利用输入图像及相机位姿，采用神经符号距离场(SDF)重建场景表面，指导高斯点渲染组件训练以确保与场景几何对齐。最终，将视觉与几何信息编码为轻量级三角网格代理。编辑通过该中间结构传播至神经表面提取的网格，更新恢复的外观。GaussianVTON [155]开创性地将高斯点渲染与二维虚拟试衣结合，提出三阶段细化策略以提升一致性。

#### 3.2.3 Style Transfer

#### 3.2.3 风格迁移

Unlike object manipulation tasks, style transfer requires preserving the structural integrity of the object while transferring the stylistic features from a reference image to the target image. This process typically involves the separation and recombination of low-level features (such as color and texture) and high-level features (such as semantic content) of the image.

与对象操作任务不同，风格迁移要求在保持对象结构完整性的同时，将参考图像的风格特征迁移到目标图像。该过程通常涉及图像的低级特征(如颜色和纹理)与高级特征(如语义内容)的分离与重组。

- Efforts toward Objective Function. SGSST [156] designs a Simultaneously Optimized Scales loss, enabling style transfer across all scales for consistent multi-scale stylization. ReGS [157] improves stylization consistency via a Stylized Pseudo View Supervision loss for uniform appearance across views, and a Template Correspondence Matching loss to propagate style to occluded regions. WaSt-3D [158] performs style transfer directly on 3D Gaussians, using an entropy-regularized Wasserstein-2 distance to smoothly map the style scene distribution to the content scene via gradient flow, and decomposing stylization into smaller subproblems for efficiency. Multi-StyleGS [159] introduces a style loss with bipartite matching between multiple style image regions and GS points to enable local style transfer.

- 目标函数的改进。SGSST [156]设计了同时优化尺度损失，实现跨所有尺度的风格迁移，保证多尺度风格一致性。ReGS [157]通过风格化伪视图监督损失提升风格一致性，实现视角间统一外观，并通过模板对应匹配损失将风格传播至遮挡区域。WaSt-3D [158]直接在三维高斯点上执行风格迁移，利用熵正则化的Wasserstein-2距离通过梯度流平滑映射风格场景分布到内容场景，并将风格化分解为更小子问题以提高效率。Multi-StyleGS [159]引入了基于多风格图像区域与高斯点之间二分匹配的风格损失，实现局部风格迁移。

- Efforts toward Integrating Diffusion Model. InstantStyle-Gaussian [160] accelerates style editing by using a diffusion model to generate target style images, incorporating them into the training set, and iteratively optimizing the Gaussian splat-ting scene. ArtNVG [161] enhances control over content and style via the CSGO model and Tile ControlNet, and introduces an Attention-based Neighboring-View Alignment mechanism to maintain consistent colors and textures across neighboring views. Morpheus [162] employs an RGBD diffusion model with depth-guided cross-attention, feature injection, and a Warp ControlNet conditioned on composite frames to guide 3D stylization. Fanta-syStyle [163] proposes Controllable Stylized Distillation, achieving 3D style transfer solely through diffusion model distillation.

- 融合扩散模型的努力。InstantStyle-Gaussian [160] 通过使用扩散模型生成目标风格图像，将其纳入训练集，并迭代优化高斯点云场景，加速风格编辑。ArtNVG [161] 通过CSGO模型和Tile ControlNet增强对内容和风格的控制，并引入基于注意力的邻视图对齐机制，以保持邻近视图间颜色和纹理的一致性。Morpheus [162] 采用带有深度引导交叉注意力的RGBD扩散模型、特征注入及基于复合帧条件的Warp ControlNet，引导3D风格化。FantasyStyle [163] 提出可控风格蒸馏方法，实现仅通过扩散模型蒸馏的3D风格迁移。

- Efforts toward VGG Feature Optimization. Saroha et al. [164] employ a pre-trained VGG model with AdaIN to transfer styles to specific views, training a 3D color module to predict new colors for each Gaussian. Similarly, StyleSplat [165] refines style transfer by applying a nearest neighbor feature matching loss between VGG features of the rendered image and the reference style image, and uses a  <img src="https://www.zhihu.com/equation?tex=2\mathrm{D}" alt="2\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  mask for selective stylization of specified objects. In contrast, StyleGaussian [166] embeds 2D VGG scene features into transformed 3D Gaussian features, combines them with reference image features, and decodes them into stylized RGB images. SemanticSplatStylization [167] integrates semantic understanding and uses VGG features to compute style loss, ensuring precise, context-aware stylization.

- VGG特征优化的努力。Saroha等人[164] 使用预训练的VGG模型结合AdaIN将风格迁移到特定视图，训练3D颜色模块预测每个高斯的新颜色。类似地，StyleSplat [165] 通过在渲染图像和参考风格图像的VGG特征间应用最近邻特征匹配损失，细化风格迁移，并使用 <img src="https://www.zhihu.com/equation?tex=2\mathrm{D}" alt="2\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 掩码选择性地对指定对象进行风格化。相比之下，StyleGaussian [166] 将二维VGG场景特征嵌入变换后的三维高斯特征，结合参考图像特征，解码为风格化RGB图像。SemanticSplatStylization [167] 融合语义理解，利用VGG特征计算风格损失，确保精确且具上下文感知的风格化。

#### 3.2.4 Other Editing Tasks

#### 3.2.4 其他编辑任务

- Object Removal. Gaussian Grouping [22] uses 2D mask predictions from the SAM [48] to assign a compact identity encoding to each Gaussian, enabling direct manipulation of specified ones. In contrast, GScream [145] introduces multi-view monocular depth estimation as an additional constraint to optimize the placement of Gaussian primitives, improving the geometric alignment between the removed area and the surrounding region. Qiu et al. [168] propose the Feature Splatting method, which distills vision-language features into a 3D Gaussian representation to enable semi-automatic scene decomposition via text queries, thereby facilitating scene editing operations such as object removal.

- 物体移除。Gaussian Grouping [22] 利用SAM [48] 的二维掩码预测为每个高斯分配紧凑的身份编码，实现对指定高斯的直接操作。相比之下，GScream [145] 引入多视角单目深度估计作为额外约束，优化高斯基元的布局，提升移除区域与周围区域的几何对齐。Qiu等人[168] 提出特征点云方法，将视觉语言特征蒸馏为三维高斯表示，通过文本查询实现半自动场景分解，从而便于场景编辑如物体移除。

- Drag. MVDrag3D [169] employs a multi-view diffusion model as a robust generative prior to achieve consistent drag editing across multiple rendered views. DYG [170] uses 3D masks and control point pairs to define the target editing area and drag direction, leveraging the advantages of implicit tri-plane representation to establish a geometric framework for the editing results.

- 拖拽。MVDrag3D [169] 采用多视角扩散模型作为稳健的生成先验，实现多视图渲染中的一致拖拽编辑。DYG [170] 利用三维掩码和控制点对定义目标编辑区域及拖拽方向，借助隐式三平面表示优势，建立编辑结果的几何框架。

- Video Editing. 3DEgo [171] introduces a noise blending module to apply the diffusion model for video frame editing and leverages 3D Gaussian splatting to generate 3D scenes from multi-view consistent edited frames, reducing the multi-stage editing workflow to a single-stage process. PortraitGen [172] lifts 2D portrait video editing into 3D by integrating 3D human priors, ensuring both 3D and temporal consistency in the edited video.

- 视频编辑。3DEgo [171] 引入噪声混合模块，将扩散模型应用于视频帧编辑，并利用三维高斯点云从多视角一致编辑帧生成三维场景，将多阶段编辑流程简化为单阶段。PortraitGen [172] 通过整合三维人体先验，将二维人像视频编辑提升至三维，确保编辑视频的三维和时间一致性。

- Inpainting. InFusion [173] uses an image-conditioned depth completion model to guide point initialization and directly recovers depth maps from images. RefFusion [174] fine-tunes an image inpainting diffusion model, effectively aligning the prior distribution with the 3D target scene. This reduces score distillation variance and yields clearer details.

- 修复。InFusion [173] 使用图像条件深度补全模型指导点初始化，直接从图像恢复深度图。RefFusion [174] 微调图像修复扩散模型，有效对齐先验分布与三维目标场景，降低评分蒸馏方差，生成更清晰细节。

### 3.3 3DGS Generation

### 3.3 3D高斯点云生成

While NeRF-based methods produce high-quality 3D content, they are slow and computationally expensive. Recent advances in 3D Gaussian Splatting (3DGS) enable faster, more efficient 3D generation from text or images, categorized by the type of output.

尽管基于NeRF的方法能生成高质量三维内容，但速度慢且计算成本高。近期三维高斯点云(3DGS)技术进展，实现了从文本或图像更快、更高效的三维生成，按输出类型分类。

#### 3.3.1 Object-level Generation

#### 3.3.1 物体级生成

A. Per-Scene Optimization Methods: Optimization-based methods extract diffusion priors from powerful 2D foundation diffusion models to guide and optimize  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  representations. DreamFu-sion [78] first introduces the Score Distillation Sampling (SDS) loss, which optimizes NeRF [2] using images generated from text prompts. Building on this, many methods [175], [176], [177], [178] have extensively explored optimization-based approaches.

A. 每场景优化方法:基于优化的方法从强大的二维基础扩散模型中提取扩散先验，以指导和优化 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 表示。DreamFusion [78] 首次引入评分蒸馏采样(SDS)损失，利用文本提示生成的图像优化NeRF [2]。在此基础上，许多方法[175]，[176]，[177]，[178] 广泛探索了基于优化的途径。

- Standard SDS-based Methods. These methods directly use Score Distillation Sampling (SDS) to optimize 3D scenes. (a) Text-to-3D. Tanget al. [9] extend 2D diffusion models to 3D by optimizing 3D Gaussians with SDS, introducing Mesh Extraction and Texture Refinement to mitigate SDS-induced blurriness. GS-GEN [179] and GaussianDreamer [40] incorporate 3D point cloud diffusion priors for geometry and appearance, while Gaussian-DreamerPro [180] adopts a geometry-guided framework to control Gaussian growth and enhance details. CompGS [181] decomposes scenes into entities for SDS optimization at entity and composition levels, dynamically adjusting spatial parameters for fine-grained details. CG3D [182] introduces a compositional framework for text-conditioned scene generation, producing detailed, multi-object, and plausible results. Hyper-3DG [183] refines Gaussians via a Geometry and Texture Hypergraph Refiner, and HCoG [184] hierarchically generates occlusion-aware assets. (b) Image-to-3D. ScalingGaussian [185] first generates point clouds with a 3D diffusion model, then refines Gaussians using a 2D diffusion model with SDS. Physics3D [186] simulates Gaussians via the Material Point Method (MPM) to estimate physical attributes and orientations, rendering them into video frames for SDS-based optimization with a pre-trained video diffusion model.

- 标准基于SDS的方法。这些方法直接使用评分蒸馏采样(Score Distillation Sampling，SDS)来优化三维场景。(a) 文本到三维。Tang等人[9]通过使用SDS优化三维高斯，将二维扩散模型扩展到三维，提出了网格提取和纹理细化以缓解SDS引起的模糊问题。GS-GEN[179]和GaussianDreamer[40]引入了用于几何和外观的三维点云扩散先验，而Gaussian-DreamerPro[180]采用了几何引导框架以控制高斯增长并增强细节。CompGS[181]将场景分解为实体，在实体和组合层面进行SDS优化，动态调整空间参数以实现细粒度细节。CG3D[182]引入了一个基于文本条件的组合框架，用于场景生成，产出细致、多物体且合理的结果。Hyper-3DG[183]通过几何和纹理超图细化器对高斯进行细化，HCoG[184]则分层生成具备遮挡感知的资产。(b) 图像到三维。ScalingGaussian[185]首先使用三维扩散模型生成点云，然后利用二维扩散模型结合SDS细化高斯。Physics3D[186]通过材料点法(Material Point Method，MPM)模拟高斯以估计物理属性和方向，将其渲染成视频帧，利用预训练视频扩散模型进行基于SDS的优化。

- Efforts toward Improving SDS. Although SDS provides guidance for  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  generation, it often suffers from low fidelity and over-smoothing. Recent works address these issues by enhancing geometric accuracy, detail preservation, and stability. (a) Text-to-3D. StableDreamer [187] formulates the SDS prior and L2 reconstruction loss as equivalent, reducing multi-face artifacts and improving geometry. Li et al. [188] propose Guided Consistency Sampling integrated with 3DGS to enhance detail and fidelity. DreamMapping [189] introduces Variational Distribution Mapping, accelerating distribution modeling by treating rendered images as degraded diffusion outputs. HumanGaussian [190] designs a structure-aware SDS for joint optimization of human appearance and geometry. LucidDreamer [41] mitigates over-smoothing via Interval Score Matching (ISM) with deterministic diffusion trajectories, while TSM [191] extends ISM by generating two reverse DDIM [192] paths to reduce cumulative errors and pseudo-ground truth inconsistencies. GaussianMotion [193] employs adaptive score distillation to balance realism and smoothness. (b) Image-to-3D. Basak et al. [194] introduce a frequency-based distillation loss, extracting low-frequency geometry from 3D diffusion priors and refining high-frequency texture with 2D diffusion. GECO [195] adopts a two-stage pipeline combining Variational Score Distillation [196] and multi-view consistency refinement, enabling sub-second high-quality generation. DreamPhysics [197] leverages video diffusion priors to learn material properties and applies motion distillation sampling to emphasize dynamic cues.

- 改进SDS的努力。尽管SDS为 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 生成提供了指导，但常常存在保真度低和过度平滑的问题。近期工作通过提升几何精度、细节保留和稳定性来解决这些问题。(a) 文本到三维。StableDreamer[187]将SDS先验和L2重建损失等价化，减少多面体伪影并改善几何形状。Li等人[188]提出了结合3DGS的引导一致性采样，以增强细节和保真度。DreamMapping[189]引入了变分分布映射，通过将渲染图像视为退化的扩散输出，加速分布建模。HumanGaussian[190]设计了结构感知的SDS，用于人体外观和几何的联合优化。LucidDreamer[41]通过带有确定性扩散轨迹的区间评分匹配(Interval Score Matching，ISM)缓解过度平滑，TSM[191]通过生成两条反向DDIM[192]路径扩展ISM，减少累积误差和伪真值不一致。GaussianMotion[193]采用自适应评分蒸馏以平衡真实感与平滑度。(b) 图像到三维。Basak等人[194]引入基于频率的蒸馏损失，从三维扩散先验中提取低频几何，并用二维扩散细化高频纹理。GECO[195]采用两阶段流程，结合变分评分蒸馏[196]和多视图一致性细化，实现亚秒级高质量生成。DreamPhysics[197]利用视频扩散先验学习材质属性，并应用运动蒸馏采样以强调动态线索。

- Multi-View Guidance. The imprecise guidance of SDS often leads to the Janus problem, characterized by multi-face ambiguity. MVGaussian [198], GradeADreamer [199], and GALA3D [200] mitigate this issue by combining SDS with MVDream [201] as a multi-view diffusion prior. GALA3D further incorporates layout-guided Gaussian representations and instance-level scene optimization to achieve high-fidelity object generation and realistic scene interactions. Li et al. [202] also adopt MVDream with hybrid Gaussian-mesh representations, introducing MVControl and SuGaR for controllable text-to-3D generation.

- 多视角引导。SDS的不精确引导常导致“雅努斯问题”，表现为多面体模糊。MVGaussian[198]、GradeADreamer[199]和GALA3D[200]通过将SDS与多视角扩散先验MVDream[201]结合，缓解了该问题。GALA3D进一步引入布局引导的高斯表示和实例级场景优化，实现高保真物体生成和逼真场景交互。Li等人[202]也采用了结合高斯-网格混合表示的MVDream，提出了MVControl和SuGaR，用于可控的文本到三维生成。

B. Feed-Forward-based Methods: Although these optimization-based methods have made significant progress in 3D content generation, they still face challenges such as lengthy optimization times. To address these issues, researchers have begun to shift toward feed-forward methods [203], [204], which involve training on large-scale datasets to directly generate  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  assets.

B. 前馈方法:尽管这些基于优化的方法在三维内容生成方面取得了显著进展，但仍面临优化时间长等挑战。为解决这些问题，研究者开始转向前馈方法[203]，[204]，即通过大规模数据集训练，直接生成 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 资产。

- Latent-space Optimization. These methods learn a latent space from conditional inputs such as RGB images, depth maps, or point clouds via an encoder, which is then decoded into 3D Gaussians. (a) Text-to-3D. Wizadwongsa et al. [205] employ a feed-forward reconstruction encoder with pre-trained models to reduce training cost, refining unstructured latents via postprocessing. They introduce a 2D perceptual rendering loss and a multi-stream transformer correction flow for efficient, high-quality text-conditioned generation. GaussianAnything [206] proposes a scalable framework with a point cloud-structured latent space, supporting multi-modal inputs, geometry-texture disentanglement, and 3D-aware editing. Atlas-Gaussians [207] use a patch-based encoder-decoder with UV sampling and a transformer decoder for efficient, high-fidelity Gaussian shape generation. Turbo3D [208] integrates a four-step, four-view latent diffusion generator with a feed-forward Gaussian reconstructor, employing a dual-teacher scheme for view consistency and photorealism while avoiding pixel-space decoding. (b) Image-to-3D. AGG [209] leverages a pre-trained DINOv2 [46] encoder and dual transformers to decode Gaussian position and texture fields for joint optimization. Hu-manSplat [210] encodes multi-view inputs via a VAE and applies a latent reconstruction transformer interacting with structured human models. Zou et al. [211] design a hybrid triplane Gaussian latent representation from images and point clouds, decoded into 3D objects with transformer-based point and triplane decoders.

- 潜空间优化。这些方法通过编码器从条件输入(如RGB图像、深度图或点云)学习潜空间，然后解码为3D高斯分布。(a) 文本到3D。Wizadwongsa等人[205]采用带有预训练模型的前馈重建编码器以降低训练成本，通过后处理细化无结构潜变量。他们引入了二维感知渲染损失和多流变换器校正流，实现高效且高质量的文本条件生成。GaussianAnything[206]提出了一个可扩展框架，具有点云结构的潜空间，支持多模态输入、几何-纹理解耦和3D感知编辑。Atlas-Gaussians[207]使用基于补丁的编码器-解码器，结合UV采样和变换器解码器，实现高效且高保真的高斯形状生成。Turbo3D[208]集成了四步四视角潜空间扩散生成器和前馈高斯重构器，采用双教师方案保证视角一致性和真实感，同时避免像素空间解码。(b) 图像到3D。AGG[209]利用预训练的DINOv2[46]编码器和双变换器解码高斯位置和纹理场，实现联合优化。Hu-manSplat[210]通过变分自编码器编码多视角输入，并应用潜空间重构变换器与结构化人体模型交互。Zou等人[211]设计了基于图像和点云的混合三平面高斯潜表示，利用基于变换器的点和三平面解码器解码为3D物体。

- Multi-View-based Methods. Incorporating multi-view inputs can substantially improve  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  generation quality. (a) Text-to-3D. Flex3D [212] adopts a two-stage framework: first, a fine-tuned multi-view image diffusion model and a video diffusion model generate a candidate view pool, filtered for quality and consistency via a view selection pipeline; second, selected views are processed by a transformer-based Flexible Reconstruction Model (FlexRM) that outputs 3D Gaussians using tri-plane representations for efficient, detailed generation. LGM [37] employs an asymmetric U-Net for high-resolution 3D Gaussian splatting, enabling expressive, scalable reconstruction from multi-view inputs without triplanes or transformers. (b) Image-to-3D. GS-LRM [39] uses a transformer architecture that patchifies posed images, processes concatenated multi-view tokens through transformer blocks, and decodes per-pixel Gaussian parameters for differentiable rendering. GEOGS3D [213] integrates orthogonal plane decomposition with a diffusion model to synthesize geometry-aware, multi-view consistent novel views, enhancing 3D object reconstruction.

- 多视角方法。引入多视角输入能显著提升 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 生成质量。(a) 文本到3D。Flex3D[212]采用两阶段框架:首先，微调的多视角图像扩散模型和视频扩散模型生成候选视角池，通过视角选择流程筛选质量和一致性；其次，选定视角由基于变换器的灵活重建模型(FlexRM)处理，使用三平面表示输出3D高斯，实现高效且细致的生成。LGM[37]采用非对称U-Net进行高分辨率3D高斯点渲染，支持多视角输入下无三平面或变换器的表达性和可扩展重建。(b) 图像到3D。GS-LRM[39]使用变换器架构，将带姿态的图像分块，处理拼接的多视角token，通过变换器块解码每像素高斯参数，实现可微渲染。GEOGS3D[213]结合正交平面分解和扩散模型，合成几何感知且多视角一致的新视角，提升3D物体重建效果。

- Network Design. Designing feed-forward networks tailored to 3DGS is an active research direction. (a) Text-to-3D. Bright-Dreamer [214] proposes a fast feed-forward framework combining a text-guided shape deformation network with a triplane generator to efficiently predict and optimize Gaussian attributes. (b) Image-to-3D. Lu et al. [215] present the Point-to-Gaussian Generator composed of multiple APP blocks, each with a point feature extractor, projection module, and cross-modal attention, followed by multi-linear heads to decode 3D Gaussians. GRM [38] introduces an upsampler using a windowed self-attention variant to capture non-local cues and enhance high-frequency details. UniGS [216] develops a DETR-like framework treating Gaussians as queries, updated via multi-view cross-attention over input images to mitigate ghosting artifacts.

- 网络设计。设计针对3D高斯点云(3DGS)的前馈网络是活跃的研究方向。(a) 文本到3D。Bright-Dreamer[214]提出快速前馈框架，结合文本引导的形状变形网络和三平面生成器，高效预测和优化高斯属性。(b) 图像到3D。Lu等人[215]提出点到高斯生成器，由多个APP模块组成，每个模块包含点特征提取器、投影模块和跨模态注意力，后接多线性头解码3D高斯。GRM[38]引入基于窗口的自注意力变体上采样器，捕捉非局部信息，增强高频细节。UniGS[216]开发类似DETR的框架，将高斯视为查询，通过多视角跨注意力更新，减少重影伪影。

- Optimization Diffusion Models. These methods directly produce 3D Gaussians by fine-tuning or training diffusion models. (a) Text-to-3D. GVGEN [204] adopts a structured Gaussian volume representation and a coarse-to-fine pipeline, using a candidate pool strategy for pruning and densification to reconstruct high-fidelity 3D scenes from complex text. (b) Image-to-3D. NovelGS [217] generates 3D Gaussians from sparse-view images with novel-view denoising, achieving state-of-the-art reconstruction with consistent, sharp textures. Ouroboros3D [218] jointly trains multiview image generation and 3D reconstruction in a recursive diffusion process, enabling mutual adaptation for robust inference. Cycle3D [219] cyclically integrates 2D diffusion-based generation and feed-forward 3D reconstruction to improve multi-view consistency and texture quality. DiffusionGS [220] outputs Gaussian point clouds at each timestep for view-consistent generation from any direction, introducing a scene-object mixed training strategy to enhance scalability. Hi3D [221] employs a video diffusion paradigm with 3D-aware priors and video-to-video refinement for consistent multi-view reconstruction.

- 优化扩散模型。这些方法通过微调或训练扩散模型直接生成3D高斯。(a) 文本到3D。GVGEN[204]采用结构化高斯体积表示和粗到细流水线，利用候选池策略进行剪枝和稠密化，从复杂文本重建高保真3D场景。(b) 图像到3D。NovelGS[217]从稀疏视角图像生成3D高斯，结合新视角去噪，实现一致且清晰纹理的最先进重建。Ouroboros3D[218]在递归扩散过程中联合训练多视角图像生成和3D重建，实现互适应以增强推理鲁棒性。Cycle3D[219]循环整合基于二维扩散的生成和前馈3D重建，提升多视角一致性和纹理质量。DiffusionGS[220]在每个时间步输出高斯点云，实现任意方向视角一致生成，提出场景-物体混合训练策略以增强可扩展性。Hi3D[221]采用视频扩散范式，结合3D感知先验和视频到视频细化，实现一致的多视角重建。

#### 3.3.2 Scene-level Generation

#### 3.3.2 场景级生成

A. Per-Scene Optimization Methods: These methods optimize each scene individually to generate high-quality 3D Gaussians. (a) Text-to-3D. DreamScene [43] builds on CSD [222], a variant of SDS, by integrating information across multiple timesteps during sampling, obtaining rich semantic guidance from 2D diffusion models. FastScene [223] proposes a Progressive Novel View Inpainting for generating refined views. DreamScene360 [44] uses text-based panorama priors for  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  scene generation with  <img src="https://www.zhihu.com/equation?tex=3\mathrm{{DGS}}" alt="3\mathrm{{DGS}}" class="ee_img tr_noresize" eeimg="1">  to maintain multi-view consistency, and mitigates invisibility in single-view inputs via semantic and geometric regularization. (b) Image-to-3D. Zhong et al. [224] introduce scene-grounding guidance in video diffusion models to improve sequence consistency and handle extrapolation and occlusion. WonderWorld [225] proposes Fast Layered Gaussian Surfels (FLAGS) for scene generation, incorporating guided depth diffusion to reduce geometric distortion. Scene4U [226] combines LLMs and segmentation models to decompose panoramic images into layers, enabling multi-layered  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  scene generation.

A. 场景优化方法:这些方法针对每个场景单独优化，以生成高质量的3D高斯体。(a) 文本到3D。DreamScene [43] 基于CSD [222]，一种SDS的变体，通过在采样过程中整合多个时间步的信息，从2D扩散模型中获得丰富的语义指导。FastScene [223] 提出渐进式新视角修复方法以生成精细视图。DreamScene360 [44] 利用基于文本的全景先验进行 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 场景生成，结合 <img src="https://www.zhihu.com/equation?tex=3\mathrm{{DGS}}" alt="3\mathrm{{DGS}}" class="ee_img tr_noresize" eeimg="1"> 以保持多视角一致性，并通过语义和几何正则化缓解单视图输入的不可见性问题。(b) 图像到3D。Zhong等人[224]在视频扩散模型中引入场景定位指导，以提升序列一致性并处理外推和遮挡问题。WonderWorld [225] 提出快速分层高斯表面点(FLAGS)用于场景生成，结合引导深度扩散以减少几何失真。Scene4U [226] 结合大语言模型(LLMs)和分割模型，将全景图像分解为多层，实现多层次的 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 场景生成。

B. Iterative Generation Methods: These approaches iteratively refine 3D Gaussian representations through repeated rendering and optimization to achieve high-quality scene reconstruction. (a) Text-to-3D. Text2Room [42] combines monocular depth estimation with a text-conditioned inpainting model, aligning scene frames with existing geometry for  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  reconstruction. Text2Immersion [227] progressively generates Gaussian point clouds using 2D diffusion and depth models, followed by refinement via interpolation. RealmDreamer [228] initializes points with a text-to-image generator, lifts them into 3D, computes occlusion volumes, and optimizes across multiple views using an image-conditioned diffusion model. WonderJourney [229] employs LLMs to generate scene descriptions, builds 3D scenes via text-driven point cloud generation, and validates them with VLMs. HoloDreamer [230] addresses global inconsistency and incompleteness by generating high-resolution panoramas for holistic initialization, followed by rapid 3DGS-based reconstruction for view-consistent, enclosed scenes. (b) Image-to-3D. Lucid-Dreamer [231] alternates between Dreaming, which generates multi-view consistent images guided by a point cloud, and Alignment, which integrates new 3D points into a unified scene, producing realistic 3D Gaussian splats from text, RGB, or RGBD inputs. VistaDream [232] enforces multi-view consistency during reverse diffusion sampling to improve temporal coherence. Kang et al. [233] propose a transformer-based latent diffusion model with explicit view geometry constraints, incorporating warped feature maps, epipolar-weighted source features, Plücker ray maps, and camera poses for high-quality novel view generation.

B. 迭代生成方法:这些方法通过反复渲染和优化，迭代地细化3D高斯表示，以实现高质量的场景重建。(a) 文本到3D。Text2Room [42] 结合单目深度估计与文本条件修复模型，将场景帧与现有几何对齐以进行 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 重建。Text2Immersion [227] 利用2D扩散和深度模型逐步生成高斯点云，随后通过插值进行细化。RealmDreamer [228] 使用文本到图像生成器初始化点云，将其提升至3D，计算遮挡体积，并通过图像条件扩散模型在多视角间优化。WonderJourney [229] 利用大语言模型生成场景描述，通过文本驱动的点云生成构建3D场景，并用视觉语言模型(VLMs)进行验证。HoloDreamer [230] 通过生成高分辨率全景图进行整体初始化，解决全局不一致和不完整问题，随后快速基于3DGS的重建实现视角一致的封闭场景。(b) 图像到3D。Lucid-Dreamer [231] 在“梦境”阶段生成由点云引导的多视角一致图像与“对齐”阶段将新3D点整合入统一场景之间交替进行，从文本、RGB或RGBD输入生成逼真的3D高斯斑点。VistaDream [232] 在反向扩散采样过程中强制多视角一致性以提升时间连贯性。Kang等人[233]提出基于变换器的潜在扩散模型，结合显式视角几何约束，融合扭曲特征图、极线加权源特征、Plücker射线图和相机位姿，实现高质量新视角生成。

C. Feed-Forward-based Methods: Similar to object-level generation, feed-forward approaches have also been explored.

C. 前馈方法:类似于对象级生成，前馈方法也被探索应用。

- Network Design. (a) Text-to-3D. TextSplat [234] introduces Text-Guided Semantic Fusion Module to integrate multi-source semantic features under sentence-level guidance, enhancing geometry semantic consistency. (b) Image-to-3D. PixelSplat [235] employs a multi-view epipolar transformer to address scale ambiguity, while MVSplat [4] and Splatter360 [236] construct planar or spherical cost volumes for improved geometry estimation. MVSplat360 [237] leverages pre-trained video diffusion for 3D-consistent views. GaussianCity [238] adopts BEV-Point as a compact intermediate representation with a spatial-aware decoder. SelfSplat [239] introduces matching-aware pose estimation and depth refinement to ensure geometric consistency. CATSplat [240] fuses vision-language text features with point cloud features via transformer blocks, and OmniSplat [241] uses a Yin-Yang grid to reduce distortion.

- 网络设计。(a) 文本到3D。TextSplat [234] 引入文本引导的语义融合模块，在句子级指导下整合多源语义特征，增强几何语义一致性。(b) 图像到3D。PixelSplat [235] 采用多视角极线变换器解决尺度歧义，MVSplat [4] 和 Splatter360 [236] 构建平面或球面代价体积以提升几何估计。MVSplat360 [237] 利用预训练视频扩散实现3D一致视图。GaussianCity [238] 采用BEV-Point作为紧凑的中间表示，配备空间感知解码器。SelfSplat [239] 引入匹配感知的位姿估计和深度细化以确保几何一致性。CATSplat [240] 通过变换器模块融合视觉语言文本特征与点云特征，OmniSplat [241] 使用阴阳格网减少失真。

- Optimization Diffusion Models. (a) Text-to-3D. Prometheus [242] sequentially trains GS-VAE and MV-LDM for direct text-to- 3D scene generation. VideoRFSplat [243] couples a pose generation model with a pretrained video generator via communication blocks to produce multi-view images and camera poses. (b) Image-to-3D. ViewCrafter [244] employs a point-conditioned video diffusion model for high-quality 3D scenes, while Wonderland [245] integrates diverse camera trajectories via a dual-branch conditioning mechanism for view-consistent latents. GGS [246] combines 3D representations with latent video diffusion and a custom decoder to synthesize scenes from feature fields. VideoScene [247] distills video diffusion through a 3D-aware leap flow strategy for one-step generation. SceneSplatter [248] adopts a momentum-based paradigm to balance generative priors and existing scene information during generation.

- 优化扩散模型。(a) 文本到3D。Prometheus [242] 依次训练GS-VAE和MV-LDM，实现直接文本到3D场景生成。VideoRFSplat [243] 通过通信模块将位姿生成模型与预训练视频生成器耦合，生成多视角图像和相机位姿。(b) 图像到3D。ViewCrafter [244] 采用点条件视频扩散模型生成高质量3D场景，Wonderland [245] 通过双分支条件机制整合多样相机轨迹，实现视角一致的潜变量。GGS [246] 结合3D表示、潜在视频扩散及定制解码器，从特征场合成场景。VideoScene [247] 通过3D感知跳跃流策略蒸馏视频扩散，实现一步生成。SceneSplatter [248] 采用动量范式，在生成过程中平衡生成先验与现有场景信息。

### 3.4 Other Application Tasks

### 3.4 其他应用任务

#### 3.4.1 Human Avatar

#### 3.4.1 人类虚拟形象

Human avatars, digital representations of users in virtual spaces, enable immersive interaction across gaming, virtual meetings, and the metaverse. 3DGS-based avatar modeling can be divided into two main directions: 1) Body-based avatars [249], [250], [251], [252], [253], [254] typically rely on parametric body models like SMPL [255] or SMPL-X [256] to guide canonical Gaussian initialization. Methods such as HUGS [253] and Animatable Gaussians [249] extend beyond rigid skeletons by introducing pose-conditioned deformation to handle garments and fine motions. Others like D3GA [257] and GauHuman [258] enhance realism with cage-based deformation or efficient monocular reconstruction to move beyond rigid templates. Despite progress, body-based methods still face challenges in handling loose clothing, fast motions, and generalization to unseen scenes. 2) Head-based avatars [6], [259], [260], [261] emphasize fine-grained reconstruction of facial geometry, expressions, and speech-driven dynamics. Many approaches combine FLAME [262] with deformable Gaussians to capture subtle expressions. For instance, Gaussian Head Avatar [263] replaces traditional LBS with MLP-based displacement, while FlashAvatar [113] achieves 300 FPS real-time rendering. Despite progress, Head-based methods still face challenges in handling subtle facial expressions, uneven illumination, and low-quality pose.

人类虚拟形象是用户在虚拟空间中的数字化代表，支持游戏、虚拟会议和元宇宙中的沉浸式交互。基于3DGS的虚拟形象建模主要分为两大方向:1)基于身体的虚拟形象[249]，[250]，[251]，[252]，[253]，[254]通常依赖于参数化身体模型如SMPL(Skinned Multi-Person Linear model)[255]或SMPL-X[256]来引导标准高斯初始化。方法如HUGS[253]和Animatable Gaussians[249]通过引入姿态条件变形，超越刚性骨骼，处理服装和细微动作。其他如D3GA[257]和GauHuman[258]通过笼形变形或高效单目重建提升真实感，突破刚性模板限制。尽管取得进展，基于身体的方法仍面临处理宽松服装、快速动作及对未见场景泛化的挑战。2)基于头部的虚拟形象[6]，[259]，[260]，[261]强调面部几何、表情及语音驱动动态的细粒度重建。许多方法结合FLAME[262]与可变形高斯捕捉细微表情。例如，Gaussian Head Avatar[263]用基于MLP的位移替代传统的线性混合蒙皮(LBS)，而FlashAvatar[113]实现了300帧每秒的实时渲染。尽管进展显著，基于头部的方法仍面临细微表情、光照不均及低质量姿态的挑战。

#### 3.4.2 SLAM

#### 3.4.2 同时定位与地图构建(SLAM)

Simultaneous Localization and Mapping (SLAM) is a foundational task in robotics and vision, aiming to estimate camera poses and reconstruct  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  environments simultaneously. We broadly categorize 3DGS-based SLAM [264], [265], [266], [267] into two directions: geometry-based SLAM and semantics-aware SLAM. The former includes RGB-D SLAM and RGB SLAM, depending on depth availability. RGB-D SLAM methods (e.g., GS-SLAM [5], SplaTAM [268]) leverage accurate depth to construct dense Gaussian maps and enable reliable tracking via silhouette rendering or Gaussian matching. In contrast, RGB SLAM (e.g., MonoGS [269], Photo-SLAM [270]) must infer geometry through multi-view optimization or depth prediction, posing challenges in outdoor or dynamic scenes due to less stable depth cues. Beyond geometry, semantic SLAM incorporates scene understanding to enable richer  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  maps for downstream tasks like navigation and interaction. Early works like SGS-SLAM [271] embed semantic colors into Gaussians, while more recent methods (e.g., OpenGS-SLAM [272],  <img src="https://www.zhihu.com/equation?tex={\mathrm{{GS}}}^{3}" alt="{\mathrm{{GS}}}^{3}" class="ee_img tr_noresize" eeimg="1">  LAM [273], LEGS [274]) introduce learned semantic features or language-guided Gaussians to enhance high-level reasoning. Such semantic integration improves robustness, particularly under sensor noise or ambiguous geometry. Despite progress, open challenges remain, like SLAM under sparse views, scalable memory, and consistent semantics in large-scale scenes.

同时定位与地图构建(SLAM)是机器人与视觉领域的基础任务，旨在同时估计相机位姿并重建 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 环境。我们将基于3DGS的SLAM[264]，[265]，[266]，[267]大致分为两类:基于几何的SLAM和语义感知SLAM。前者包括RGB-D SLAM和RGB SLAM，取决于深度信息的可用性。RGB-D SLAM方法(如GS-SLAM[5]、SplaTAM[268])利用精确深度构建稠密高斯地图，并通过轮廓渲染或高斯匹配实现可靠跟踪。相比之下，RGB SLAM(如MonoGS[269]、Photo-SLAM[270])需通过多视角优化或深度预测推断几何，在户外或动态场景中因深度线索不稳定而面临挑战。除几何外，语义SLAM融合场景理解，生成更丰富的 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 地图，支持导航和交互等下游任务。早期工作如SGS-SLAM[271]将语义颜色嵌入高斯中，近期方法(如OpenGS-SLAM[272]、 <img src="https://www.zhihu.com/equation?tex={\mathrm{{GS}}}^{3}" alt="{\mathrm{{GS}}}^{3}" class="ee_img tr_noresize" eeimg="1">  LAM[273]、LEGS[274])引入学习的语义特征或语言引导的高斯以增强高层推理。此类语义整合提升了鲁棒性，尤其在传感器噪声或几何模糊情况下。尽管取得进展，仍存在稀疏视角下SLAM、大规模场景的可扩展内存及语义一致性等开放挑战。

#### 3.4.3 3DGS Object Detection

#### 3.4.3 3DGS目标检测

3D object detection focuses on identifying and localizing objects in 3D space. Similar to segmentation, detection methods can be categorized into prompt-based and generic approaches. Prompt-based detection, often referred to as open-vocabulary detection or visual grounding, takes textual inputs (e.g., category names or language expressions) and outputs object locations. Most 3DGS-based segmentation methods also support this type of detection by generating the maximum response point, and are thus not detailed here. Specifically, SpatialReasoner [275] combines LLM-driven spatial reasoning with a hierarchical feature field enhanced by visual properties. It distills CLIP and SAM features and leverages fine-tuned LLMs to infer spatial instructions, enabling precise localization of target instances based on relational language cues. For generic object detection, representative methods include: Gaussian-Det [276], which models objects continuously using input Gaussians as feature descriptors across partial surfaces; 3DGS-DET [277], which incorporates boundary guidance and box-focused sampling to refine Gaussian distributions and suppress background noise; and MATT-GS [278], which employs masked attention 3D Gaussian Splatting to enhance object localization.

3D目标检测聚焦于识别和定位三维空间中的物体。类似于分割，检测方法可分为基于提示和通用方法。基于提示的检测，常称为开放词汇检测或视觉定位，接受文本输入(如类别名称或语言表达)并输出物体位置。大多数基于3DGS的分割方法也支持此类检测，通过生成最大响应点实现，故此处不再详述。具体而言，SpatialReasoner[275]结合大语言模型(LLM)驱动的空间推理与层次化特征场，增强视觉属性。它蒸馏CLIP和SAM特征，利用微调的LLM推断空间指令，实现基于关系语言线索的目标实例精确定位。通用目标检测的代表方法包括:Gaussian-Det[276]，利用输入高斯作为特征描述符对部分表面连续建模物体；3DGS-DET[277]，结合边界引导和盒子聚焦采样以优化高斯分布并抑制背景噪声；以及MATT-GS[278]，采用掩码注意力3D高斯点云增强物体定位。

## 4 PERFORMANCE COMPARISON

## 4 性能比较

This section compares different methods across tasks. For each task, we introduce the datasets and evaluation metrics, followed by detailed result tables. Fig. 3 shows examples from 13 commonly used datasets in segmentation, editing, and generation.

本节比较不同方法在各任务上的表现。针对每个任务，我们介绍数据集和评估指标，随后给出详细结果表。图3展示了13个常用分割、编辑和生成数据集的示例。

### 4.1 Performance Benchmarking: 3DGS Segmentation

### 4.1 性能基准测试:3DGS分割

- Datasets. Table 1 summarizes the key features of the datasets for 3DGS segmentation, followed by detailed descriptions.

- 数据集。表1总结了3DGS分割数据集的关键特征，随后附有详细描述。

ScanNet [280] contains 1,513 RGB-D sequences from 707 indoor scenes, offering  <img src="https://www.zhihu.com/equation?tex={2.5}\mathrm{M}" alt="{2.5}\mathrm{M}" class="ee_img tr_noresize" eeimg="1">  frames with  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  reconstructions, camera poses, and dense semantic/instance annotations. It includes  <img src="https://www.zhihu.com/equation?tex={36}\mathrm{\;K}" alt="{36}\mathrm{\;K}" class="ee_img tr_noresize" eeimg="1">  labeled objects across 20 categories and is a standard benchmark for indoor scene understanding.

ScanNet [280] 包含来自707个室内场景的1,513个RGB-D序列，提供 <img src="https://www.zhihu.com/equation?tex={2.5}\mathrm{M}" alt="{2.5}\mathrm{M}" class="ee_img tr_noresize" eeimg="1"> 帧，带有 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 重建、相机位姿以及密集的语义/实例标注。它包括跨20个类别的 <img src="https://www.zhihu.com/equation?tex={36}\mathrm{\;K}" alt="{36}\mathrm{\;K}" class="ee_img tr_noresize" eeimg="1"> 个标注对象，是室内场景理解的标准基准。

Replica [285] is a high-quality indoor 3D dataset with dense geometry, HDR textures, accurate material properties (e.g., glass, mirrors), planar segmentation, and semantic/instance annotations, supporting realistic simulation and scene understanding.

Replica [285] 是一个高质量的室内三维数据集，具有密集几何结构、高动态范围(HDR)纹理、准确的材质属性(如玻璃、镜面)、平面分割以及语义/实例标注，支持逼真的仿真和场景理解。

NVOS [279] is a segmentation-focused extension of LLFF with undistorted images, mask and scribble annotations, designed for language-guided 2D-to-3D segmentation.

NVOS [279] 是LLFF的一个以分割为重点的扩展，包含无畸变图像、掩码和涂鸦标注，设计用于语言引导的二维到三维分割。

Mip-NeRF [281] provides unbounded real-world scenes with precise poses for  <img src="https://www.zhihu.com/equation?tex={360}^{ \circ  }" alt="{360}^{ \circ  }" class="ee_img tr_noresize" eeimg="1">  novel view synthesis.

Mip-NeRF [281] 提供具有精确位姿的无限制真实世界场景，用于 <img src="https://www.zhihu.com/equation?tex={360}^{ \circ  }" alt="{360}^{ \circ  }" class="ee_img tr_noresize" eeimg="1"> 的新视角合成。

SPIn-NeRF [67] contains 10 real-world forward-facing scenes with annotated object masks. Each scene includes 60 training images with the object and 40 test images without the object.

SPIn-NeRF [67] 包含10个真实世界的前向场景，带有标注的物体掩码。每个场景包括60张带物体的训练图像和40张不带物体的测试图像。

3D-OVS [63] is a benchmark for open-vocabulary 3D segmentation with language-aligned semantic labels, containing 10 scenes with around  <img src="https://www.zhihu.com/equation?tex={30}^{ \circ  }" alt="{30}^{ \circ  }" class="ee_img tr_noresize" eeimg="1">  images each.

3D-OVS [63] 是一个开放词汇三维分割的基准，带有语言对齐的语义标签，包含10个场景，每个场景约有 <img src="https://www.zhihu.com/equation?tex={30}^{ \circ  }" alt="{30}^{ \circ  }" class="ee_img tr_noresize" eeimg="1"> 张图像。

LERF-OVS [26] contains open-vocabulary semantic annotations for 4 scenes derived from the LERF dataset [62].

LERF-OVS [26] 包含从LERF数据集[62]派生的4个场景的开放词汇语义标注。

LERF-Mask [22] contains semantic annotations of three scenes from LERF dataset [62] with a total of 23 prompts.

LERF-Mask [22] 包含来自LERF数据集[62]的三个场景的语义标注，共计23个提示。

Ref-LERF [7] extends the LERF dataset [62] with 295 language expressions referring to 59 objects in 4 scenes, emphasizing spatial relations to support referring segmentation [288], [289] in 3DGS.

Ref-LERF [7] 扩展了LERF数据集[62]，包含295条语言表达，指代4个场景中的59个对象，强调空间关系以支持三维图形分割(3DGS)中的指称分割[288]，[289]。

SceneSplat-7K [127] is the first large-scale, high-quality 3DGS dataset for indoor environments, containing 7,916 scenes aggregated from seven well-established datasets.

SceneSplat-7K [127] 是首个大规模高质量的室内环境三维图形分割(3DGS)数据集，包含从七个知名数据集中汇总的7,916个场景。

TABLE 1: Statistics of representative 3DGS segmentation datasets. See §4.1 for more detailed descriptions.

表1:代表性三维图形分割(3DGS)数据集的统计信息。详见§4.1节。

<table><tr><td>Dataset</td><td>Venue</td><td>#Scene</td><td>#Views(Avg.)</td><td>Characterization</td></tr><tr><td>ScanNet [280]</td><td>[CVPR'17]</td><td>1513</td><td>1500</td><td>Large-scale RGB-D scans with 3D poses and semantics for advanced scene understanding.</td></tr><tr><td>Replica [285]</td><td>[ArXiv'19]</td><td>18</td><td>175</td><td>High-quality indoor scans with geometry, HDR textures, and rich semantic labels.</td></tr><tr><td>NVOS [279]</td><td>[CVPR'21]</td><td>8</td><td>36</td><td>Built on LLFF with undistorted images, annotated with masks and scribbles for segmentation tasks.</td></tr><tr><td>Mip-NeRF 360 [281]</td><td>[CVPR'22]</td><td>9</td><td>215</td><td>Focusing on capturing complex lighting, geometry, and texture details.</td></tr><tr><td>SPIn-NeRF [67]</td><td>[CVPR'23]</td><td>10</td><td>100</td><td>Providing challenging real-world scenes with views both with and without a target object.</td></tr><tr><td>3D-OVS [63]</td><td>[NeurlPS'23]</td><td>10</td><td>30</td><td>Including high-quality 3D objects spanning diverse categories with language-aligned semantic labels.</td></tr><tr><td>LERF-OVS [26]</td><td>[CVPR'24]</td><td>4</td><td>200</td><td>An extended version of LERF dataset with ground truth mask annotations for open-vocabulary segmentation.</td></tr><tr><td>LERF-Mask [22]</td><td>[ECCV'24]</td><td>3</td><td>200</td><td>Containing semantic annotations of three scenes from LERF dataset [62] with a total of 23 prompts.</td></tr><tr><td>Ref-LERF [7]</td><td>[ICML'25]</td><td>4</td><td>200</td><td>Focusing on spatial relationships, annotated with natural language expressions for referring 3DGS segmentation.</td></tr><tr><td>SceneSplat-7K [127]</td><td>[ICCV'25]</td><td>7k</td><td>-</td><td>The first large-scale, high-quality 3DGS dataset for indoor environments boosting scene understanding research.</td></tr><tr><td>SceneSplat-49K [286]</td><td>[ArXiv'25]</td><td>49k</td><td>-</td><td>Containing diverse indoor and outdoor scenes, featuring complex, high-quality full scenes from multiple sources.</td></tr></table>

<table><tbody><tr><td>数据集</td><td>会议</td><td>场景数</td><td>平均视图数</td><td>特征描述</td></tr><tr><td>ScanNet [280]</td><td>[CVPR'17]</td><td>1513</td><td>1500</td><td>大规模RGB-D扫描，包含三维姿态和语义信息，用于高级场景理解。</td></tr><tr><td>Replica [285]</td><td>[ArXiv'19]</td><td>18</td><td>175</td><td>高质量室内扫描，具备几何结构、高动态范围纹理(HDR textures)及丰富的语义标签。</td></tr><tr><td>NVOS [279]</td><td>[CVPR'21]</td><td>8</td><td>36</td><td>基于LLFF，提供无畸变图像，带有用于分割任务的掩码和涂鸦标注。</td></tr><tr><td>Mip-NeRF 360 [281]</td><td>[CVPR'22]</td><td>9</td><td>215</td><td>专注于捕捉复杂的光照、几何和纹理细节。</td></tr><tr><td>SPIn-NeRF [67]</td><td>[CVPR'23]</td><td>10</td><td>100</td><td>提供具有挑战性的真实场景，包含有目标物体和无目标物体的视图。</td></tr><tr><td>3D-OVS [63]</td><td>[NeurlPS'23]</td><td>10</td><td>30</td><td>包含多类别高质量三维物体，配有语言对齐的语义标签。</td></tr><tr><td>LERF-OVS [26]</td><td>[CVPR'24]</td><td>4</td><td>200</td><td>LERF数据集的扩展版本，带有用于开放词汇分割的真实掩码标注。</td></tr><tr><td>LERF-Mask [22]</td><td>[ECCV'24]</td><td>3</td><td>200</td><td>包含LERF数据集[62]中三个场景的语义标注，共23条提示语。</td></tr><tr><td>Ref-LERF [7]</td><td>[ICML'25]</td><td>4</td><td>200</td><td>聚焦空间关系，使用自然语言表达进行三维图形分割的指称标注。</td></tr><tr><td>SceneSplat-7K [127]</td><td>[ICCV'25]</td><td>7k</td><td>-</td><td>首个大规模高质量室内三维图形分割(3DGS)数据集，推动场景理解研究。</td></tr><tr><td>SceneSplat-49K [286]</td><td>[ArXiv'25]</td><td>49k</td><td>-</td><td>包含多样的室内外场景，汇集多个来源的复杂高质量完整场景。</td></tr></tbody></table>

![bo_d39q8b77aajc73907iag_10_135_477_750_1272_0.jpg](https://raw.githubusercontent.com/brandfucker/Markdown4Zhihu/master/Data/A_Survey_on_3D_Gaussian_Splatting_Applications/bo_d39q8b77aajc73907iag_10_135_477_750_1272_0.jpg)

Fig. 3: Examples from 13 commonly used datasets for segmentation, editing, and generation.

图3:13个常用用于分割、编辑和生成的数据集示例。

SceneSplat-49K [286] is a curated dataset of diverse indoor and outdoor scenes, featuring complex, high-quality full-scene 3DGS reconstructions from multiple sources.

SceneSplat-49K [286] 是一个精心整理的多样化室内外场景数据集，包含来自多个来源的复杂、高质量全场景三维几何重建(3DGS)。

- Metrics. Common evaluation metrics in the segmentation domain are outlined below:

- 指标。以下列出了分割领域常用的评估指标:

Mean Intersection over Union (mIoU) measures segmentation accuracy by computing the average IoU across all classes, where IoU is the ratio between the intersection and the union of predicted and ground truth regions for each class.

平均交并比(mIoU)通过计算所有类别的IoU平均值来衡量分割准确性，其中IoU是每个类别预测区域与真实区域交集与并集的比率。

Mean Accuracy (mAcc) measures the proportion of correctly predicted pixels or instances over the total, reflecting the overall correctness of the segmentation or classification results.

平均准确率(mAcc)衡量正确预测的像素或实例占总数的比例，反映分割或分类结果的整体正确性。

TABLE 2: Quantitative 3D instance segmentation on Replica [285] and LERF-Mask [22]. Following [20], we report mIoU and F-score on Replica, and mIoU and mBIoU on LERF-Mask.

表2:在Replica [285]和LERF-Mask [22]上的定量三维实例分割。遵循文献[20]，我们报告了Replica上的mIoU和F分数，以及LERF-Mask上的mIoU和mBIoU。

<table><tr><td rowspan="2">Method</td><td rowspan="2">Venue</td><td rowspan="2">Type</td><td colspan="2">Replica [285]</td><td colspan="2">LERF-Mask [22]</td></tr><tr><td/><td> <img src="https://www.zhihu.com/equation?tex=\mathbf{{mIoU}\left( \% \right) \;F - {score}\left( \% \right) }" alt="\mathbf{{mIoU}\left( \% \right) \;F - {score}\left( \% \right) }" class="ee_img tr_noresize" eeimg="1"> </td><td> <img src="https://www.zhihu.com/equation?tex=\mathbf{{mloU}}\left( \% \right)" alt="\mathbf{{mloU}}\left( \% \right)" class="ee_img tr_noresize" eeimg="1"> </td><td> <img src="https://www.zhihu.com/equation?tex=\mathbf{{mBIoU}}\left( \% \right)" alt="\mathbf{{mBIoU}}\left( \% \right)" class="ee_img tr_noresize" eeimg="1"> </td></tr><tr><td>GaussianGrouping [22]</td><td>[ECCV’24]</td><td>Pre-Process.</td><td>23.6</td><td>30.4</td><td>72.8</td><td>67.6</td></tr><tr><td>OmniSeg3D [18]</td><td>[CVPR'24]</td><td>Post-Process.</td><td>39.1</td><td>35.9</td><td>74.7</td><td>71.8</td></tr><tr><td>Gaga [19]</td><td>[ArXiv'24]</td><td>Pre-Process.</td><td>-</td><td>-</td><td>74.7</td><td>72.2</td></tr><tr><td>Unified-Lift [20]</td><td>[CVPR’25]</td><td>End-to-End</td><td>41.6</td><td>43.9</td><td>80.9</td><td>77.1</td></tr></table>

<table><tbody><tr><td rowspan="2">方法</td><td rowspan="2">会议</td><td rowspan="2">类型</td><td colspan="2">Replica [285]</td><td colspan="2">LERF-Mask [22]</td></tr><tr><td></td><td> <img src="https://www.zhihu.com/equation?tex=\mathbf{{mIoU}\left( \% \right) \;F - {score}\left( \% \right) }" alt="\mathbf{{mIoU}\left( \% \right) \;F - {score}\left( \% \right) }" class="ee_img tr_noresize" eeimg="1"> </td><td> <img src="https://www.zhihu.com/equation?tex=\mathbf{{mloU}}\left( \% \right)" alt="\mathbf{{mloU}}\left( \% \right)" class="ee_img tr_noresize" eeimg="1"> </td><td> <img src="https://www.zhihu.com/equation?tex=\mathbf{{mBIoU}}\left( \% \right)" alt="\mathbf{{mBIoU}}\left( \% \right)" class="ee_img tr_noresize" eeimg="1"> </td></tr><tr><td>GaussianGrouping [22]</td><td>[ECCV’24]</td><td>预处理</td><td>23.6</td><td>30.4</td><td>72.8</td><td>67.6</td></tr><tr><td>OmniSeg3D [18]</td><td>[CVPR'24]</td><td>后处理</td><td>39.1</td><td>35.9</td><td>74.7</td><td>71.8</td></tr><tr><td>Gaga [19]</td><td>[ArXiv'24]</td><td>预处理</td><td>-</td><td>-</td><td>74.7</td><td>72.2</td></tr><tr><td>Unified-Lift [20]</td><td>[CVPR’25]</td><td>端到端</td><td>41.6</td><td>43.9</td><td>80.9</td><td>77.1</td></tr></tbody></table>

TABLE 3: Quantitative 3D interactive segmentation on NVOS [287] and SPIn-NeRF [67] in terms of mIoU and mAcc.

表3:基于mIoU和mAcc指标对NVOS [287]和SPIn-NeRF [67]的定量三维交互式分割结果。

<table><tr><td rowspan="2">Method</td><td rowspan="2">Venue</td><td rowspan="2">Train</td><td rowspan="2">Input Prompt</td><td colspan="2">NVOS [287]</td><td colspan="2">SPIn-NeRF [67]</td></tr><tr><td> <img src="https://www.zhihu.com/equation?tex=\mathbf{{mloU}}\left( \% \right)" alt="\mathbf{{mloU}}\left( \% \right)" class="ee_img tr_noresize" eeimg="1"> </td><td> <img src="https://www.zhihu.com/equation?tex=\mathbf{{mAcc}}\left( \% \right)" alt="\mathbf{{mAcc}}\left( \% \right)" class="ee_img tr_noresize" eeimg="1"> </td><td> <img src="https://www.zhihu.com/equation?tex=\mathbf{{mloU}}\left( \% \right)" alt="\mathbf{{mloU}}\left( \% \right)" class="ee_img tr_noresize" eeimg="1"> </td><td> <img src="https://www.zhihu.com/equation?tex=\mathbf{{mAcc}}\left( \% \right)" alt="\mathbf{{mAcc}}\left( \% \right)" class="ee_img tr_noresize" eeimg="1"> </td></tr><tr><td>SA3D-GS [104]</td><td>[NeurIPS'23]</td><td>✓</td><td>2D Click/Text</td><td>90.7</td><td>98.3</td><td>93.2</td><td>99.1</td></tr><tr><td>SAGD [23]</td><td>[ArXiv’24]</td><td>✘</td><td>2D Click/Text</td><td>90.4</td><td>98.2</td><td>89.9</td><td>98.7</td></tr><tr><td>FlashSplat [113]</td><td>[ECCV'24]</td><td>✘</td><td>2D Click</td><td>91.8</td><td>98.6</td><td>-</td><td>-</td></tr><tr><td>Click-Gaussian [24]</td><td>[ECCV’24]</td><td>✓</td><td>2D Click</td><td>-</td><td>-</td><td>94.0</td><td>-</td></tr><tr><td>GaussianCut [29]</td><td>[NeurIPS’24]</td><td>✘</td><td>2D Click/Text</td><td>92.5</td><td>98.4</td><td>92.9</td><td>99.2</td></tr><tr><td>SAGA [30]</td><td>[AAAI'25]</td><td>✓</td><td>2D Click/Text</td><td>90.9</td><td>98.3</td><td>88.0</td><td>98.5</td></tr><tr><td>COB-GS [111]</td><td>[CVPR'25]</td><td>✓</td><td>Text</td><td>92.1</td><td>98.6</td><td>-</td><td>-</td></tr><tr><td>iSegMan [25]</td><td>[CVPR125]</td><td>✘</td><td>2D Click</td><td>92.0</td><td>98.4</td><td>92.4</td><td>99.1</td></tr><tr><td>LUDVIG [116]</td><td>[ICCV'25]</td><td>✘</td><td>Text</td><td>92.4</td><td>-</td><td>93.8</td><td>-</td></tr></table>

<table><tbody><tr><td rowspan="2">方法</td><td rowspan="2">会议</td><td rowspan="2">训练</td><td rowspan="2">输入提示</td><td colspan="2">NVOS [287]</td><td colspan="2">SPIn-NeRF [67]</td></tr><tr><td> <img src="https://www.zhihu.com/equation?tex=\mathbf{{mloU}}\left( \% \right)" alt="\mathbf{{mloU}}\left( \% \right)" class="ee_img tr_noresize" eeimg="1"> </td><td> <img src="https://www.zhihu.com/equation?tex=\mathbf{{mAcc}}\left( \% \right)" alt="\mathbf{{mAcc}}\left( \% \right)" class="ee_img tr_noresize" eeimg="1"> </td><td> <img src="https://www.zhihu.com/equation?tex=\mathbf{{mloU}}\left( \% \right)" alt="\mathbf{{mloU}}\left( \% \right)" class="ee_img tr_noresize" eeimg="1"> </td><td> <img src="https://www.zhihu.com/equation?tex=\mathbf{{mAcc}}\left( \% \right)" alt="\mathbf{{mAcc}}\left( \% \right)" class="ee_img tr_noresize" eeimg="1"> </td></tr><tr><td>SA3D-GS [104]</td><td>[NeurIPS'23]</td><td>✓</td><td>二维点击/文本</td><td>90.7</td><td>98.3</td><td>93.2</td><td>99.1</td></tr><tr><td>SAGD [23]</td><td>[ArXiv’24]</td><td>✘</td><td>二维点击/文本</td><td>90.4</td><td>98.2</td><td>89.9</td><td>98.7</td></tr><tr><td>FlashSplat [113]</td><td>[ECCV'24]</td><td>✘</td><td>二维点击</td><td>91.8</td><td>98.6</td><td>-</td><td>-</td></tr><tr><td>Click-Gaussian [24]</td><td>[ECCV’24]</td><td>✓</td><td>二维点击</td><td>-</td><td>-</td><td>94.0</td><td>-</td></tr><tr><td>GaussianCut [29]</td><td>[NeurIPS’24]</td><td>✘</td><td>二维点击/文本</td><td>92.5</td><td>98.4</td><td>92.9</td><td>99.2</td></tr><tr><td>SAGA [30]</td><td>[AAAI'25]</td><td>✓</td><td>二维点击/文本</td><td>90.9</td><td>98.3</td><td>88.0</td><td>98.5</td></tr><tr><td>COB-GS [111]</td><td>[CVPR'25]</td><td>✓</td><td>文本</td><td>92.1</td><td>98.6</td><td>-</td><td>-</td></tr><tr><td>iSegMan [25]</td><td>[CVPR125]</td><td>✘</td><td>二维点击</td><td>92.0</td><td>98.4</td><td>92.4</td><td>99.1</td></tr><tr><td>LUDVIG [116]</td><td>[ICCV'25]</td><td>✘</td><td>文本</td><td>92.4</td><td>-</td><td>93.8</td><td>-</td></tr></tbody></table>

TABLE 4: Performance comparison of open-vocabulary 3D semantic segmentation on ScanNet [280] benchmark. The

表4:在ScanNet [280]基准上的开放词汇3D语义分割性能比较。

evaluation metrics are mIoU and mAcc, respectively.

评估指标分别为mIoU和mAcc。

<table><tr><td rowspan="2"> <img src="https://www.zhihu.com/equation?tex=\mathbf{{Method}}" alt="\mathbf{{Method}}" class="ee_img tr_noresize" eeimg="1"> </td><td rowspan="2">Venue</td><td rowspan="2">Evaluation</td><td colspan="2">19 classes</td><td colspan="2">15 classes</td><td colspan="2">10 classes</td></tr><tr><td>mIoU</td><td>mAcc</td><td>mloU</td><td>mAcc</td><td>mloU</td><td>mAcc</td></tr><tr><td>LangSplat [26]</td><td>[CVPR'24]</td><td>Point</td><td>3.8</td><td>9.1</td><td>5.4</td><td>13.2</td><td>8.4</td><td>22.1</td></tr><tr><td>LEGaussians [84]</td><td>[CVPR'24]</td><td>Point</td><td>3.8</td><td>10.9</td><td>9.0</td><td>22.2</td><td>12.8</td><td>28.6</td></tr><tr><td>OpenGaussian [32]</td><td>[NeurIPS’24]</td><td>Point</td><td>24.7</td><td>41.5</td><td>30.1</td><td>48.3</td><td>38.3</td><td>55.2</td></tr><tr><td>InstanceGaussian [31]</td><td>[CVPR'25]</td><td>Point</td><td>40.7</td><td>54.0</td><td>42.5</td><td>59.2</td><td>47.9</td><td>64.0</td></tr><tr><td>PanoGS [112]</td><td>[CVPR'25]</td><td>Point</td><td>50.7</td><td>70.2</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Dr. Splat [119]</td><td>[CVPR'25]</td><td>Gaussian</td><td>28.0</td><td>44.6</td><td>38.2</td><td>60.4</td><td>47.2</td><td>68.9</td></tr><tr><td>CAGS [103]</td><td>[ArXiv'25]</td><td>Gaussian</td><td>32.6</td><td>48.9</td><td>41.1</td><td>62.0</td><td>54.8</td><td>75.9</td></tr></table>

<table><tbody><tr><td rowspan="2"> <img src="https://www.zhihu.com/equation?tex=\mathbf{{Method}}" alt="\mathbf{{Method}}" class="ee_img tr_noresize" eeimg="1"> </td><td rowspan="2">会议</td><td rowspan="2">评估</td><td colspan="2">19 类</td><td colspan="2">15 类</td><td colspan="2">10 类</td></tr><tr><td>平均交并比 (mIoU)</td><td>平均准确率 (mAcc)</td><td>平均交并比 (mIoU)</td><td>平均准确率 (mAcc)</td><td>平均交并比 (mIoU)</td><td>平均准确率 (mAcc)</td></tr><tr><td>LangSplat [26]</td><td>[CVPR'24]</td><td>点</td><td>3.8</td><td>9.1</td><td>5.4</td><td>13.2</td><td>8.4</td><td>22.1</td></tr><tr><td>LEGaussians [84]</td><td>[CVPR'24]</td><td>点</td><td>3.8</td><td>10.9</td><td>9.0</td><td>22.2</td><td>12.8</td><td>28.6</td></tr><tr><td>OpenGaussian [32]</td><td>[NeurIPS’24]</td><td>点</td><td>24.7</td><td>41.5</td><td>30.1</td><td>48.3</td><td>38.3</td><td>55.2</td></tr><tr><td>InstanceGaussian [31]</td><td>[CVPR'25]</td><td>点</td><td>40.7</td><td>54.0</td><td>42.5</td><td>59.2</td><td>47.9</td><td>64.0</td></tr><tr><td>PanoGS [112]</td><td>[CVPR'25]</td><td>点</td><td>50.7</td><td>70.2</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Dr. Splat [119]</td><td>[CVPR'25]</td><td>高斯</td><td>28.0</td><td>44.6</td><td>38.2</td><td>60.4</td><td>47.2</td><td>68.9</td></tr><tr><td>CAGS [103]</td><td>[ArXiv'25]</td><td>高斯</td><td>32.6</td><td>48.9</td><td>41.1</td><td>62.0</td><td>54.8</td><td>75.9</td></tr></tbody></table>

Mean Boundary Intersection over Union (mBIoU) computes the average IoU over narrow boundary regions, emphasizing boundary precision compared to standard mIoU.

平均边界交并比(mBIoU)计算狭窄边界区域的平均IoU，相较于标准mIoU更强调边界的精确度。

Panoptic Quality (PQ) calculates panoptic segmentation performance by recognition quality (RQ) and segmentation quality (SQ). F-score measures the balance between precision and recall, computed using an IoU threshold of 0.5 .

全景质量(PQ)通过识别质量(RQ)和分割质量(SQ)来计算全景分割性能。F分数衡量精确率与召回率的平衡，计算时采用0.5的IoU阈值。

- Results. Following the taxonomy in Sec.2.1.1, we report results across four representative segmentation settings. For 3D instance segmentation (Table 2), Unified-Lift [20] achieves the best overall performance on both the Replica [285] and LERF-Mask [22] datasets, demonstrating the effectiveness of consistency-aware end-to-end learning. In the setting of  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  interactive segmentation (Table 3), GaussianCut [29] performs best on NVOS [279], while Click-Gaussian [24] attains the highest mIoU on SPIn-NeRF [67]. Notably, GaussianCut is a training-free method that flexibly supports both 2D clicks and text prompts as input. For open-vocabulary 3D semantic segmentation (Table 4), PanoGS [112] and InstanceGaussian [31] achieve the best performance under point-based evaluation protocols, whereas CAGS [103] performs best under 3D Gaussian-based evaluation. In the open-vocabulary 2D semantic segmentation setting, VLGaussian [28] reaches a state-of-the-art mIoU of 97.1% on the 3D-OVS benchmark [63], significantly outperforming existing approaches in Table 5. Table 6 further presents results for two subtasks: open-vocabulary 2D localization (detection) and semantic segmentation. FMLGS [98] achieves the best performance on the segmentation task, while VLGaussian [28] leads in localization. Across these settings, most methods leverage powerful vision-language foundation models such as CLIP and SAM to enhance open-vocabulary capabilities.

- 结果。按照第2.1.1节的分类法，我们报告了四种代表性分割设置下的结果。对于3D实例分割(表2)，Unified-Lift [20]在Replica [285]和LERF-Mask [22]数据集上均取得最佳整体表现，展示了一致性感知端到端学习的有效性。在 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 交互式分割设置(表3)中，GaussianCut [29]在NVOS [279]上表现最佳，而Click-Gaussian [24]在SPIn-NeRF [67]上获得最高mIoU。值得注意的是，GaussianCut是一种无需训练的方法，灵活支持2D点击和文本提示作为输入。对于开放词汇3D语义分割(表4)，PanoGS [112]和InstanceGaussian [31]在基于点的评估协议下表现最佳，而CAGS [103]在基于3D高斯的评估中表现最佳。在开放词汇2D语义分割设置中，VLGaussian [28]在3D-OVS基准 [63]上达到97.1%的mIoU，显著优于表5中的现有方法。表6进一步展示了两个子任务的结果:开放词汇2D定位(检测)和语义分割。FMLGS [98]在分割任务中表现最佳，而VLGaussian [28]在定位任务中领先。在这些设置中，大多数方法利用了强大的视觉-语言基础模型，如CLIP和SAM，以增强开放词汇能力。

TABLE 5: Quantitative open-vocabulary 2D semantic segmentation on 3D-OVS [63] dataset in terms of mIoU.

表5:基于3D-OVS [63]数据集的开放词汇2D语义分割定量结果，采用mIoU指标。

<table><tr><td>Method</td><td>Venue</td><td>Foundation Model</td><td>Bed</td><td>Bench</td><td>Room</td><td>Sofa</td><td>Lawn</td><td>Mean</td></tr><tr><td>LangSplat [26]</td><td>[CVPR'24]</td><td>CLIP & SAM</td><td>92.5</td><td>94.2</td><td>94.1</td><td>90.0</td><td>96.1</td><td>93.4</td></tr><tr><td>Feature 3DGS [27]</td><td>[CVPR’24]</td><td>CLIP</td><td>83.5</td><td>90.7</td><td>84.7</td><td>86.9</td><td>93.4</td><td>87.8</td></tr><tr><td>LEGaussians [84]</td><td>[CVPR’24]</td><td>CLIP</td><td>84.9</td><td>91.1</td><td>86.0</td><td>87.8</td><td>92.5</td><td>88.5</td></tr><tr><td>GaussianGrouping [22]</td><td>[ECCV'24]</td><td>SAM</td><td>83.0</td><td>91.5</td><td>85.9</td><td>87.3</td><td>90.6</td><td>87.7</td></tr><tr><td>N2F2 [88]</td><td>[ECCV'24]</td><td>CLIP & SAM</td><td>93.8</td><td>92.6</td><td>93.5</td><td>92.1</td><td>96.3</td><td>93.9</td></tr><tr><td>GOI [96]</td><td>[MM'24]</td><td>APE [290] & CLIP</td><td>89.4</td><td>92.8</td><td>91.3</td><td>85.6</td><td>94.1</td><td>90.6</td></tr><tr><td>FMGS [95]</td><td>[HCV'24]</td><td>CLIP & DINO</td><td>80.6</td><td>84.5</td><td>87.9</td><td>90.8</td><td>92.6</td><td>87.3</td></tr><tr><td>SLGaussian [117]</td><td>[ArXiv'24]</td><td>CLIP & SAM</td><td>41.3</td><td>47.0</td><td>-</td><td>30.3</td><td>54.4</td><td>44.1</td></tr><tr><td>FMLGS [98]</td><td>[ArXiv’25]</td><td>SAM2 & CLIP</td><td>95.7</td><td>96.3</td><td>96.8</td><td>95.2</td><td>-</td><td>96.0</td></tr><tr><td>SAGA [30]</td><td>[AAAI'25]</td><td>CLIP & SAM</td><td>97.4</td><td>95.4</td><td>96.8</td><td>93.5</td><td>96.6</td><td>96.0</td></tr><tr><td>FastLGS [97]</td><td>[AAAI'25]</td><td>CLIP & SAM</td><td>94.7</td><td>95.1</td><td>95.3</td><td>90.6</td><td>96.2</td><td>94.4</td></tr><tr><td>econSG [107]</td><td>[ICLR'25]</td><td>SAM</td><td>94.9</td><td>93.0</td><td>95.8</td><td>91.6</td><td>96.3</td><td>94.3</td></tr><tr><td>VLGaussian [28]</td><td>[ICLR'25]</td><td>CLIP</td><td>96.8</td><td>97.3</td><td>97.7</td><td>95.5</td><td>97.9</td><td>97.1</td></tr><tr><td>LBG [115]</td><td>[WACV’25]</td><td>CLIP & SAM & DINO</td><td>97.7</td><td>96.3</td><td>95.9</td><td>97.3</td><td>87.4</td><td>94.9</td></tr><tr><td>CLIP-GS [94]</td><td>[TOMM’25]</td><td>SAM</td><td>97.2</td><td>94.8</td><td>-</td><td>94.1</td><td>96.5</td><td>95.6</td></tr><tr><td>CCL-LGS [108]</td><td>[ICCV'25]</td><td>SAM & CLIP</td><td>97.3</td><td>95.0</td><td>-</td><td>92.3</td><td>96.1</td><td>95.2</td></tr><tr><td>ObjectGS [29]</td><td>[ICCV'25]</td><td>SAM2</td><td>98.0</td><td>96.4</td><td>95.1</td><td>97.2</td><td>95.4</td><td>96.4</td></tr></table>

<table><tbody><tr><td>方法</td><td>场地</td><td>基础模型(Foundation Model)</td><td>床</td><td>长椅</td><td>房间</td><td>沙发</td><td>草坪</td><td>平均值</td></tr><tr><td>LangSplat [26]</td><td>[CVPR'24]</td><td>CLIP & SAM</td><td>92.5</td><td>94.2</td><td>94.1</td><td>90.0</td><td>96.1</td><td>93.4</td></tr><tr><td>Feature 3DGS [27]</td><td>[CVPR’24]</td><td>CLIP</td><td>83.5</td><td>90.7</td><td>84.7</td><td>86.9</td><td>93.4</td><td>87.8</td></tr><tr><td>LEGaussians [84]</td><td>[CVPR’24]</td><td>CLIP</td><td>84.9</td><td>91.1</td><td>86.0</td><td>87.8</td><td>92.5</td><td>88.5</td></tr><tr><td>GaussianGrouping [22]</td><td>[ECCV'24]</td><td>SAM</td><td>83.0</td><td>91.5</td><td>85.9</td><td>87.3</td><td>90.6</td><td>87.7</td></tr><tr><td>N2F2 [88]</td><td>[ECCV'24]</td><td>CLIP & SAM</td><td>93.8</td><td>92.6</td><td>93.5</td><td>92.1</td><td>96.3</td><td>93.9</td></tr><tr><td>GOI [96]</td><td>[MM'24]</td><td>APE [290] & CLIP</td><td>89.4</td><td>92.8</td><td>91.3</td><td>85.6</td><td>94.1</td><td>90.6</td></tr><tr><td>FMGS [95]</td><td>[HCV'24]</td><td>CLIP & DINO</td><td>80.6</td><td>84.5</td><td>87.9</td><td>90.8</td><td>92.6</td><td>87.3</td></tr><tr><td>SLGaussian [117]</td><td>[ArXiv'24]</td><td>CLIP & SAM</td><td>41.3</td><td>47.0</td><td>-</td><td>30.3</td><td>54.4</td><td>44.1</td></tr><tr><td>FMLGS [98]</td><td>[ArXiv’25]</td><td>SAM2 & CLIP</td><td>95.7</td><td>96.3</td><td>96.8</td><td>95.2</td><td>-</td><td>96.0</td></tr><tr><td>SAGA [30]</td><td>[AAAI'25]</td><td>CLIP & SAM</td><td>97.4</td><td>95.4</td><td>96.8</td><td>93.5</td><td>96.6</td><td>96.0</td></tr><tr><td>FastLGS [97]</td><td>[AAAI'25]</td><td>CLIP & SAM</td><td>94.7</td><td>95.1</td><td>95.3</td><td>90.6</td><td>96.2</td><td>94.4</td></tr><tr><td>econSG [107]</td><td>[ICLR'25]</td><td>SAM</td><td>94.9</td><td>93.0</td><td>95.8</td><td>91.6</td><td>96.3</td><td>94.3</td></tr><tr><td>VLGaussian [28]</td><td>[ICLR'25]</td><td>CLIP</td><td>96.8</td><td>97.3</td><td>97.7</td><td>95.5</td><td>97.9</td><td>97.1</td></tr><tr><td>LBG [115]</td><td>[WACV’25]</td><td>CLIP & SAM & DINO</td><td>97.7</td><td>96.3</td><td>95.9</td><td>97.3</td><td>87.4</td><td>94.9</td></tr><tr><td>CLIP-GS [94]</td><td>[TOMM’25]</td><td>SAM</td><td>97.2</td><td>94.8</td><td>-</td><td>94.1</td><td>96.5</td><td>95.6</td></tr><tr><td>CCL-LGS [108]</td><td>[ICCV'25]</td><td>SAM & CLIP</td><td>97.3</td><td>95.0</td><td>-</td><td>92.3</td><td>96.1</td><td>95.2</td></tr><tr><td>ObjectGS [29]</td><td>[ICCV'25]</td><td>SAM2</td><td>98.0</td><td>96.4</td><td>95.1</td><td>97.2</td><td>95.4</td><td>96.4</td></tr></tbody></table>

TABLE 6: Quantitative results on LERF-OVS [26] Dataset. We report the open-vocabulary localization accuracy (mAcc) and 2D semantic segmentation (mIoU).

表6:LERF-OVS [26] 数据集的定量结果。我们报告了开放词汇定位准确率(mAcc)和二维语义分割(mIoU)。

<table><tr><td rowspan="2">Method</td><td rowspan="2">Venue</td><td colspan="2">Ramen</td><td colspan="2">Figurines</td><td colspan="2">Teamtime</td><td colspan="2">Kitchen</td></tr><tr><td>mloU</td><td>mAcc</td><td>mloU</td><td>mAcc</td><td>mloU</td><td>mAcc</td><td>mloU</td><td>mAcc</td></tr><tr><td>LangSplat [26]</td><td>[CVPR'24]</td><td>51.2</td><td>73.2</td><td>44.7</td><td>80.4</td><td>65.1</td><td>88.1</td><td>44.5</td><td>95.5</td></tr><tr><td>Feature3DGS [27]</td><td>[CVPR'24]</td><td>43.7</td><td>69.8</td><td>40.5</td><td>73.4</td><td>58.8</td><td>77.2</td><td>39.6</td><td>87.6</td></tr><tr><td>LEGaussians [84]</td><td>[CVPR’24]</td><td>46.0</td><td>67.5</td><td>40.8</td><td>75.2</td><td>60.3</td><td>75.6</td><td>39.4</td><td>90.3</td></tr><tr><td>GaussianGrouping [22]</td><td>[ECCV’24]</td><td>45.5</td><td>68.6</td><td>40.0</td><td>74.3</td><td>60.9</td><td>75.0</td><td>38.7</td><td>88.2</td></tr><tr><td>N2F2 [88]</td><td>[ECCV'24]</td><td>56.6</td><td>78.8</td><td>47.0</td><td>85.7</td><td>69.2</td><td>91.5</td><td>47.9</td><td>95.5</td></tr><tr><td>FMGS [95]</td><td>[IJCV'24]</td><td>-</td><td>90.0</td><td>-</td><td>93.8</td><td>-</td><td>89.7</td><td>-</td><td>92.6</td></tr><tr><td>GOI [96]</td><td>[MM’24]</td><td>52.6</td><td>75.5</td><td>44.5</td><td>82.9</td><td>63.7</td><td>88.6</td><td>41.4</td><td>90.4</td></tr><tr><td>LangSurf [89]</td><td>[ArXiv’24]</td><td>47.0</td><td>63.4</td><td>-</td><td>-</td><td>73.6</td><td>84.8</td><td>55.0</td><td>81.9</td></tr><tr><td>FMLGS [98]</td><td>[ArXiv’25]</td><td>73.2</td><td>89.2</td><td>72.4</td><td>94.3</td><td>81.8</td><td>96.7</td><td>64.3</td><td>96.2</td></tr><tr><td>FastLGS [97]</td><td>[AAAF25]</td><td>-</td><td>84.2</td><td>-</td><td>91.4</td><td>-</td><td>95.0</td><td>-</td><td>96.2</td></tr><tr><td>econSG [107]</td><td>[ICLR’25]</td><td>-</td><td>83.2</td><td>-</td><td>89.3</td><td>-</td><td>93.4</td><td>-</td><td>96.2</td></tr><tr><td>VLGaussian [28]</td><td>IICLR’251</td><td>61.4</td><td>92.5</td><td>58.1</td><td>97.1</td><td>73.5</td><td>95.8</td><td>54.8</td><td>98.6</td></tr><tr><td>LangScene-X [121]</td><td>[ICCV’25]</td><td>42.9</td><td>72.7</td><td>-</td><td>-</td><td>45.0</td><td>78.9</td><td>63.6</td><td>90.9</td></tr><tr><td>LUDVIG [116]</td><td>[ICCV'25]</td><td>58.1</td><td>78.9</td><td>63.3</td><td>80.4</td><td>77.1</td><td>94.9</td><td>58.5</td><td>90.9</td></tr><tr><td>CCL-LGS [108]</td><td>[ICCV'25]</td><td>62.3</td><td>-</td><td>61.2</td><td>-</td><td>71.8</td><td>-</td><td>67.1</td><td>-</td></tr></table>

<table><tbody><tr><td rowspan="2">方法</td><td rowspan="2">场地</td><td colspan="2">拉面</td><td colspan="2">小雕像</td><td colspan="2">团队时间</td><td colspan="2">厨房</td></tr><tr><td>平均交并比(mloU)</td><td>平均准确率(mAcc)</td><td>平均交并比(mloU)</td><td>平均准确率(mAcc)</td><td>平均交并比(mloU)</td><td>平均准确率(mAcc)</td><td>平均交并比(mloU)</td><td>平均准确率(mAcc)</td></tr><tr><td>LangSplat [26]</td><td>[CVPR'24]</td><td>51.2</td><td>73.2</td><td>44.7</td><td>80.4</td><td>65.1</td><td>88.1</td><td>44.5</td><td>95.5</td></tr><tr><td>Feature3DGS [27]</td><td>[CVPR'24]</td><td>43.7</td><td>69.8</td><td>40.5</td><td>73.4</td><td>58.8</td><td>77.2</td><td>39.6</td><td>87.6</td></tr><tr><td>LEGaussians [84]</td><td>[CVPR’24]</td><td>46.0</td><td>67.5</td><td>40.8</td><td>75.2</td><td>60.3</td><td>75.6</td><td>39.4</td><td>90.3</td></tr><tr><td>GaussianGrouping [22]</td><td>[ECCV’24]</td><td>45.5</td><td>68.6</td><td>40.0</td><td>74.3</td><td>60.9</td><td>75.0</td><td>38.7</td><td>88.2</td></tr><tr><td>N2F2 [88]</td><td>[ECCV'24]</td><td>56.6</td><td>78.8</td><td>47.0</td><td>85.7</td><td>69.2</td><td>91.5</td><td>47.9</td><td>95.5</td></tr><tr><td>FMGS [95]</td><td>[IJCV'24]</td><td>-</td><td>90.0</td><td>-</td><td>93.8</td><td>-</td><td>89.7</td><td>-</td><td>92.6</td></tr><tr><td>GOI [96]</td><td>[MM’24]</td><td>52.6</td><td>75.5</td><td>44.5</td><td>82.9</td><td>63.7</td><td>88.6</td><td>41.4</td><td>90.4</td></tr><tr><td>LangSurf [89]</td><td>[ArXiv’24]</td><td>47.0</td><td>63.4</td><td>-</td><td>-</td><td>73.6</td><td>84.8</td><td>55.0</td><td>81.9</td></tr><tr><td>FMLGS [98]</td><td>[ArXiv’25]</td><td>73.2</td><td>89.2</td><td>72.4</td><td>94.3</td><td>81.8</td><td>96.7</td><td>64.3</td><td>96.2</td></tr><tr><td>FastLGS [97]</td><td>[AAAF25]</td><td>-</td><td>84.2</td><td>-</td><td>91.4</td><td>-</td><td>95.0</td><td>-</td><td>96.2</td></tr><tr><td>econSG [107]</td><td>[ICLR’25]</td><td>-</td><td>83.2</td><td>-</td><td>89.3</td><td>-</td><td>93.4</td><td>-</td><td>96.2</td></tr><tr><td>VLGaussian [28]</td><td>IICLR’251</td><td>61.4</td><td>92.5</td><td>58.1</td><td>97.1</td><td>73.5</td><td>95.8</td><td>54.8</td><td>98.6</td></tr><tr><td>LangScene-X [121]</td><td>[ICCV’25]</td><td>42.9</td><td>72.7</td><td>-</td><td>-</td><td>45.0</td><td>78.9</td><td>63.6</td><td>90.9</td></tr><tr><td>LUDVIG [116]</td><td>[ICCV'25]</td><td>58.1</td><td>78.9</td><td>63.3</td><td>80.4</td><td>77.1</td><td>94.9</td><td>58.5</td><td>90.9</td></tr><tr><td>CCL-LGS [108]</td><td>[ICCV'25]</td><td>62.3</td><td>-</td><td>61.2</td><td>-</td><td>71.8</td><td>-</td><td>67.1</td><td>-</td></tr></tbody></table>

### 4.2 Performance Benchmarking: 3DGS Editing

### 4.2 性能基准测试:3DGS 编辑

- Datasets. We summarize the key features of the datasets in Table 7, with detailed descriptions provided below.

- 数据集。我们在表7中总结了数据集的主要特征，详细描述如下。

DTU [292] contains 80 scenes of large variability. Each scene consists of 49 or 64 accurate camera positions and reference structured light scans, all acquired by a 6-axis industrial robot.

DTU [292] 包含80个场景，变化范围大。每个场景由49或64个精确的相机位置和参考结构光扫描组成，均由6轴工业机器人采集。

Tanks and Temples [293] consists of 21 scenes, including individual objects such as "Tank" and "Train," as well as large indoor scenes like "Auditorium" and "Museum".

Tanks and Temples [293] 包含21个场景，包括单个物体如“坦克”和“火车”，以及大型室内场景如“礼堂”和“博物馆”。

GL3D [294] contains 125,623 high-resolution images, mostly drone-captured at varying scales and angles with strong geometric overlap, covering urban, rural, and scenic areas.

GL3D [294] 包含125,623张高分辨率图像，主要由无人机在不同尺度和角度拍摄，具有强几何重叠，覆盖城市、乡村和风景区。

LLFF [295] consists of both synthetic and real-world datasets. The synthetic part includes rendered images from 45,000 simplified indoor scenes, while the real-world part covers 24 training scenes and 8 testing scenes, each with 20-30 multi-view images.

LLFF [295] 包含合成和真实世界数据集。合成部分包括来自45,000个简化室内场景的渲染图像，真实部分涵盖24个训练场景和8个测试场景，每个场景有20-30张多视角图像。

NeRF-synthetic [2] consists of 8 scenes of an object placed on a white background. Each scene includes 100 training images with a resolution of  <img src="https://www.zhihu.com/equation?tex={800} \times  {800}" alt="{800} \times  {800}" class="ee_img tr_noresize" eeimg="1">  and associated camera poses.

NeRF-synthetic [2] 包含8个物体置于白色背景的场景。每个场景包括100张分辨率为 <img src="https://www.zhihu.com/equation?tex={800} \times  {800}" alt="{800} \times  {800}" class="ee_img tr_noresize" eeimg="1"> 的训练图像及相应的相机位姿。

BlendedMVS [296] contains 113 scenes with 20-1000 images each, totaling 17,818 images.

BlendedMVS [296] 包含113个场景，每个场景有20-1000张图像，总计17,818张图像。

Co3D [297] contains 1.5 million frames from 19,000 videos of objects across  <img src="https://www.zhihu.com/equation?tex={50}\mathrm{{MS}} - \mathrm{{COCO}}" alt="{50}\mathrm{{MS}} - \mathrm{{COCO}}" class="ee_img tr_noresize" eeimg="1">  categories, with camera poses and 3D point cloud annotations.

Co3D [297] 包含来自 <img src="https://www.zhihu.com/equation?tex={50}\mathrm{{MS}} - \mathrm{{COCO}}" alt="{50}\mathrm{{MS}} - \mathrm{{COCO}}" class="ee_img tr_noresize" eeimg="1"> 类别的19,000个视频中的150万帧，附带相机位姿和3D点云标注。

Mip-NeRF360 [281] provides  <img src="https://www.zhihu.com/equation?tex={360}^{ \circ  }" alt="{360}^{ \circ  }" class="ee_img tr_noresize" eeimg="1">  indoor and outdoor panoramas for evaluating neural rendering methods, focusing on complex lighting, geometry, and textures.

Mip-NeRF360 [281] 提供 <img src="https://www.zhihu.com/equation?tex={360}^{ \circ  }" alt="{360}^{ \circ  }" class="ee_img tr_noresize" eeimg="1"> 个室内外全景图，用于评估神经渲染方法，重点关注复杂的光照、几何和纹理。

Nerfstudio [298] contains 10 scenes: 4 captured by phones with pinhole lenses and 6 by mirrorless cameras with fisheye lenses.

Nerfstudio [298] 包含10个场景:4个由带针孔镜头的手机拍摄，6个由带鱼眼镜头的无反相机拍摄。

SPIn-NeRF [67] contains 10 forward-facing in-the-wild scenes, including 3 indoor and 7 outdoor scenes, each with 100 multiview images and annotated foreground masks.

SPIn-NeRF [67] 包含10个面向前方的野外场景，包括3个室内和7个室外场景，每个场景有100张多视角图像和标注的前景掩码。

IN2N [75] contains 6 real-world scenes (e.g., bear, face, person), each with an average of 172 images.

IN2N [75] 包含6个真实场景(如熊、脸、人)，每个场景平均有172张图像。

ScanNet++ [299] contains 460 scenes, 280,000 captured DSLR images, and over 3.7M iPhone RGBD frames.

ScanNet++ [299] 包含460个场景，280,000张DSLR拍摄的图像，以及超过370万张iPhone RGBD帧。

360-USID [300] includes 4 outdoor and 3 indoor scenes, each with 171-347 training and 31-33 novel views.

360-USID [300] 包含4个室外和3个室内场景，每个场景有171-347张训练视图和31-33张新颖视图。

- Metrics. Commonly used evaluation metrics in the editing domain are listed below:

- 评估指标。编辑领域常用的评估指标如下:

CLIP Text-Image Direction Similarity. CLIP [47] extracts two paired sets of text-image embeddings (original and modified), and computes their directional shifts via cosine similarity.

CLIP文本-图像方向相似度。CLIP [47]提取两组成对的文本-图像嵌入(原始和修改后)，并通过余弦相似度计算它们的方向变化。

CLIP Text-Image Similarity. CLIP [47] measures text-image similarity by encoding text and images into embeddings separately and then computing their cosine similarity.

CLIP文本-图像相似度。CLIP [47]通过分别将文本和图像编码为嵌入向量，然后计算它们的余弦相似度来衡量文本与图像的相似度。

CLIP Image-Image Similarity. CLIP [47] computes image-image similarity by encoding edited image and subject image into embeddings separately and then computing their cosine similarity.

CLIP图像-图像相似度。CLIP [47]通过分别将编辑后的图像和目标图像编码为嵌入向量，然后计算它们的余弦相似度来衡量图像间的相似度。

Frechet Inception Distance (FID) calculates the similarity between ground truth and generated images by comparing their Inception-v3 [301] feature distributions using the Frechet distance.

Frechet Inception距离(FID)通过比较Inception-v3 [301]特征分布的Frechet距离，计算真实图像与生成图像之间的相似度。

DINO Similarity. DINO [302] extracts features from the edited scene and reference image, then computes their average visual similarity to evaluate image-image alignment.

DINO相似度。DINO [302]从编辑场景和参考图像中提取特征，然后计算它们的平均视觉相似度以评估图像间的对齐程度。

Peak Signal-to-Noise Ratio (PSNR) measures image quality by comparing the maximum pixel intensity to the MSE between images, converted to decibels.

峰值信噪比(PSNR)通过比较最大像素强度与图像间均方误差(MSE)，并转换为分贝，来衡量图像质量。

Structural Similarity (SSIM) measures image similarity by comparing brightness, contrast, and structure.

结构相似性(SSIM)通过比较亮度、对比度和结构来衡量图像的相似度。

RMSE. It is computed as the root mean squared pixel-wise difference between the edited image and ground truth.

均方根误差(RMSE)。它是编辑图像与真实图像在像素级别差异的均方根值。

Learned Perceptual Image Patch Similarity (LPIPS) measures perceptual similarity by comparing deep features of two images. Aesthetic score. Aesthetic Predictor V2.5 [303] is a SigLIP-based model that rates image aesthetics from 1 to 10 .

学习感知图像块相似度(LPIPS)通过比较两幅图像的深层特征来衡量感知相似度。美学评分。Aesthetic Predictor V2.5 [303]是基于SigLIP的模型，评分范围为1到10。

User study. It evaluates results by collecting and analyzing feedback from human participants across customized dimensions such as preference in comparative results, text-image alignment, and view consistency.

用户研究。通过收集和分析人类参与者在定制维度上的反馈，如比较结果偏好、文本-图像对齐和视角一致性，来评估结果。

- Results. For 3D image editing tasks, the most commonly used datasets are Mip-NeRF360 [281] and IN2N [75]. We adopt these two datasets as benchmarks to record the performance of various methods. However, as most existing 3D editing methods are evaluated on different scene subsets, it remains difficult to perform a comprehensive and fair comparison under a unified protocol. Given that IN2N [75] is widely adopted as a baseline, we report the relative gains of each method over IN2N to enable an indirect yet relatively fair comparison, as summarized in Table 8. Overall, GaussianVTON [155] demonstrates the best performance across multiple metrics. For the style transfer task, we use Mip-NeRF 360 [281] as the benchmark and compare the results of different methods in Table 9, where SemanticSplatSty. [167] achieves the best overall performance.

- 结果。对于3D图像编辑任务，最常用的数据集是Mip-NeRF360 [281]和IN2N [75]。我们采用这两个数据集作为基准，记录各种方法的性能。然而，由于大多数现有3D编辑方法在不同场景子集上进行评估，难以在统一协议下进行全面且公平的比较。鉴于IN2N [75]被广泛采用作为基线，我们报告各方法相对于IN2N的相对提升，以实现间接但相对公平的比较，详见表8。总体而言，GaussianVTON [155]在多个指标上表现最佳。对于风格迁移任务，我们使用Mip-NeRF 360 [281]作为基准，并在表9中比较不同方法的结果，其中SemanticSplatSty. [167]取得了最佳整体表现。

TABLE 7: Statistics of representative 3DGS editing datasets. See §4.2 for more detailed descriptions.

表7:代表性3DGS编辑数据集的统计信息。详见§4.2。

<table><tr><td>Dataset</td><td>Venue</td><td>#Scenes</td><td> <img src="https://www.zhihu.com/equation?tex=\mathbf{\# {Views}\left( {{Avg}.}\right) }" alt="\mathbf{\# {Views}\left( {{Avg}.}\right) }" class="ee_img tr_noresize" eeimg="1"> </td><td>Characterization</td></tr><tr><td>DTU [292]</td><td>[CVPR'14]</td><td>80</td><td>343</td><td>Each scene consists of 49 or 64 accurate camera positions and reference structured light scans.</td></tr><tr><td>Tanks and Temples [293]</td><td>[TOG'17]</td><td>14</td><td>-</td><td>It includes individual objects (e.g., "Tank", "Train") and large indoor scenes (e.g., "Auditorium", "Museum").</td></tr><tr><td>GL3D [294]</td><td>[ACCV'18]</td><td>543</td><td>230</td><td>It contains 125,623 high-resolution images, most of which are captured by drones from multiple scales and perspectives with significant geometric overlap. The images cover urban areas, rural regions, and scenic spots.</td></tr><tr><td>LLFF [295]</td><td>[TOG'19]</td><td>32</td><td>25</td><td>Using the COLMAP structure from motion implementation to compute poses for real images.</td></tr><tr><td>NeRF-synthetic [2]</td><td>[ECCV'20]</td><td>8</td><td>100</td><td>Each object is placed on a white background, with images at  <img src="https://www.zhihu.com/equation?tex={800} \times  {800}" alt="{800} \times  {800}" class="ee_img tr_noresize" eeimg="1">  resolution and correspond</td></tr><tr><td>BlendedMVS [296]</td><td>[CVPR’20]</td><td>113</td><td>158</td><td>A large-scale MVS dataset, which contains a total of 17,818 images.</td></tr><tr><td>Co3D [297]</td><td>[CVPR'21]</td><td>-</td><td>-</td><td>Consists of 1.5 million frames extracted from approximately 19,000 videos, capturing objects from 50 MS-COCO categories. Each image is annotated with camera poses and ground-truth 3D point clouds.</td></tr><tr><td>Mip-NeRF360 [281]</td><td>[CVPR'22]</td><td>9</td><td>215</td><td>It consists of 360-degree panoramic images from both indoor and outdoor scenes.</td></tr><tr><td>Nerfstudio [298]</td><td>[SIGGRAPH’23]</td><td>10</td><td>-</td><td>4 phone captures with pinhole lenses and 6 mirrorless camera captures with a fisheye lens.</td></tr><tr><td>SPIn-NeRF [67]</td><td>[CVPR’23]</td><td>10</td><td>100</td><td>Providing challenging real-world scenes with views both with and without a target object.</td></tr><tr><td>IN2N [75]</td><td>[ICCV'23]</td><td>6</td><td>172</td><td>Enabling structured and globally consistent 3D scene modifications while preserving the original scene's identity.</td></tr><tr><td>ScanNet++ [299]</td><td>[ICCV'23]</td><td>460</td><td>608</td><td>280,000 captured DSLR images, and over 3.7M iPhone RGBD frames.</td></tr><tr><td>360-USID [300]</td><td>[CVPR'25]</td><td>7</td><td>300</td><td>Four outdoor (Box, Cone, Lawn, Plant) and three indoor (Cookie, Sunflower, Dustpan).</td></tr></table>

<table><tbody><tr><td>数据集</td><td>会议</td><td>场景数量</td><td> <img src="https://www.zhihu.com/equation?tex=\mathbf{\# {Views}\left( {{Avg}.}\right) }" alt="\mathbf{\# {Views}\left( {{Avg}.}\right) }" class="ee_img tr_noresize" eeimg="1"> </td><td>特征描述</td></tr><tr><td>DTU [292]</td><td>[CVPR'14]</td><td>80</td><td>343</td><td>每个场景包含49或64个精确的相机位置和参考结构光扫描。</td></tr><tr><td>Tanks and Temples [293]</td><td>[TOG'17]</td><td>14</td><td>-</td><td>包括单个物体(如“坦克”、“火车”)和大型室内场景(如“礼堂”、“博物馆”)。</td></tr><tr><td>GL3D [294]</td><td>[ACCV'18]</td><td>543</td><td>230</td><td>包含125,623张高分辨率图像，大部分由无人机从多尺度和多视角拍摄，具有显著的几何重叠。图像覆盖城市区域、农村地区和风景名胜。</td></tr><tr><td>LLFF [295]</td><td>[TOG'19]</td><td>32</td><td>25</td><td>使用COLMAP的结构光运动(structure from motion)实现计算真实图像的相机位姿。</td></tr><tr><td>NeRF-synthetic [2]</td><td>[ECCV'20]</td><td>8</td><td>100</td><td>每个物体置于白色背景，图像分辨率为 <img src="https://www.zhihu.com/equation?tex={800} \times  {800}" alt="{800} \times  {800}" class="ee_img tr_noresize" eeimg="1"> ，并对应</td></tr><tr><td>BlendedMVS [296]</td><td>[CVPR’20]</td><td>113</td><td>158</td><td>一个大规模多视图立体(MVS)数据集，共包含17,818张图像。</td></tr><tr><td>Co3D [297]</td><td>[CVPR'21]</td><td>-</td><td>-</td><td>由约19,000个视频中提取的150万帧组成，涵盖50个MS-COCO类别的物体。每张图像均标注了相机位姿和真实三维点云。</td></tr><tr><td>Mip-NeRF360 [281]</td><td>[CVPR'22]</td><td>9</td><td>215</td><td>包含室内外场景的360度全景图像。</td></tr><tr><td>Nerfstudio [298]</td><td>[SIGGRAPH’23]</td><td>10</td><td>-</td><td>4个带针孔镜头的手机拍摄和6个带鱼眼镜头的无反相机拍摄。</td></tr><tr><td>SPIn-NeRF [67]</td><td>[CVPR’23]</td><td>10</td><td>100</td><td>提供具有挑战性的真实场景视图，包含有目标物体和无目标物体的视角。</td></tr><tr><td>IN2N [75]</td><td>[ICCV'23]</td><td>6</td><td>172</td><td>实现结构化且全局一致的三维场景修改，同时保持原始场景的身份特征。</td></tr><tr><td>ScanNet++ [299]</td><td>[ICCV'23]</td><td>460</td><td>608</td><td>280,000张DSLR相机拍摄的图像，以及超过370万帧iPhone RGBD数据。</td></tr><tr><td>360-USID [300]</td><td>[CVPR'25]</td><td>7</td><td>300</td><td>四个户外场景(盒子、锥体、草坪、植物)和三个室内场景(饼干、向日葵、簸箕)。</td></tr></tbody></table>

TABLE 8: Performance comparison of 3DGS editing on Mip-NeRF360 [281] and IN2N [75] datasets. The similarity for CLIP Text-Image Direction, CLIP Text-Image, CLIP Image-Image, DINO are denoted as  <img src="https://www.zhihu.com/equation?tex={\mathrm{{CLIP}}}_{dir},{\mathrm{{CLIP}}}_{T2I},{\mathrm{{CLIP}}}_{I2I}" alt="{\mathrm{{CLIP}}}_{dir},{\mathrm{{CLIP}}}_{T2I},{\mathrm{{CLIP}}}_{I2I}" class="ee_img tr_noresize" eeimg="1">  , and  <img src="https://www.zhihu.com/equation?tex={\mathrm{{DINO}}}_{sim}" alt="{\mathrm{{DINO}}}_{sim}" class="ee_img tr_noresize" eeimg="1">  , respectively.

表8:在Mip-NeRF360 [281]和IN2N [75]数据集上3DGS编辑性能比较。CLIP文本-图像方向、CLIP文本-图像、CLIP图像-图像、DINO的相似度分别表示为 <img src="https://www.zhihu.com/equation?tex={\mathrm{{CLIP}}}_{dir},{\mathrm{{CLIP}}}_{T2I},{\mathrm{{CLIP}}}_{I2I}" alt="{\mathrm{{CLIP}}}_{dir},{\mathrm{{CLIP}}}_{T2I},{\mathrm{{CLIP}}}_{I2I}" class="ee_img tr_noresize" eeimg="1"> 和 <img src="https://www.zhihu.com/equation?tex={\mathrm{{DINO}}}_{sim}" alt="{\mathrm{{DINO}}}_{sim}" class="ee_img tr_noresize" eeimg="1"> 。

<table><tr><td>Methods</td><td>Venue</td><td>Category</td><td>Condition</td><td>Foundation Model</td><td/><td> <img src="https://www.zhihu.com/equation?tex={\mathbf{{CLIP}}}_{T2I} \uparrow" alt="{\mathbf{{CLIP}}}_{T2I} \uparrow" class="ee_img tr_noresize" eeimg="1"> </td><td> <img src="https://www.zhihu.com/equation?tex={\mathbf{{CLIP}}}_{I2I} \uparrow" alt="{\mathbf{{CLIP}}}_{I2I} \uparrow" class="ee_img tr_noresize" eeimg="1"> </td><td> <img src="https://www.zhihu.com/equation?tex={\mathbf{{DINO}}}_{sim} \uparrow" alt="{\mathbf{{DINO}}}_{sim} \uparrow" class="ee_img tr_noresize" eeimg="1"> </td><td>FID  <img src="https://www.zhihu.com/equation?tex=\downarrow" alt="\downarrow" class="ee_img tr_noresize" eeimg="1"> </td><td>PSNR ↑</td><td>SSIM↑</td><td>FPS  <img src="https://www.zhihu.com/equation?tex=\downarrow" alt="\downarrow" class="ee_img tr_noresize" eeimg="1"> </td></tr><tr><td>IN2N [75]</td><td>[ICCV'23]</td><td colspan="11">Baseline</td></tr><tr><td>GaussianEditor [8]</td><td>[CVPR'24]</td><td>Localizing the Editing Object</td><td>Text</td><td>InstructPix2Pix [128]</td><td>+29.44%</td><td>-</td><td/><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>GaussianEditor [35]</td><td>[CVPR’24]</td><td>Localizing the Editing Object</td><td>Text</td><td>InstructPix2Pix [128]</td><td>+27.27%</td><td>-</td><td>+11.76%</td><td>-</td><td>-50.49%</td><td>-</td><td>-</td><td>-60.87%</td></tr><tr><td>TIP-Editor [33]</td><td>[TOG'24]</td><td>Parameter-Efficient Tuning</td><td>Text&Image</td><td>Stable Diffusion [55]</td><td>+86.75%</td><td>-</td><td>-</td><td>+8.52%</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>GaussCtrl [136]</td><td>[ECCV'24]</td><td>Multi-View Consistency</td><td>Text</td><td>ControlNet [202]</td><td>+19.34%</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-33.33%</td></tr><tr><td>DGE [139]</td><td>[ECCV’24]</td><td>Multi-View Consistency</td><td>Text</td><td>InstructPix2Pix [128]</td><td>+4.69%</td><td>+5.12%</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-92.16%</td></tr><tr><td>GaussianVTON [155]</td><td>[ArXiv'24]</td><td>Multi-Stage Refinement</td><td>Image</td><td>Stable Diffusion [55]</td><td>-</td><td>+105.81%</td><td>+23.29%</td><td>-</td><td>-40.47%</td><td>+28.39%</td><td>+11.61%</td><td>-</td></tr><tr><td>Gomel et al. [138]</td><td>[ArXiv'24]</td><td>Multi-View Consistency</td><td>Text</td><td>Stable Diffusion [55]</td><td>+26.00%</td><td>+9.09%</td><td>+10.50%</td><td>+43.44%</td><td>-</td><td>+2.37%</td><td>-</td><td>-</td></tr><tr><td>ProGDF [142]</td><td>[ArXiv'24]</td><td>Efficiency and Speed</td><td>Text</td><td>InstructPix2Pix [128]</td><td>+36.25%</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>TrAME [137]</td><td>[TMM’25]</td><td>Multi-View Consistency</td><td>Text</td><td>Stable Diffusion [55]</td><td>-</td><td>-</td><td>+9.15%</td><td>-</td><td>-</td><td>+16.60%</td><td>-</td><td>-</td></tr><tr><td>DreamCatalyst [36]</td><td>[ICLR'25]</td><td>Efficiency and Speed</td><td>Text</td><td>InstructPix2Pix [128]</td><td>+10.14%</td><td>-</td><td>+1.04%</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-46.15%</td></tr></table>

<table><tbody><tr><td>方法</td><td>会议</td><td>类别</td><td>条件</td><td>基础模型</td><td></td><td> <img src="https://www.zhihu.com/equation?tex={\mathbf{{CLIP}}}_{T2I} \uparrow" alt="{\mathbf{{CLIP}}}_{T2I} \uparrow" class="ee_img tr_noresize" eeimg="1"> </td><td> <img src="https://www.zhihu.com/equation?tex={\mathbf{{CLIP}}}_{I2I} \uparrow" alt="{\mathbf{{CLIP}}}_{I2I} \uparrow" class="ee_img tr_noresize" eeimg="1"> </td><td> <img src="https://www.zhihu.com/equation?tex={\mathbf{{DINO}}}_{sim} \uparrow" alt="{\mathbf{{DINO}}}_{sim} \uparrow" class="ee_img tr_noresize" eeimg="1"> </td><td>FID  <img src="https://www.zhihu.com/equation?tex=\downarrow" alt="\downarrow" class="ee_img tr_noresize" eeimg="1"> </td><td>峰值信噪比(PSNR)↑</td><td>结构相似性指数(SSIM)↑</td><td>帧率(FPS)  <img src="https://www.zhihu.com/equation?tex=\downarrow" alt="\downarrow" class="ee_img tr_noresize" eeimg="1"> </td></tr><tr><td>IN2N [75]</td><td>[ICCV'23]</td><td colspan="11">基线</td></tr><tr><td>GaussianEditor [8]</td><td>[CVPR'24]</td><td>定位编辑对象</td><td>文本</td><td>InstructPix2Pix [128]</td><td>+29.44%</td><td>-</td><td></td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>GaussianEditor [35]</td><td>[CVPR’24]</td><td>定位编辑对象</td><td>文本</td><td>InstructPix2Pix [128]</td><td>+27.27%</td><td>-</td><td>+11.76%</td><td>-</td><td>-50.49%</td><td>-</td><td>-</td><td>-60.87%</td></tr><tr><td>TIP-Editor [33]</td><td>[TOG'24]</td><td>参数高效调优</td><td>文本与图像</td><td>Stable Diffusion [55]</td><td>+86.75%</td><td>-</td><td>-</td><td>+8.52%</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>GaussCtrl [136]</td><td>[ECCV'24]</td><td>多视角一致性</td><td>文本</td><td>ControlNet [202]</td><td>+19.34%</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-33.33%</td></tr><tr><td>DGE [139]</td><td>[ECCV’24]</td><td>多视角一致性</td><td>文本</td><td>InstructPix2Pix [128]</td><td>+4.69%</td><td>+5.12%</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-92.16%</td></tr><tr><td>GaussianVTON [155]</td><td>[ArXiv'24]</td><td>多阶段细化</td><td>图像</td><td>Stable Diffusion [55]</td><td>-</td><td>+105.81%</td><td>+23.29%</td><td>-</td><td>-40.47%</td><td>+28.39%</td><td>+11.61%</td><td>-</td></tr><tr><td>Gomel 等 [138]</td><td>[ArXiv'24]</td><td>多视角一致性</td><td>文本</td><td>Stable Diffusion [55]</td><td>+26.00%</td><td>+9.09%</td><td>+10.50%</td><td>+43.44%</td><td>-</td><td>+2.37%</td><td>-</td><td>-</td></tr><tr><td>ProGDF [142]</td><td>[ArXiv'24]</td><td>效率与速度</td><td>文本</td><td>InstructPix2Pix [128]</td><td>+36.25%</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>TrAME [137]</td><td>[TMM’25]</td><td>多视角一致性</td><td>文本</td><td>Stable Diffusion [55]</td><td>-</td><td>-</td><td>+9.15%</td><td>-</td><td>-</td><td>+16.60%</td><td>-</td><td>-</td></tr><tr><td>DreamCatalyst [36]</td><td>[ICLR'25]</td><td>效率与速度</td><td>文本</td><td>InstructPix2Pix [128]</td><td>+10.14%</td><td>-</td><td>+1.04%</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-46.15%</td></tr></tbody></table>

TABLE 9: Performance comparison of 3DGS style transfer on Mip-NeRF360 [281] dataset, evaluated by LPIPS and RMSE.

表9:基于Mip-NeRF360 [281] 数据集的3DGS风格迁移性能比较，评估指标为LPIPS和RMSE。

<table><tr><td rowspan="2"> <img src="https://www.zhihu.com/equation?tex=\mathbf{{Method}}" alt="\mathbf{{Method}}" class="ee_img tr_noresize" eeimg="1"> </td><td rowspan="2">Venue</td><td rowspan="2">2D Model</td><td colspan="2">Short-Term Consis.</td><td colspan="2">Long-Term Consis.</td></tr><tr><td>LPIPS</td><td>RMSE↓</td><td>LPIPS  <img src="https://www.zhihu.com/equation?tex=\downarrow" alt="\downarrow" class="ee_img tr_noresize" eeimg="1"> </td><td>RMSE↓</td></tr><tr><td>StyleGaus. [166]</td><td>[SIGGRAPH'24]</td><td>VGG</td><td>0.033</td><td>0.029</td><td>0.055</td><td>0.063</td></tr><tr><td>SemanticSplatSty. [167]</td><td>[TEGA'24]</td><td>VGG</td><td>0.019</td><td>0.042</td><td>0.028</td><td>0.055</td></tr><tr><td>InstantStyleGaus. [160]</td><td>[ArXiv’24]</td><td>Diffusion</td><td>0.024</td><td>0.026</td><td>0.074</td><td>0.076</td></tr><tr><td>SGSST [156]</td><td>[CVPR'25]</td><td>VGG</td><td>0.030</td><td>0.032</td><td>0.055</td><td>0.063</td></tr></table>

<table><tbody><tr><td rowspan="2"> <img src="https://www.zhihu.com/equation?tex=\mathbf{{Method}}" alt="\mathbf{{Method}}" class="ee_img tr_noresize" eeimg="1"> </td><td rowspan="2">场景</td><td rowspan="2">二维模型</td><td colspan="2">短期一致性</td><td colspan="2">长期一致性</td></tr><tr><td>LPIPS</td><td>均方根误差↓</td><td>LPIPS  <img src="https://www.zhihu.com/equation?tex=\downarrow" alt="\downarrow" class="ee_img tr_noresize" eeimg="1"> </td><td>均方根误差↓</td></tr><tr><td>StyleGaus. [166]</td><td>[SIGGRAPH'24]</td><td>VGG</td><td>0.033</td><td>0.029</td><td>0.055</td><td>0.063</td></tr><tr><td>SemanticSplatSty. [167]</td><td>[TEGA'24]</td><td>VGG</td><td>0.019</td><td>0.042</td><td>0.028</td><td>0.055</td></tr><tr><td>InstantStyleGaus. [160]</td><td>[ArXiv’24]</td><td>扩散</td><td>0.024</td><td>0.026</td><td>0.074</td><td>0.076</td></tr><tr><td>SGSST [156]</td><td>[CVPR'25]</td><td>VGG</td><td>0.030</td><td>0.032</td><td>0.055</td><td>0.063</td></tr></tbody></table>

### 4.3 Performance Benchmarking: 3DGS Generation

### 4.3 性能基准测试:3DGS生成

- Datasets. We summarize the key features of the datasets in Table 10, with detailed descriptions provided below.

- 数据集。我们在表10中总结了数据集的主要特征，详细描述如下。

NYUdepth [304] contains 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations.

NYUdepth [304] 包含1449张RGBD图像，涵盖464个多样化的室内场景，附有详细标注。

ShapeNet [284] contains approximately 50,000 3D models across 55 object categories. Each model includes a corresponding geometry file and a unique identifier.

ShapeNet [284] 包含约5万个3D模型，分布于55个物体类别。每个模型包括对应的几何文件和唯一标识符。

ScanNet [280] is a large RGB-D dataset containing 2.5 M views in 1,513 indoor scenes annotated with 3D camera poses.

ScanNet [280] 是一个大型RGB-D数据集，包含1513个室内场景中的250万视角，配有3D相机位姿标注。

RealEstate10K [305] contains 67,477 training and 7,289 testing home walkthrough video scenes from YouTube.

RealEstate10K [305] 包含来自YouTube的67477个训练和7289个测试的家庭漫游视频场景。

Replica [285] is a dataset of 18 highly photorealistic 3D indoor scenes, each with dense meshes, HDR textures, semantic and instance labels, and reflective surfaces like mirrors and glass.

Replica [285] 是一个包含18个高度真实感3D室内场景的数据集，每个场景配有密集网格、高动态范围(HDR)纹理、语义和实例标签，以及镜面和玻璃等反射表面。

ACID [306] featuring aerial landscape videos, includes 11,075 training scenes and 1,972 testing scenes.

ACID [306] 以航拍景观视频为特色，包含11075个训练场景和1972个测试场景。

GSO [283] comprises 1,030 3D scanned household items.

GSO [283] 包含1030个3D扫描的家用物品。

LAION-5B [307] consists of three subsets: 2.32 billion English image-text pairs, 2.26 billion image-text pairs in over 100 other languages, and 1.27 billion samples with undetectable language.

LAION-5B [307] 由三个子集组成:23.2亿对英文图文对，22.6亿对超过100种其他语言的图文对，以及12.7亿个无法检测语言的样本。

Objaverse [282] is a large-scale dataset of objects with  <img src="https://www.zhihu.com/equation?tex={800}\mathrm{K} +" alt="{800}\mathrm{K} +" class="ee_img tr_noresize" eeimg="1">  (and growing) 3D models with descriptive captions, tags, and animations.

Objaverse [282] 是一个大规模物体数据集，包含 <img src="https://www.zhihu.com/equation?tex={800}\mathrm{K} +" alt="{800}\mathrm{K} +" class="ee_img tr_noresize" eeimg="1"> (且数量持续增长)的3D模型，附带描述性标题、标签和动画。

OmniObject3D [308] contains 6,000 scanned objects across 190 daily categories. Each object includes textured meshes, point clouds, multiview renders, and real-captured videos.

OmniObject3D [308] 包含6000个扫描物体，涵盖190个日常类别。每个物体包括带纹理的网格、点云、多视角渲染和真实拍摄视频。

LOM [309] comprises five real-world scenes ("buu", "chair", "sofa", "bike", and "shrub"), each containing 25~48 sRGB images captured by a DJI Osmo Action 3 camera under adverse lighting conditions, including low light and overexposure. G-objaverse [310]. Derived from Objaverse [282], G-Objaverse filters out poorly captioned 3D models and includes high-quality renderings produced via a hybrid of rasterization and path tracing. DL3DV-10K [311] comprises  <img src="https://www.zhihu.com/equation?tex={51.2}\mathrm{M}" alt="{51.2}\mathrm{M}" class="ee_img tr_noresize" eeimg="1">  frames from 10,510 videos across 65 locations, covering diverse indoor and outdoor scenes with varying reflection, transparency, and lighting conditions.

LOM [309] 包含五个真实场景(“buu”、“chair”、“sofa”、“bike”和“shrub”)，每个场景包含25至48张由DJI Osmo Action 3相机在恶劣光照条件下(包括低光和过曝)拍摄的sRGB图像。G-objaverse [310] 源自Objaverse [282]，过滤掉描述不佳的3D模型，包含通过光栅化与路径追踪混合生成的高质量渲染图。DL3DV-10K [311] 包含来自65个地点的10510个视频的 <img src="https://www.zhihu.com/equation?tex={51.2}\mathrm{M}" alt="{51.2}\mathrm{M}" class="ee_img tr_noresize" eeimg="1"> 帧，涵盖多样的室内外场景，具有不同的反射、透明度和光照条件。

- Metrics. Common evaluation metrics we used are listed below: MLLM evaluation use multimodal large language models (e.g., GPT-4, LLaVA [317]) to evaluate generation quality from multiple dimensions. Common evaluation metrics include BLIP-VQA [318], CLIP-IQA [319], Q-Align [320] and others.

- 评估指标。我们使用的常见评估指标如下:多模态大语言模型(如GPT-4、LLaVA [317])用于从多维度评估生成质量。常用评估指标包括BLIP-VQA [318]、CLIP-IQA [319]、Q-Align [320]等。

Natural Image Quality Evaluator (NIQE) computes image quality by extracting natural scene statistics from image patches, comparing them to a Gaussian model trained on pristine images, and calculating the deviation as a quality score.

自然图像质量评估器(NIQE)通过提取图像块的自然场景统计特征，比较其与在无损图像上训练的高斯模型的差异，计算偏差作为质量分数。

Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE) estimates perceptual quality by extracting natural scene statistics from locally normalized luminance and applying a regression model trained on human-rated distorted images.

盲/无参考图像空间质量评估器(BRISQUE)通过提取局部归一化亮度的自然场景统计特征，并应用在人类评分失真图像上训练的回归模型，估计感知质量。

Kernel Inception Distance measures distributional differences between real and generated images by computing maximum mean discrepancy with polynomial kernels on Inception-v3 features.

核起始距离(Kernel Inception Distance)通过在Inception-v3特征上使用多项式核计算最大均值差异(maximum mean discrepancy)来衡量真实图像与生成图像的分布差异。

Chamfer distance computes the similarity between two point sets by computing the average closest-point distance in both directions.

Chamfer距离通过计算两个点集之间双向的平均最近点距离来衡量它们的相似性。

Thresholded Symmetric Epipolar Distance (TSED) calculates the number of consistent frame pairs in a sequence.

阈值对称极线距离(Thresholded Symmetric Epipolar Distance，TSED)计算序列中一致帧对的数量。

Inception Score evaluates the quality of generated images using Inception-V3 [301].

Inception分数使用Inception-V3 [301]评估生成图像的质量。

CMMD [321] is based on richer CLIP embeddings and the maximum mean discrepancy with the Gaussian RBF kernel.

CMMD [321]基于更丰富的CLIP嵌入和带有高斯径向基函数(Gaussian RBF)核的最大均值差异。

HPSv2 [322] is a scoring model that more accurately predicts human preferences for generated images.

HPSv2 [322]是一种评分模型，能够更准确地预测人类对生成图像的偏好。

User study. It evaluates results by gathering human feedback on preferences, appearance quality, and geometric quality.

用户研究。通过收集人类对偏好、外观质量和几何质量的反馈来评估结果。

- Results. We select two representative datasets, GSO [283] and Objaverse [282], as benchmarks for 3DGS generation. The performance of various methods is summarized in Table 11 and Table 12, respectively. Due to the limited number of publicly comparable baselines, we provide a quantitative assessment based on the available results. On GSO dataset, NovelGS [217] achieves the best performance by tailoring the diffusion process to suit the  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  Gaussian representation. It supports both text-to-3D and image-to- 3D generation, demonstrating strong generalization across input modalities. On Objaverse dataset, Atlas-Gaussians [207] performs best, leveraging latent space optimization to enhance generation quality. It primarily supports text-conditioned generation and shows promising results on open-domain content.

- 结果。我们选择两个具有代表性的数据集GSO [283]和Objaverse [282]作为3DGS生成的基准。各种方法的性能分别汇总在表11和表12中。由于公开可比基线数量有限，我们基于现有结果提供定量评估。在GSO数据集上，NovelGS [217]通过定制扩散过程以适应 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 高斯表示，取得了最佳性能。它支持文本到3D和图像到3D的生成，展示了跨输入模态的强泛化能力。在Objaverse数据集上，Atlas-Gaussians [207]表现最佳，利用潜在空间优化提升生成质量。它主要支持基于文本的生成，并在开放域内容上展现出良好效果。

TABLE 10: Statistics of representative 3DGS generation datasets. See §4.3 for more detailed descriptions.

表10:代表性3DGS生成数据集的统计信息。详见§4.3。

<table><tr><td>Dataset</td><td>Venue</td><td>#Type</td><td>#Scenes</td><td>Characterization</td></tr><tr><td>NYUdepth [304]</td><td>[ECCV'12]</td><td>Image-to-3D</td><td>464</td><td>It contains 1449 RGB-D images, capturing 464 diverse indoor scenes with detailed annotations.</td></tr><tr><td>ShapeNet [284]</td><td>[ArXiv'15]</td><td>Image&Text-to-3D</td><td>60k</td><td>These 3D models span 55 categories, each with a geometry file and unique identifier.</td></tr><tr><td>ScanNet [280]</td><td>[CVPR’17]</td><td>Image-to-3D</td><td>1513</td><td>It contains  <img src="https://www.zhihu.com/equation?tex={2.5}\mathrm{M}" alt="{2.5}\mathrm{M}" class="ee_img tr_noresize" eeimg="1">  views in 1513 indoor scenes annotated with  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  camera poses.</td></tr><tr><td>RealEstate10K [305]</td><td>[SIGGRAPH'18]</td><td>Image-to-3D</td><td>80k</td><td>It comprises home walkthrough videos from YouTube.</td></tr><tr><td>Replica [285]</td><td>[ArXiv'19]</td><td>Image-to-3D</td><td>18</td><td>A 3D indoor scene dataset featuring dense meshes, HDR textures, and semantic labels.</td></tr><tr><td>ACID [306]</td><td>[ICCV'21]</td><td>Image-to-3D</td><td>13047</td><td>Featuring aerial landscape videos, includes 11,075 training scenes and 1,972 testing scenes.</td></tr><tr><td>GSO [283]</td><td>[ICRA'22]</td><td>Image&Text-to-3D</td><td>1030</td><td>It comprises 3D scanned common household items.</td></tr><tr><td>LAION-5B [307]</td><td>[NeurIPS’22]</td><td>Text-to-3D</td><td>-</td><td>LAION-5B's key feature is its vast scale, with 5.85 billion image-text pairs.</td></tr><tr><td>Objaverse [282]</td><td>[CVPR’23]</td><td>Image&Text-to-3D</td><td>800K</td><td>Objaverse has vast scale of  <img src="https://www.zhihu.com/equation?tex={800}\mathrm{K} + 3\mathrm{D}" alt="{800}\mathrm{K} + 3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  models with rich annotations.</td></tr><tr><td>OmniObject3D [308]</td><td>[CVPR'23]</td><td>Image-to-3D</td><td>6k</td><td>A large-scale collection of high-quality real-scanned 3D objects with rich 2D and 3D annotations.</td></tr><tr><td>LOM [309]</td><td>[AAAI'24]</td><td>Image-to-3D</td><td>5</td><td>It includes five real-world scenes, each with 25~48 sRGB images captured in difficult lighting.</td></tr><tr><td>G-objaverse [310]</td><td>[ECCV'24]</td><td>Image&Text-to-3D</td><td>280K</td><td>10 general classes which gives about  <img src="https://www.zhihu.com/equation?tex={280}\mathrm{\;K}" alt="{280}\mathrm{\;K}" class="ee_img tr_noresize" eeimg="1">  samples.</td></tr><tr><td>DL3DV-10K [311]</td><td>[CVPR’24]</td><td>Image-to-3D</td><td>10K</td><td>Large-scale scene dataset that contains both indoor and outdoor scenarios.</td></tr></table>

<table><tbody><tr><td>数据集</td><td>发布会议</td><td>类型</td><td>场景数量</td><td>特征描述</td></tr><tr><td>NYUdepth [304]</td><td>[ECCV'12]</td><td>图像到三维</td><td>464</td><td>包含1449张RGB-D图像，涵盖464个多样化的室内场景，附有详细标注。</td></tr><tr><td>ShapeNet [284]</td><td>[ArXiv'15]</td><td>图像与文本到三维</td><td>60k</td><td>这些三维模型涵盖55个类别，每个模型包含几何文件和唯一标识符。</td></tr><tr><td>ScanNet [280]</td><td>[CVPR’17]</td><td>图像到三维</td><td>1513</td><td>包含 <img src="https://www.zhihu.com/equation?tex={2.5}\mathrm{M}" alt="{2.5}\mathrm{M}" class="ee_img tr_noresize" eeimg="1"> 个视角，1513个室内场景，配有 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 个相机位姿标注。</td></tr><tr><td>RealEstate10K [305]</td><td>[SIGGRAPH'18]</td><td>图像到三维</td><td>80k</td><td>包含来自YouTube的家庭漫游视频。</td></tr><tr><td>Replica [285]</td><td>[ArXiv'19]</td><td>图像到三维</td><td>18</td><td>一个三维室内场景数据集，包含密集网格、高动态范围纹理(HDR)和语义标签。</td></tr><tr><td>ACID [306]</td><td>[ICCV'21]</td><td>图像到三维</td><td>13047</td><td>包含航拍景观视频，含11,075个训练场景和1,972个测试场景。</td></tr><tr><td>GSO [283]</td><td>[ICRA'22]</td><td>图像与文本到三维</td><td>1030</td><td>包含三维扫描的常见家用物品。</td></tr><tr><td>LAION-5B [307]</td><td>[NeurIPS’22]</td><td>文本到三维</td><td>-</td><td>LAION-5B的主要特点是其庞大规模，拥有58.5亿对图文数据。</td></tr><tr><td>Objaverse [282]</td><td>[CVPR’23]</td><td>图像与文本到三维</td><td>800K</td><td>Objaverse拥有规模庞大的 <img src="https://www.zhihu.com/equation?tex={800}\mathrm{K} + 3\mathrm{D}" alt="{800}\mathrm{K} + 3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 模型，附带丰富标注。</td></tr><tr><td>OmniObject3D [308]</td><td>[CVPR'23]</td><td>图像到三维</td><td>6k</td><td>一个大规模高质量真实扫描三维物体集合，附带丰富的二维和三维标注。</td></tr><tr><td>LOM [309]</td><td>[AAAI'24]</td><td>图像到三维</td><td>5</td><td>包含五个真实场景，每个场景有25至48张在复杂光照条件下拍摄的sRGB图像。</td></tr><tr><td>G-objaverse [310]</td><td>[ECCV'24]</td><td>图像与文本到三维</td><td>280K</td><td>10个通用类别，约有 <img src="https://www.zhihu.com/equation?tex={280}\mathrm{\;K}" alt="{280}\mathrm{\;K}" class="ee_img tr_noresize" eeimg="1"> 个样本。</td></tr><tr><td>DL3DV-10K [311]</td><td>[CVPR’24]</td><td>图像到三维</td><td>10K</td><td>包含室内外场景的大规模场景数据集。</td></tr></tbody></table>

TABLE 11: Quantitative 3DGS generation on GSO [283] dataset. CLIP Image-Image Similarity as CLIP  <img src="https://www.zhihu.com/equation?tex={}_{I2I}" alt="{}_{I2I}" class="ee_img tr_noresize" eeimg="1">  .

表11:在GSO [283]数据集上的定量3DGS生成。CLIP图像-图像相似度作为CLIP  <img src="https://www.zhihu.com/equation?tex={}_{I2I}" alt="{}_{I2I}" class="ee_img tr_noresize" eeimg="1">  。

<table><tr><td>Methods</td><td>Venue</td><td>Category</td><td>Condition</td><td>2D Foundation Model</td><td>PSNR↑</td><td>SSIM↑</td><td>LPIPS  <img src="https://www.zhihu.com/equation?tex=\downarrow" alt="\downarrow" class="ee_img tr_noresize" eeimg="1"> </td><td> <img src="https://www.zhihu.com/equation?tex={\mathbf{{CLIP}}}_{I2I} \uparrow" alt="{\mathbf{{CLIP}}}_{I2I} \uparrow" class="ee_img tr_noresize" eeimg="1"> </td><td>FID  <img src="https://www.zhihu.com/equation?tex=\downarrow" alt="\downarrow" class="ee_img tr_noresize" eeimg="1"> </td><td>KID  <img src="https://www.zhihu.com/equation?tex=\downarrow" alt="\downarrow" class="ee_img tr_noresize" eeimg="1"> </td></tr><tr><td colspan="11">Per-Scene Optimization</td></tr><tr><td>DreamGaussian [9]</td><td>[ICLR'24]</td><td>Standard SDS</td><td>Text&Image</td><td>Stable Diffusion [55]</td><td>18.27</td><td>0.834</td><td>0.189</td><td>0.748</td><td>-</td><td>-</td></tr><tr><td>Hritam et al. [194]</td><td>[ArXiv'24]</td><td>Standard SDS</td><td>Image</td><td>Stable Diffusion [55]</td><td>22.16</td><td>0.887</td><td>0.121</td><td>-</td><td>-</td><td>-</td></tr><tr><td>GECO [195]</td><td>[ArXiv'24]</td><td>Improving SDS</td><td>Image</td><td>Zero123++ [312]</td><td>19.31</td><td>0.825</td><td>0.154</td><td>-</td><td>-</td><td>-</td></tr><tr><td colspan="11">Feed-Forward-based</td></tr><tr><td>LGM [37]</td><td>[ECCV'24]</td><td>Multi-View-based</td><td>Text&Image</td><td>MVDream [201]</td><td>17.13</td><td>0.810</td><td>0.250</td><td>-</td><td>19.93</td><td>0.55</td></tr><tr><td>GRM [38]</td><td>[ECCV'24]</td><td>Network Design</td><td>Text&Image</td><td>Zero123++ [312]</td><td>25.03</td><td>0.899</td><td>0.102</td><td>0.869</td><td>-</td><td>-</td></tr><tr><td>GS-LRM [39]</td><td>[ECCV'24]</td><td>Multi-View-based</td><td>Image</td><td>Zero123++ [312]</td><td>17.70</td><td>0.795</td><td>0.241</td><td>-</td><td>112.96</td><td>-</td></tr><tr><td>Lu et al. [215]</td><td>[MM’24]</td><td>Network Design</td><td>Image</td><td>-</td><td>17.92</td><td>0.810</td><td>0.210</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Hi3D [221]</td><td>[MM’24]</td><td>Optimization Diffusion</td><td>Image</td><td>Stable Video Diffusion [313]</td><td>24.26</td><td>0.864</td><td>0.119</td><td>-</td><td>-</td><td>-</td></tr><tr><td>DiffusionGS [220]</td><td>[ArXiv'24]</td><td>Optimization Diffusion</td><td>Image</td><td>FLUX [57] & SD [55]</td><td>22.07</td><td>0.854</td><td>0.111</td><td>-</td><td>11.52</td><td>-</td></tr><tr><td>GeoGS3D [213]</td><td>[ArXiv*24]</td><td>Multi-View-based</td><td>Image</td><td>Zero-1-to-3 [314]</td><td>22.98</td><td>0.899</td><td>0.146</td><td>-</td><td>-</td><td>-</td></tr><tr><td>NovelGS [217]</td><td>[ArXiv'24]</td><td>Optimization Diffusion</td><td>Text&Image</td><td>-</td><td>31.30</td><td>0.946</td><td>0.065</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Cycle3D [219]</td><td>[AAAI'25]</td><td>Optimization Diffusion</td><td>Image</td><td>MVDream [201]</td><td>21.40</td><td>0.884</td><td>0.115</td><td>0.855</td><td>-</td><td>-</td></tr><tr><td>GaussianAnvthing [206]</td><td>IICLR’251</td><td>Latent Optimization</td><td>Text&Image</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>24.21</td><td>0.76</td></tr><tr><td>Flex3D [212]</td><td>[ICML'25]</td><td>Multi-View-based</td><td>Image</td><td>Emu Model [315]</td><td>25.55</td><td>0.894</td><td>0.074</td><td>0.893</td><td>-</td><td>-</td></tr><tr><td>Ouroboros3D [218]</td><td>[CVPR'25]</td><td>Optimization Diffusion</td><td>Image</td><td>Stable Video Diffusion [313]</td><td>21.76</td><td>0.889</td><td>0.109</td><td>-</td><td>-</td><td>-</td></tr></table>

<table><tbody><tr><td>方法</td><td>会议</td><td>类别</td><td>条件</td><td>二维基础模型</td><td>峰值信噪比↑</td><td>结构相似性指数↑</td><td>LPIPS  <img src="https://www.zhihu.com/equation?tex=\downarrow" alt="\downarrow" class="ee_img tr_noresize" eeimg="1"> </td><td> <img src="https://www.zhihu.com/equation?tex={\mathbf{{CLIP}}}_{I2I} \uparrow" alt="{\mathbf{{CLIP}}}_{I2I} \uparrow" class="ee_img tr_noresize" eeimg="1"> </td><td>FID  <img src="https://www.zhihu.com/equation?tex=\downarrow" alt="\downarrow" class="ee_img tr_noresize" eeimg="1"> </td><td>KID  <img src="https://www.zhihu.com/equation?tex=\downarrow" alt="\downarrow" class="ee_img tr_noresize" eeimg="1"> </td></tr><tr><td colspan="11">逐场景优化</td></tr><tr><td>DreamGaussian [9]</td><td>[ICLR'24]</td><td>标准SDS</td><td>文本与图像</td><td>Stable Diffusion [55]</td><td>18.27</td><td>0.834</td><td>0.189</td><td>0.748</td><td>-</td><td>-</td></tr><tr><td>Hritam等人 [194]</td><td>[ArXiv'24]</td><td>标准SDS</td><td>图像</td><td>Stable Diffusion [55]</td><td>22.16</td><td>0.887</td><td>0.121</td><td>-</td><td>-</td><td>-</td></tr><tr><td>GECO [195]</td><td>[ArXiv'24]</td><td>改进SDS</td><td>图像</td><td>Zero123++ [312]</td><td>19.31</td><td>0.825</td><td>0.154</td><td>-</td><td>-</td><td>-</td></tr><tr><td colspan="11">基于前馈</td></tr><tr><td>LGM [37]</td><td>[ECCV'24]</td><td>基于多视角</td><td>文本与图像</td><td>MVDream [201]</td><td>17.13</td><td>0.810</td><td>0.250</td><td>-</td><td>19.93</td><td>0.55</td></tr><tr><td>GRM [38]</td><td>[ECCV'24]</td><td>网络设计</td><td>文本与图像</td><td>Zero123++ [312]</td><td>25.03</td><td>0.899</td><td>0.102</td><td>0.869</td><td>-</td><td>-</td></tr><tr><td>GS-LRM [39]</td><td>[ECCV'24]</td><td>基于多视角</td><td>图像</td><td>Zero123++ [312]</td><td>17.70</td><td>0.795</td><td>0.241</td><td>-</td><td>112.96</td><td>-</td></tr><tr><td>陆等人 [215]</td><td>[MM’24]</td><td>网络设计</td><td>图像</td><td>-</td><td>17.92</td><td>0.810</td><td>0.210</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Hi3D [221]</td><td>[MM’24]</td><td>优化扩散</td><td>图像</td><td>Stable Video Diffusion [313]</td><td>24.26</td><td>0.864</td><td>0.119</td><td>-</td><td>-</td><td>-</td></tr><tr><td>DiffusionGS [220]</td><td>[ArXiv'24]</td><td>优化扩散</td><td>图像</td><td>FLUX [57] & SD [55]</td><td>22.07</td><td>0.854</td><td>0.111</td><td>-</td><td>11.52</td><td>-</td></tr><tr><td>GeoGS3D [213]</td><td>[ArXiv*24]</td><td>基于多视角</td><td>图像</td><td>Zero-1-to-3 [314]</td><td>22.98</td><td>0.899</td><td>0.146</td><td>-</td><td>-</td><td>-</td></tr><tr><td>NovelGS [217]</td><td>[ArXiv'24]</td><td>优化扩散</td><td>文本与图像</td><td>-</td><td>31.30</td><td>0.946</td><td>0.065</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Cycle3D [219]</td><td>[AAAI'25]</td><td>优化扩散</td><td>图像</td><td>MVDream [201]</td><td>21.40</td><td>0.884</td><td>0.115</td><td>0.855</td><td>-</td><td>-</td></tr><tr><td>GaussianAnvthing [206]</td><td>IICLR’251</td><td>潜空间优化</td><td>文本与图像</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>24.21</td><td>0.76</td></tr><tr><td>Flex3D [212]</td><td>[ICML'25]</td><td>基于多视角</td><td>图像</td><td>Emu模型 [315]</td><td>25.55</td><td>0.894</td><td>0.074</td><td>0.893</td><td>-</td><td>-</td></tr><tr><td>Ouroboros3D [218]</td><td>[CVPR'25]</td><td>优化扩散</td><td>图像</td><td>Stable Video Diffusion [313]</td><td>21.76</td><td>0.889</td><td>0.109</td><td>-</td><td>-</td><td>-</td></tr></tbody></table>

TABLE 12: Quantitative 3DGS generation on Objaverse [282] dataset. CLIP Text-Image Similarity as CLIP  <img src="https://www.zhihu.com/equation?tex={}_{T2I}" alt="{}_{T2I}" class="ee_img tr_noresize" eeimg="1"> 

表12:在Objaverse [282]数据集上的定量3DGS生成。CLIP文本-图像相似度作为CLIP  <img src="https://www.zhihu.com/equation?tex={}_{T2I}" alt="{}_{T2I}" class="ee_img tr_noresize" eeimg="1"> 

<table><tr><td>Methods</td><td>Venue</td><td>Category</td><td>Condition</td><td>2D Foundation Model</td><td>PSNR↑</td><td>SSIM↑</td><td>LPIPS  <img src="https://www.zhihu.com/equation?tex=\downarrow" alt="\downarrow" class="ee_img tr_noresize" eeimg="1"> </td><td> <img src="https://www.zhihu.com/equation?tex={\mathbf{{CLIP}}}_{T2I} \uparrow" alt="{\mathbf{{CLIP}}}_{T2I} \uparrow" class="ee_img tr_noresize" eeimg="1"> </td><td>FID  <img src="https://www.zhihu.com/equation?tex=\downarrow" alt="\downarrow" class="ee_img tr_noresize" eeimg="1"> </td><td>KID  <img src="https://www.zhihu.com/equation?tex=\downarrow" alt="\downarrow" class="ee_img tr_noresize" eeimg="1"> </td></tr><tr><td colspan="11">Feed-Forward-based</td></tr><tr><td>LGM [37]</td><td>[ECCV'24]</td><td>Multi-View-based</td><td>Text&Image</td><td>MVDream [201]</td><td>-</td><td>-</td><td>-</td><td>27.21</td><td>123.8</td><td>4.53</td></tr><tr><td>GVGEN [204]</td><td>[ECCV'24]</td><td>Optimization Diffusion</td><td>Text</td><td>Stable Diffusion [55]</td><td>-</td><td>-</td><td>-</td><td>27.33</td><td>132.4</td><td>6.04</td></tr><tr><td>GeoGS3D [213]</td><td>[ArXiv'24]</td><td>Multi-View-based</td><td>Image</td><td>Zero-1-to-3 [314]</td><td>23.97</td><td>0.921</td><td>0.113</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Wizadwongsa et al. [205]</td><td>[ArXiv'24]</td><td>Latent Optimization</td><td>Text</td><td>Stable Diffusion 3 [316]</td><td>-</td><td>-</td><td>-</td><td>27.61</td><td>-</td><td>-</td></tr><tr><td>Atlas-Gaussians [207]</td><td>[ICLR'25]</td><td>Latent Optimization</td><td>Text</td><td>Latent Diffusion Model [55]</td><td>-</td><td>-</td><td>-</td><td>30.66</td><td>109.5</td><td>4.04</td></tr><tr><td>Turbo3D [208]</td><td>[CVPR'25]</td><td>Latent Optimization</td><td>Text</td><td>DiT [56]</td><td>-</td><td>-</td><td>-</td><td>27.61</td><td>-</td><td>-</td></tr></table>

<table><tbody><tr><td>方法</td><td>会议</td><td>类别</td><td>条件</td><td>二维基础模型</td><td>峰值信噪比↑</td><td>结构相似性指数↑</td><td>感知相似度(LPIPS) <img src="https://www.zhihu.com/equation?tex=\downarrow" alt="\downarrow" class="ee_img tr_noresize" eeimg="1"> </td><td> <img src="https://www.zhihu.com/equation?tex={\mathbf{{CLIP}}}_{T2I} \uparrow" alt="{\mathbf{{CLIP}}}_{T2I} \uparrow" class="ee_img tr_noresize" eeimg="1"> </td><td>生成对抗网络距离(FID) <img src="https://www.zhihu.com/equation?tex=\downarrow" alt="\downarrow" class="ee_img tr_noresize" eeimg="1"> </td><td>核嵌入距离(KID) <img src="https://www.zhihu.com/equation?tex=\downarrow" alt="\downarrow" class="ee_img tr_noresize" eeimg="1"> </td></tr><tr><td colspan="11">基于前馈网络</td></tr><tr><td>LGM [37]</td><td>[ECCV'24]</td><td>基于多视角</td><td>文本与图像</td><td>MVDream [201]</td><td>-</td><td>-</td><td>-</td><td>27.21</td><td>123.8</td><td>4.53</td></tr><tr><td>GVGEN [204]</td><td>[ECCV'24]</td><td>优化扩散</td><td>文本</td><td>Stable Diffusion(稳定扩散)[55]</td><td>-</td><td>-</td><td>-</td><td>27.33</td><td>132.4</td><td>6.04</td></tr><tr><td>GeoGS3D [213]</td><td>[ArXiv'24]</td><td>基于多视角</td><td>图像</td><td>Zero-1-to-3 [314]</td><td>23.97</td><td>0.921</td><td>0.113</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Wizadwongsa 等人 [205]</td><td>[ArXiv'24]</td><td>潜空间优化</td><td>文本</td><td>Stable Diffusion 3(稳定扩散3)[316]</td><td>-</td><td>-</td><td>-</td><td>27.61</td><td>-</td><td>-</td></tr><tr><td>Atlas-Gaussians [207]</td><td>[ICLR'25]</td><td>潜空间优化</td><td>文本</td><td>潜空间扩散模型(Latent Diffusion Model)[55]</td><td>-</td><td>-</td><td>-</td><td>30.66</td><td>109.5</td><td>4.04</td></tr><tr><td>Turbo3D [208]</td><td>[CVPR'25]</td><td>潜空间优化</td><td>文本</td><td>DiT [56]</td><td>-</td><td>-</td><td>-</td><td>27.61</td><td>-</td><td>-</td></tr></tbody></table>

## 5 FUTURE DIRECTIONS

## 5 未来方向

- Large-Scale Feed-Forward Learning. To overcome the inefficiency of per-scene optimization, especially under dense calibrated images conditions, more and more methods adopt feed-forward architectures to enable fast and generalizable segmentation, editing, and generation. In the future, future research should explore large-scale training regimes across diverse scenarios and domains. This includes scaling up data volume, improving data diversity (e.g., egocentric, indoor, outdoor), and designing robust pipelines that generalize beyond static, clean environments.

- 大规模前馈学习。为克服逐场景优化的低效性，尤其是在密集校准图像条件下，越来越多的方法采用前馈架构以实现快速且具泛化能力的分割、编辑和生成。未来的研究应探索跨多样场景和领域的大规模训练方案，包括扩大数据量、提升数据多样性(如自我视角、室内、室外)以及设计能够超越静态、干净环境的鲁棒流程。

- Faithful Evaluation Metrics. Current evaluation protocols often inherit metrics from traditional 2D or NeRF-based tasks, which may not align well with 3DGS goals. Developing reliable and interpretable metrics specifically designed for evaluating application tasks from a 3D Gaussian perspective is essential to track progress and guide future research. Existing metrics mainly evaluate accuracy after projection onto 2D images, lacking meaningful insights into the inherent  <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  geometric and semantic consistency of generated or edited scenes.

- 可靠的评估指标。目前的评估协议多沿用传统二维或基于NeRF(神经辐射场)任务的指标，这些指标可能与3DGS(3D高斯点渲染)目标不完全契合。开发专门针对从3D高斯视角评估应用任务的可靠且可解释的指标，对于跟踪进展和指导未来研究至关重要。现有指标主要评估投影到二维图像后的准确性，缺乏对生成或编辑场景内在 <img src="https://www.zhihu.com/equation?tex=3\mathrm{D}" alt="3\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 几何和语义一致性的有效洞察。

- Integrating Large Language Models. Combining 3DGS with Large Language Models (LLMs) opens new opportunities for semantic understanding and instruction-based manipulation, such as text-driven editing, generation, and open-vocabulary understanding. Research is needed to build effective 3D-text alignment and prompting strategies. Leveraging LLMs to reason over spatial relationships and affordances in 3D scenes also presents promising avenues for embodied intelligence and interactive applications.

- 融合大型语言模型。将3DGS与大型语言模型(LLMs)结合，为语义理解和基于指令的操作开辟了新机遇，如文本驱动的编辑、生成和开放词汇理解。需要研究构建有效的3D-文本对齐和提示策略。利用LLMs推理3D场景中的空间关系和可用性，也为具身智能和交互式应用提供了有前景的方向。

- Synthesizing 3D Data. Given the persistent shortage of annotated 3D data, future research can leverage the generative capabilities of 3DGS to synthesize large-scale, high-quality 3D datasets from abundant  <img src="https://www.zhihu.com/equation?tex=2\mathrm{D}" alt="2\mathrm{D}" class="ee_img tr_noresize" eeimg="1">  image collections. Such synthetic datasets could significantly enhance training data diversity and volume, ultimately driving substantial improvements in the performance of 3DGS application tasks. Additionally, combining synthesized data with domain adaptation techniques could further bridge the domain gap between synthetic and real-world data, enhancing the model's robustness and generalization capabilities.

- 合成3D数据。鉴于标注3D数据的持续短缺，未来研究可利用3DGS的生成能力，从丰富的 <img src="https://www.zhihu.com/equation?tex=2\mathrm{D}" alt="2\mathrm{D}" class="ee_img tr_noresize" eeimg="1"> 图像集合中合成大规模高质量的3D数据集。此类合成数据集可显著提升训练数据的多样性和规模，最终推动3DGS应用任务性能的显著提升。此外，将合成数据与领域自适应技术结合，能够进一步缩小合成与真实数据间的领域差距，增强模型的鲁棒性和泛化能力。

- Toward Generalist Models. Instead of designing separate models for segmentation, editing, and generation, a promising direction is to develop generalist 3DGS-based architectures that can perform multiple scene-level tasks within a unified framework. Developing such versatile models can facilitate the sharing of learned representations across tasks, resulting in improved efficiency, reduced redundancy in training, and enhanced generalization capabilities.

- 迈向通用模型。与其为分割、编辑和生成设计独立模型，一个有前景的方向是开发基于3DGS的通用架构，能够在统一框架内执行多种场景级任务。开发此类多功能模型有助于跨任务共享学习到的表示，提高效率，减少训练冗余，并增强泛化能力。

- Combining 3D Foundation Model. Emerging 3D foundation models such as VGGT [323] and FLARE [324] demonstrate robust, generalizable representations by pretraining on diverse 3D tasks, including depth estimation, point cloud reconstruction, camera prediction, and 3D point tracking from multi-view images. These models encode rich geometric priors that can benefit various downstream applications. Future work may incorporate them into 3DGS to improve generalization and efficiency.

- 结合3D基础模型。新兴的3D基础模型如VGGT [323]和FLARE [324]通过在多样的3D任务上预训练(包括深度估计、点云重建、相机预测和多视角图像的3D点跟踪)，展现出鲁棒且具泛化能力的表示。这些模型编码了丰富的几何先验，可惠及多种下游应用。未来工作可将其整合进3DGS，以提升泛化性和效率。

## 6 CONCLUSION

## 6 结论

This survey systematically explores recent advances in utilizing 3D Gaussian Splatting (3DGS) for downstream application tasks, representing a pioneering effort in summarizing this emerging field. We first introduce essential background knowledge, covering fundamental concepts and foundational models. Subsequently, we comprehensively review over 200 representative methods across several key tasks, including segmentation, editing, generation, and other related application tasks, providing a structured categorization from a technical perspective. Furthermore, we present a thorough analysis of existing benchmarks and conduct fair comparisons of representative models clearly. Lastly, we highlight current challenges and outline promising future directions to inspire continued research efforts toward enhanced high-level tasks. REFERENCES

本综述系统地探讨了利用3D高斯点渲染(3DGS)进行下游应用任务的最新进展，代表了对该新兴领域的开创性总结。我们首先介绍了基础知识，涵盖基本概念和基础模型。随后，全面回顾了200多种代表性方法，涵盖分割、编辑、生成及其他相关应用任务，从技术角度提供了结构化分类。此外，我们对现有基准进行了深入分析，并对代表模型进行了公平比较。最后，强调了当前挑战并勾勒了有前景的未来方向，以激励持续研究，推动更高级任务的发展。参考文献

